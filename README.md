<p align="center">
  <a href="https://github.com/trimstray/test-your-sysadmin-skills">
    <img src="https://github.com/trimstray/test-your-sysadmin-skills/blob/master/static/img/sysadmin_preview.png" alt="マスター">
  </a>
</p>

<br>

<p align="center">:star:</p>

<p align="center">"<i>優れた管理者はすべてを知る必要はないが、不可能なプロジェクトに対して驚くべき解決策を考え出すことができるべきだ。</i>" - cwheeler33 (ServerFault)</p>

<p align="center">:star:</p>

<p align="center">"<i>私のスキルは物事を動かすことであり、無数の事実を知っていることではありません。[…] システムを修正する必要がある場合は、問題を特定し、ログを確認し、エラーを調べます。解決策を実装する必要がある場合は、正しい解決策を調査し、それを実装し、文書化します。その後、それに頻繁に触れることがなければ、大まかな理解にとどまることが多いですが、それゆえに文書化されているのです。</i>" - Sparcrypt (Reddit)</p>

<br>

<p align="center">
  <a href="https://github.com/trimstray/test-your-sysadmin-skills/pulls">
    <img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?longCache=true" alt="プルリクエスト歓迎">
  </a>
  <a href="LICENSE.md">
    <img src="https://img.shields.io/badge/License-MIT-lightgrey.svg?longCache=true" alt="MITライセンス">
  </a>
</p>

<p align="center">
  <a href="https://twitter.com/trimstray" target="_blank">
    <img src="https://img.shields.io/twitter/follow/trimstray.svg?logo=twitter" alt="Twitterフォロー">
  </a>
</p>

<div align="center">
  <sub>作成者: <a href="https://twitter.com/trimstray">trimstray</a> と <a href="https://github.com/trimstray/test-your-sysadmin-skills/graphs/contributors">コントリビューター</a>
</div>

<br>

****

<br>

:information_source: &nbsp;このプロジェクトには、**Linux (\*nix) システム管理者**のポジションに関する知識をテストしたり、面接や試験で使用できる**284**のテスト問題と回答が含まれています。

:heavy_check_mark: &nbsp;回答はあくまで**一例**であり、全てのトピックを網羅しているわけではありません。ほとんどの回答には、より深い理解のための**有用なリソース**が含まれています。

:warning: &nbsp;**`***`**でマークされた質問にはまだ回答がないか、回答が不完全です。**プルリクエストを送って追加してください**！

:traffic_light: &nbsp;何かおかしい点があったり、何かが正しくないように見える場合は、**ぜひプルリクエストを送ってください**。変更やコメントについて、正当でよく考えられた説明を追加してください。

:books: &nbsp;知識やスキルを向上させるために、[devops-interview-questions](https://github.com/bregman-arie/devops-interview-questions)を参照してください。とても興味深い内容です。

<br>

<p align="center">
  » <b><code><a href="https://github.com/trimstray/test-your-sysadmin-skills/issues">すべての提案を歓迎します</a></code></b> «
</p>

<br>

## 目次

| <b><u>チャプターの種類</u></b> | <b><u>質問数</u></b> | <b><u>簡単な説明</u></b> |
| :---         | :---         | :---         |
| <b>[はじめに](#introduction)</b> |||
| :small_orange_diamond: [シンプルな質問](#simple-questions) | 14の質問 | リラックスした、楽しい、そしてシンプルな質問です。すべてを始めるのに最適です。 |
| <b>[一般的な知識](#general-knowledge)</b> |||
| :small_orange_diamond: [ジュニアシステム管理者](#junior-sysadmin) | 65の質問 | 基本的な知識に基づいた比較的簡単でストレートな質問です。 |
| :small_orange_diamond: [レギュラーシステム管理者](#regular-sysadmin) | 94の質問 | しっかりとした知識があれば解ける中級レベルの質問です。 |
| :small_orange_diamond: [シニアシステム管理者](#senior-sysadmin) | 99の質問 | 難しい質問や謎解きです。良くなりたいなら挑戦してください。 |
| <b>[秘密の知識](#secret-knowledge)</b> ||
| :small_orange_diamond: [グルシステム管理者](#guru-sysadmin) | 12の質問 | 本当に深い質問です。グルシステム管理者になるための知識を得るためのものです。 |

<br>

## <a name="introduction">はじめに</a>

### :diamond_shape_with_a_dot_inside: <a name="simple-questions">シンプルな質問</a>

- <b>今週学んだことは何ですか？</b>
- <b>システム管理の世界で何に興奮や興味を感じますか？</b>
- <b>最近経験した技術的な課題とその解決方法を教えてください。</b>
- <b>最後に完了した大きなプロジェクトについて教えてください。</b>
- <b>オープンソースプロジェクトに貢献していますか？</b>
- <b>自分のホームラボのセットアップについて説明してください。</b>
- <b>最も誇りに思っている個人的な達成は何ですか？</b>
- <b>これまでに犯した最大のミスは何ですか？今ならどのように対処しますか？</b>
- <b>新しい仕事の初日にインストールするソフトウェアツールは何ですか？</b>
- <b>自分の知識データベース（例：ウィキ、ファイル、ポータル）の管理方法について教えてください。</b>
- <b>毎日確認するニュースソースは何ですか？（システム管理、セキュリティ関連、その他）</b>
- <b>NOCチームがシステム管理者の認証のための新しい予算を持っています。どの認証を取得したいですか、そしてその理由は何ですか？</b>
- <b>開発者とのやり取りはどうしていますか？「彼ら対私たち」か、それとも「異なるアプローチで協力し合う」？</b>
- <b>もし私が面接をしていて、非標準的な状況に対する私の能力を知るためにどの質問をしますか？</b>

## <a name="general-knowledge">一般的な知識</a>

### :diamond_shape_with_a_dot_inside: <a name="junior-sysadmin">ジュニアシステム管理者</a>

###### システムに関する質問 (37)

<details>
<summary><b>Linuxディストリビューションの例をいくつか挙げてください。あなたのお気に入りのディストリビューションは何ですか？その理由は？</b></summary><br>
- Red Hat Enterprise Linux
- Fedora
- CentOS
- Debian
- Ubuntu
- Mint
- SUSE Linux Enterprise Server (SLES)
- SUSE Linux Enterprise Desktop (SLED)
- Slackware
- Arch
- Kali
- Backbox

私のお気に入りのLinuxディストリビューション:

- **Arch Linux** は、カスタムオペレーティングシステムを構築するための最小限の基本システムを提供します。また、公式のバイナリリポジトリと組み合わせると、おそらくすべてのディストリビューションの中で最大のリポジトリを持つ可能性があるArchユーザーリポジトリ（AUR）も持っています。パッケージングプロセスも非常に簡単で、公式リポジトリやAURにないパッケージが欲しい場合、自分で作成するのも容易です。
- **Linux Mint** は、Ubuntu LTSリリースから派生しており、Cinnamon、MATE、Xfceなど、いくつかの異なるデスクトップ環境を特徴としています。Mintは非常に洗練されており、その美学は非常に魅力的です。特に新しいアイコンテーマが気に入っていますが、GTK+テーマは私の好みに合わないのであまり好きではありません。最新リリースのMint 19ではバグがあり、約2週間前にフォーラムで質問しましたが、まだ返信がなく、そのバグは私の生活を少し不便にしています。
- **Kali Linux** は、上級のペネトレーションテストやセキュリティ監査を目的としたDebianベースのLinuxディストリビューションです。Kaliには、ペネトレーションテスト、セキュリティリサーチ、コンピューターフォレンジクス、リバースエンジニアリングなど、さまざまな情報セキュリティタスクに向けた数百のツールが含まれています。

役立つリソース:

- [Linuxディストリビューションの一覧](https://en.wikipedia.org/wiki/List_of_Linux_distributions)
- [あなたのお気に入りのLinuxディストリビューションは何ですか？その理由は？](https://www.quora.com/What-is-your-favorite-Linux-distro-and-why)

</details>

<details>
<summary><b>Unix、Linux、BSD、GNUの違いは何ですか？</b></summary><br>

**GNU** は実際にはOSではありません。むしろ、自由ソフトウェアを統治する一連のルールや哲学であり、同時にOSを作成しようとする過程で多くのツールを生み出しました。つまり、**GNU**ツールは、すでに存在していたツールのオープンバージョンであり、オープンソフトウェアの原則に従って再実装されたものです。**GNU/Linux** は、これらのツールと **Linuxカーネル** を組み合わせた完全なOSですが、他にも**GNU/Hurd**のようなGNUが存在します。

**Unix** と **BSD** は、さまざまなレベルの「クローズドソース」であるPOSIXの「古い」実装です。**Unix** は通常、完全にクローズドソースですが、**Unix**のフレーバーは **Linux** のフレーバーと同じくらい多いかもしれません。**BSD** は通常「オープン」とは見なされませんが、リリースされた当時は非常にオープンだと考えられていました。ライセンスも、当時の「よりオープン」なライセンスよりも、商業利用に対してはるかに少ない制約を持っていました。

**Linux** はこの4つの中で最も新しいです。厳密には「カーネルのみ」です。しかし、一般的には、GNUツールやその他のコアコンポーネントと組み合わせることで、完全なOSと見なされます。

これらの主な違いは、その理想にあります。**Unix**、**Linux**、**BSD** は、それぞれ異なる理想を実装しています。すべてPOSIXに準拠しており、基本的に互換性があります。彼らは同じ問題を異なる方法で解決することもあります。そのため、理想とPOSIX標準の実装方法以外には、大きな違いはほとんどありません。

詳細については、**GNU**、**OSS**、**Linux**、**BSD**、**UNIX** の作成に関する簡単な記事を読むことをお勧めします。それらは個々の理想に傾倒していますが、これらの記事を読むことで、違いをよりよく理解できるでしょう。

役立つリソース:

- [Unix、Linux、BSD、GNUの違いは何ですか？ (オリジナル)](https://unix.stackexchange.com/questions/104714/what-is-the-difference-between-unix-linux-bsd-and-gnu)
- [大論争: それはLinuxか、GNU/Linuxか？](https://www.howtogeek.com/139287/the-great-debate-is-it-linux-or-gnulinux/)

</details>

<details>
<summary><b>CLIとは何ですか？お気に入りのCLIツール、ヒント、ハックについて教えてください。</b></summary><br>

**CLI** はコマンドラインインターフェースまたはコマンド言語インタープリタの略です。コマンドラインはシステムやコンピュータを制御するための最も強力な方法の一つです。

Unixのようなシステムでは、**CLI** はユーザーがシステムに実行させるコマンドを入力できるインターフェースです。**CLI** は非常に強力ですが、エラーに対して寛容ではありません。

**CLI** を使用すると、システムの内部やコードを非常に細かく操作できます。どのOSを使用していても、GUIよりも柔軟性とコントロールを提供します。Githubにホストされているソフトウェアを使用する場合、**CLI** でいくつかのコマンドを実行して動作させる必要があることが多いです。

**お気に入りのツール**

- `screen` - 無料のターミナルマルチプレクサーで、セッションを開始すると、接続が切断されてもターミナルが保存されるので、後で再開したり、自宅から再開できます。
- `ssh` - 学ぶべき最も価値のあるコマンドで、次のような驚くべきことができます:
  * `sshfs` を使用してインターネット経由でファイルシステムをマウント
  * コマンドを転送：`rsync` デーモンがないサーバーで、ssh経由で自分で開始して実行
  * バッチファイルで実行：リモートコマンドの出力をリダイレクトしてローカルバッチファイル内で使用
- `vi/vim` - 最も人気があり強力なテキストエディタで、汎用性が高く、大きなファイルでも非常に高速に動作します。
- `bash-completion` - シェルのためのいくつかの事前定義された補完ルールを含んでいます。

**ヒント & ハック**

- `CTRL + R` でコマンド履歴を検索
- ディレクトリスタックを操作できる `popd/pushd` などのシェル組み込みコマンド
- `CTRL + U`、`CTRL + E` などの編集キーボードショートカット
- 組み合わせが自動展開されます:
  * `!*` - 最後のコマンドのすべての引数
  * `!!` - 最後のコマンド全体
  * `!ssh` - sshで始まる最後のコマンド

役立つリソース:

- [コマンドラインインターフェースの定義](http://www.linfo.org/command_line_interface.html)
- [Bashを使用したお気に入りのコマンドライントリックは何ですか？](https://stackoverflow.com/questions/68372/what-is-your-single-most-favorite-command-line-trick-using-bash/69716)
- [お気に入りのコマンドライン機能やトリックは何ですか？](https://unix.stackexchange.com/questions/6/what-are-your-favorite-command-line-features-or-tricks)

</details>

<details>
<summary><b>お気に入りのシェルは何ですか？その理由は？</b></summary><br>

**BASH** が私のお気に入りです。これは個人的な好みの問題で、シンタックスが好きで、自分にとってしっくりくるからです。入力/出力のリダイレクトシンタックス（`>>`、`<< 2>&1`、`2>`、`1>` など）はC++に似ているので、認識しやすいです。

また、**ZSH** シェルも好きです。**BASH** よりもはるかにカスタマイズ性が高いからです。Oh-My-Zshフレームワーク、強力なコンテキストベースのタブ補完、パターンマッチング/グロービングの強化、ロード可能なモジュールなどがあります。

役立つリソース:

- [コマンドシェルの比較](https://en.wikipedia.org/wiki/Comparison_of_command_shells)

</details>


<details>
<summary><b>コマンドラインでヘルプを得るにはどうすればよいですか？***</b></summary><br>

- `man` [コマンド名] を使用して、コマンドの説明を見ることができます（例: `man less`、`man cat`）

- `-h` または `--help` 一部のプログラムでは、このパラメータを渡すと使い方の説明が表示されます（例: `python -h` や `python --help`）

</details>

<details>
<summary><b>ログイン後に*nixサーバーで最初に実行する5つのコマンド</b></summary><br>

- `w` - サーバーの稼働時間などの多くの有用な情報が表示されます
- `top` - 実行中のすべてのプロセスを確認し、CPUやメモリの使用量などで並べ替えることができます
- `netstat` - サーバーがどのポートとIPでリッスンしているか、およびそれを使用しているプロセスを確認できます
- `df` - ファイルシステムによって使用されているディスクスペースの利用状況を報告します
- `history` - 現在接続しているユーザーが以前に実行したコマンドを表示します

役立つリソース:

- [Linuxサーバーに接続した際の最初の5つのコマンド (オリジナル)](https://www.linux.com/blog/first-5-commands-when-i-connect-linux-server)

</details>

<details>
<summary><b><code>ls -al</code>の出力におけるフィールドの意味は何ですか？</b></summary><br>

出力の順序は以下の通りです:

```bash
-rwxrw-r--    1    root   root 2048    Jan 13 07:11 db.dump
```

- ファイルのアクセス権限、
- リンクの数、
- 所有者の名前、
- 所有者のグループ、
- ファイルサイズ、
- 最終更新時刻、
- ファイル/ディレクトリ名

ファイルのアクセス権限は以下のように表示されます:

- 最初の文字は `-`、`l`、または `d` で、`d` はディレクトリ、`-` はファイル、`l` はシンボリックリンク（またはソフトリンク） - 特殊なファイルタイプを示します
- 3セットの文字が3回表示され、所有者、グループ、その他のアクセス権限を示します:
  - `r` = 読み取り可能
  - `w` = 書き込み可能
  - `x` = 実行可能

あなたの例では `-rwxrw-r--` は以下を意味します:

- 通常のファイル（`-` と表示される）
- 所有者によって読み取り、書き込み、実行が可能 (`rwx`)
- グループによって読み取り、書き込みが可能、実行は不可 (`rw-`)
- その他のユーザーによって読み取り可能で、書き込みや実行は不可 (`r--`)

役立つリソース:

- [<code>ls -al</code>の出力におけるフィールドの意味は何ですか？（オリジナル）](https://unix.stackexchange.com/questions/103114/what-do-the-fields-in-ls-al-output-mean)

</details>

<details>
<summary><b>ログインしているユーザーのリストを取得するにはどうすればよいですか？</b></summary><br>

ログインしているユーザーの概要、各ユーザーのログイン、端末、ログイン日時、および接続元のコンピュータ（可能な場合）を含む情報を表示するには、次のコマンドを入力します：

```bash
# これには、/var/run/utmp および /var/log/wtmp ファイルを使用して詳細情報を取得します。
who
```

より詳細な情報、例えばユーザー名、端末、ソースコンピュータのIP番号、ログインが開始された時間、アイドル時間、プロセスのCPUサイクル、ジョブのCPUサイクル、現在実行中のコマンドなどを含む情報を取得するには、次のコマンドを入力します：

```bash
# それには、/var/run/utmp および /proc のプロセスが使用されます。
w
```

最後にログインしたユーザーのリストを表示するには、次のコマンドを入力します：

```bash
# /var/log/wtmpを使用します。
last
```

役立つリソース：

- [4 Ways to Identify Who is Logged-In on Your Linux System](https://www.thegeekstuff.com/2009/03/4-ways-to-identify-who-is-logged-in-on-your-linux-system/)

</details>

<details>
<summary><b>バックグラウンドでプロセスを実行する利点は何ですか？どうやってそれを行うことができますか？</b></summary><br>

バックグラウンドでプロセスを実行する最も大きな利点は、他のプロセスがバックグラウンドで実行されている間に、同時に他のタスクを実行できることです。これにより、他のプロセスを作業しながらバックグラウンドでより多くのプロセスを完了できます。これは、コマンドの最後に特別な文字 `&` を追加することで実現できます。

一般的に、実行に時間がかかり、ユーザーの操作を必要としないアプリケーションは、バックグラウンドに送信され、ターミナルでの作業を続けることができます。

たとえば、バックグラウンドで何かをダウンロードしたい場合、次のように実行できます：

```bash
wget https://url-to-download.com/download.tar.gz &
```


コマンドを実行すると、次のような出力が得られます：

```bash
[1] 2203
```

ここで、1はジョブのシリアル番号で、2203はジョブのPIDです。

バックグラウンドで実行中のジョブを確認するには、次のコマンドを使用します：

```bash
jobs
```

バックグラウンドで実行中のジョブに対してPIDが表示され、ジョブを終了するには次のコマンドを使用します：

```bash
kill PID
```

PIDはジョブのPIDに置き換えてください。バックグラウンドで1つのジョブのみが実行されている場合、次のコマンドでそれを前面に持ってくることができます：

```bash
fg
```

バックグラウンドで複数のジョブが実行されている場合、次のコマンドを使用して任意のジョブを前面に持ってくることができます：

```bash
fg %#
```

ジョブのシリアル番号を `#` に置き換えてください。

有用なリソース：

- [How do I run a Unix process in the background?](https://kb.iu.edu/d/afnz)
- [Job Control Commands](http://tldp.org/LDP/abs/html/x9644.html)
- [What is/are the advantage(s) of running applications in background?](https://unix.stackexchange.com/questions/162186/what-is-are-the-advantages-of-running-applications-in-backgound)

</details>

<details>
<summary><b>プロセスを管理する前に、それらを識別できる必要があります。どのツールを使用しますか？ ***</b></summary><br>

未完成です。

</details>

<details>
<summary><b>rootユーザーとしてコマンドを実行することは良い習慣ですか、それとも悪い習慣ですか？</b></summary><br>

すべてをrootとして実行することは悪い習慣です。その理由は以下の通りです：

- **愚かさ**: 不注意なミスを防ぐものがありません。システムに潜在的に危害を加える変更を試みる場合は、`sudo` を使用する必要があります。これにより、パスワードを入力する際に一時停止があり、ミスを犯す可能性がないことを確認できます。

- **セキュリティ**: 管理者ユーザーのログインアカウントを知らなければ、ハッキングが難しくなります。rootはすでに管理者の認証情報の半分を持っていることになります。

- **本当に必要ない**: 複数のコマンドをrootとして実行する必要があり、`sudo` が期限切れになるたびにパスワードを何度も入力するのが面倒な場合は、`sudo -i` を実行すればrootになります。パイプを使ってコマンドを実行したい場合は、`sudo sh -c "command1 | command2"` を使用してください。

- **リカバリコンソールで使用することができます**: リカバリコンソールを使用すると、大きなミスから回復することができるほか、アプリケーション（`sudo` として実行する必要があったもの）によって引き起こされた問題を修正できます。Ubuntuでは、rootアカウントにパスワードは設定されていませんが、変更方法をオンラインで検索することができます。これにより、物理的にアクセスできる人が損害を与えるのが難しくなります。

有用なリソース：

- [Why is it bad to log in as root? (original)](https://askubuntu.com/questions/16178/why-is-it-bad-to-log-in-as-root)
- [What's wrong with always being root?](https://serverfault.com/questions/57962/whats-wrong-with-always-being-root)
- [Why you should avoid running applications as root](https://bencane.com/2012/02/20/why-you-should-avoid-running-applications-as-root/)

</details>

<details>
<summary><b>メモリ統計とCPU統計を確認するにはどうすればよいですか？</b></summary><br>

どちらも `top/htop` を使用します。物理メモリと仮想メモリの統計を表示するには、`free` と `vmstat` コマンドを使用します。CPUの使用率やその他の統計を確認するには、`sar` コマンドを使用します（ただし、`sar` は多くのシステムにはインストールされていません）。

有用なリソース：

- [How do I Find Out Linux CPU Utilization?](https://www.cyberciti.biz/tips/how-do-i-find-out-linux-cpu-utilization.html)
- [16 Linux server monitoring commands you really need to know](https://www.hpe.com/us/en/insights/articles/16-linux-server-monitoring-commands-you-really-need-to-know-1703.html)

</details>

<details>
<summary><b>ロードアベレージとは何ですか？</b></summary><br>

Linuxの**ロードアベレージ**は、「システムロードアベレージ」を示し、システム上の実行中のスレッド（タスク）の要求を、実行中および待機中のスレッドの平均数として表示します。これは、システムが現在処理している以上の要求を測定します。ほとんどのツールでは、1分、5分、15分の3つの平均が表示されます。

これらの3つの数字は、異なるCPUの数字ではありません。これらの数字は、指定された期間（直近の1分、5分、15分）のロード数の平均値です。

**ロードアベレージ**は通常「実行キューの平均長さ」として説明されます。したがって、少数のCPU消費プロセスやスレッドが**ロードアベレージ**を1を超えて上昇させる可能性があります。**ロードアベレージ**がCPUコアの総数未満であれば問題はありません。しかし、CPUの数を超えると、いくつかのスレッド/プロセスがキューに留まり、実行の準備が整っているが、空きCPUを待っていることを意味します。

これは、システムの状態をいくつかの期間にわたって平均化したものを示すためのものであり、平均化されているため、システムに重い負荷がかかった後は0に戻るのに時間がかかります。

いくつかの解釈：

- 平均が0.0の場合、システムはアイドル状態です。
- 1分間の平均が5分または15分の平均より高い場合、負荷が増加しています。
- 1分間の平均が5分または15分の平均より低い場合、負荷が減少しています。
- これらの数値がCPUの数を超える場合、パフォーマンスに問題があるかもしれません（依存します）。

有用なリソース：

- [Linux Load Averages: Solving the Mystery (original)](http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html)
- [Linux load average - the definitive summary](http://blog.angulosolido.pt/2015/04/linux-load-average-definitive-summary.html)
- [How CPU load averages work (and using them to triage webserver performance!)](https://jvns.ca/blog/2016/02/07/cpu-load-averages/)

</details>

<details>
<summary><b>Linux/Unixでは、私のパスワードはどこに保存されていますか？</b></summary><br>

パスワードはシステム上にはどこにも保存されていません。`/etc/shadow` に保存されているのは、いわゆるパスワードのハッシュです。

テキスト（パスワード）のハッシュは、テキストに対していわゆる一方向関数を実行することで作成され、チェック用の文字列が生成されます。設計上、このプロセスを逆にすることは「不可能」（計算上困難）です。

古いUnixのバリアントでは、暗号化されたパスワードが `/etc/passwd` に保存されており、各アカウントに関するその他の情報も含まれていました。

新しいバリアントでは、関連するフィールドに `*` があり、パスワードを保存するために `/etc/shadow` を使用します。これは、他の情報だけが必要な場合にパスワードへの読み取りアクセスを誰も得られないようにするためです（`shadow` は通常、`passwd` よりも強く保護されています）。

詳細については、`man crypt`、`man shadow`、`man passwd` を参照してください。

有用なリソース：

- [Where is my password stored on Linux?](https://security.stackexchange.com/questions/37050/where-is-my-password-stored-on-linux)
- [Where are the passwords of the users located in Linux?](https://www.cyberciti.biz/faq/where-are-the-passwords-of-the-users-located-in-linux/)
- [Linux Password & Shadow File Formats](https://www.tldp.org/LDP/lame/LAME/linux-admin-made-easy/shadow-file-formats.html)

</details>

<details>
<summary><b>すべてのディレクトリのパーミッションを変更するにはどうすればよいですか？ファイルを除いてすべてのディレクトリに、ファイルを除いてすべてのファイルに適用するにはどうすればよいですか？</b></summary><br>

すべてのディレクトリのパーミッションを例えば **755**（`drwxr-xr-x`）に変更するには：

```bash
find /path/to/directory -type d -exec chmod 755 {} +

```bash
find /opt/data -type d -exec chmod 755 {} \;
```

すべてのファイルのパーミッションを例えば **644**（`-rw-r--r--`）に変更するには：

```bash
find /opt/data -type f -exec chmod 644 {} \;
```

役立つリソース：

- [フォルダおよびそのサブフォルダとファイルすべてに対してchmodを設定するには？（原文）](https://stackoverflow.com/questions/3740152/how-do-i-set-chmod-for-a-folder-and-all-of-its-subfolders-and-files?rq=1)

</details>

<details>
<summary><b>すべてのコマンドが <code>command not found</code> で失敗します。エラーの原因を特定して解決するにはどうすればよいですか？</b></summary><br>

この問題は、`PATH` 環境変数がどこかで上書きされていることが原因です。エラーの内容から、`PATH` に `/bin` など、コマンド（bash を含む）が存在するディレクトリが含まれていないことが示唆されます。

デバッグを始める一つの方法は、`-x` オプションを使ってサブシェルを起動することです：

```bash
bash --login -x
```

これにより、そのシェルを起動する際に実行されるすべてのコマンドとその引数が表示されます。

また、`PATH` 変数の値を表示することも非常に役立ちます：

```bash
echo $PATH
```

次のコマンドを実行すると：

```bash
PATH=/bin:/sbin:/usr/bin:/usr/sbin
```

ほとんどのコマンドが動作するようになります。その後、~/.bash_profile を編集し、~/.bashrc ではなく、PATH をリセットしている原因を修正します。root および他のユーザーのデフォルトの PATH 変数の値は /etc/profile ファイルに記載されています。

有用なリソース：

- [PATHにパスを正しく追加するにはどうすればよいですか？](https://unix.stackexchange.com/questions/26047/how-to-correctly-add-a-path-to-path)

</details>

<details>
<summary><b><code>CTRL + C</code> を押してもスクリプトがまだ実行中です。どうやって停止しますか？</b></summary><br>

ほとんどの場合、スクリプトを停止するには `CTRL + C` キーボードコンビネーションを使用します。これにより、スクリプトに中断信号（SIGINT）が送信され、その実行が終了します。これが機能しない場合、スクリプトがまだ実行中である場合は、`CTRL + \` コンビネーションを使用してみてください。これにより、スクリプトに終了信号（SIGQUIT）が送信され、即座に終了する可能性があります。

別の方法として、ターミナルやコマンドラインインターフェースを使用している場合は、`kill` コマンドを使ってスクリプトプロセスに信号を送信することもできます。`ps` または `top` コマンドを使用してスクリプトのプロセスID（PID）を見つけ、`kill` コマンドでそのPIDを指定してスクリプトを停止します。

場合によっては、`kill -9` コマンドを使用してスクリプトを強制的に停止する必要があるかもしれません。通常の `kill` コマンドがスクリプトがスタックしている場合や応答していない場合に機能しないことがあります。`-9` オプションはSIGKILL信号を送信し、プロセスを即座に停止させます。

</details>

<details>
<summary><b><code>grep</code> コマンドとは何ですか？同じ行に複数の文字列を一致させるにはどうすればよいですか？</b></summary><br>

`grep` ユーティリティは、`egrep` や `fgrep` を含むUnixツールのファミリーです。

`grep` はファイルパターンを検索します。他のコマンドの出力の中から特定のパターンを探している場合、`grep` は関連する行を強調表示します。ログファイルや特定のプロセスなどを検索する際にこの `grep` コマンドを使用します。

複数の文字列を一致させるには：

```bash
grep -E "string1|string2" filename
```

または

```bash
grep -e "string1" -e "string2" filename
```

有用なリソース：

- [grepとは何で、どうやって使うのか？](https://kb.iu.edu/d/afiy)

</details>

<details>
<summary><b>ファイルコンテンツコマンドの説明とその説明をしてください。</b></summary><br>

- `head`: ファイルの先頭部分を確認するために使用します。
- `tail`: ファイルの末尾部分を確認するために使用します。`head` コマンドの逆です。
- `cat`: ファイルを表示、作成、結合するために使用します。
- `more`: テキストを端末ウィンドウにページ形式で表示するために使用します。
- `less`: テキストを逆方向に表示し、また単一行の移動も提供します。

有用なリソース：

- [シェルプロンプトからテキストファイルを表示する方法](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/4/html/Step_by_Step_Guide/s1-viewingtext-terminal.html)

</details>

<details>
<summary><b>SIGHUP、SIGINT、SIGKILL、および SIGTERM の POSIX シグナルについて説明してください。</b></summary><br>

- **SIGHUP** - 制御端末が閉じられたときにプロセスに送信されます。もともとはシリアルラインの切断（ハングアップ）をプロセスに通知するために設計されました。このシグナルを受け取ると、多くのデーモンは終了する代わりに設定ファイルを再読み込みし、ログファイルを再オープンします。
- **SIGINT** - ユーザーがプロセスを中断したいときに、その制御端末からプロセスに送信されます。通常は `Ctrl+C` を押すことで開始されますが、一部のシステムでは「削除」文字や「ブレーク」キーも使用できます。
- **SIGKILL** - プロセスを即座に終了させるために送信されます（強制終了）。**SIGTERM** や **SIGINT** とは異なり、このシグナルは捕捉または無視することができず、受信したプロセスはクリーンアップを行うことができません。
- **SIGTERM** - プロセスに終了を要求するために送信されます。**SIGKILL** シグナルとは異なり、捕捉して解釈したり無視したりすることができます。これにより、プロセスはリソースを解放し、状態を保存するなどのクリーンな終了を行うことができます。**SIGINT** は **SIGTERM** にほぼ同じです。

有用なリソース：

- [POSIXシグナル](https://dsa.cs.tsinghua.edu.cn/oj/static/unix_signal.html)
- [UNIXシグナルプログラミングの紹介](http://titania.ctie.monash.edu.au/signals/)

</details>

<details>
<summary><b><code>kill</code> コマンドは何をするのですか？</b></summary><br>

UNIX および UNIX 系オペレーティングシステムでは、`kill` はプロセスにシグナルを送信するためのコマンドです。デフォルトでは、送信されるメッセージは終了シグナルであり、プロセスに終了を要求します。しかし、`kill` という名前は誤解を招くことがあり、送信されるシグナルがプロセスの終了に関連していない場合もあります。

有用なリソース：

- [Linuxでの「kill」コマンドの習得](https://www.maketecheasier.com/kill-command-in-linux/)

</details>

<details>
<summary><b><code>rm</code> と <code>rm -rf</code> の違いは何ですか？</b></summary><br>

`rm` は指定されたファイルのみを削除します（ディレクトリは削除しません）。`-rf` を付けると、次のようになります：

- `-r`、`-R`、`--recursive` はディレクトリの内容を再帰的に削除します。これには隠しファイルやサブディレクトリも含まれます。
- `-f`、`--force` は存在しないファイルを無視し、確認を行いません。

有用なリソース：

- [`rm -r` と `rm -f` の違いは何ですか？](https://superuser.com/questions/1126206/what-is-the-difference-between-rm-r-and-rm-f)

</details>

<details>
<summary><b><code>grep</code> を再帰的に実行する方法は？いくつかの例で説明してください。</b></summary>

完了次第追加します。

</details>

<details>
<summary><b><code>archive.tgz</code> は約 30 GB です。このファイルの内容をリスト表示し、特定のファイルだけを抽出するにはどうすればよいですか？</b></summary><br>

```bash
# 内容のリスト表示
tar tf archive.tgz

# ファイルの解凍
tar xf archive.tgz filename
```

有用なリソース：

- [tar または tar.gz ファイルの内容をリストする方法](https://www.cyberciti.biz/faq/list-the-contents-of-a-tar-or-targz-file/)
- [tar.gz から特定のファイルを抽出する方法](https://unix.stackexchange.com/questions/61461/how-to-extract-specific-files-from-tar-gz)

</details>

<details>
<summary><b>複数のシェルコマンドを1行で実行する</b></summary><br>

前のコマンドが成功した場合にのみ次のコマンドを実行したい場合は、`&&`演算子を使用して結合します：

```bash
cd /my_folder && rm *.jar && svn co path to repo && mvn compile package install
```

コマンドのうち1つが失敗すると、それに続く他のすべてのコマンドは実行されません。

前のコマンドが失敗したかどうかに関係なく、すべてのコマンドを実行したい場合は、セミコロンで区切ります：

```bash
cd /my_folder; rm *.jar; svn co path to repo; mvn compile package install
```

この場合、次のコマンドの実行が前のコマンドの成功に依存する最初のケースが望ましいと思われます。

また、すべてのコマンドをスクリプトにまとめて、それを実行することもできます：

```bash
#! /bin/sh
cd /my_folder \
&& rm *.jar \
&& svn co path to repo \
&& mvn compile package install
```

有用なリソース:

- [複数のLinuxコマンドを1行で結合して実行する方法（オリジナル）](https://stackoverflow.com/questions/13077241/execute-combine-multiple-linux-commands-in-one-line)

</details>

<details>
<summary><b>他の権限に影響を与えずに、すべてのユーザーにファイルの実行アクセスを付与するために <code>chmod</code> に渡すことができる記号表現は何ですか？</b></summary><br>

```bash
chmod a+x /path/to/file
```

- `a` - すべてのユーザーに適用
- `x` - 実行権限
- `r` - 読み取り権限
- `w` - 書き込み権限

有用なリソース:
- [chmod を使用してファイル権限を設定する方法](https://www.washington.edu/computing/unix/permissions.html)
- [「chmod +x your_file_name」とは何をするコマンドで、どのように使用するのですか？](https://askubuntu.com/questions/443789/what-does-chmod-x-filename-do-and-how-do-i-use-it)

</details>

<details>
<summary><b>2つのローカルディレクトリを同期するにはどうすればよいですか？</b></summary><br>

同じシステム上で **dir1** の内容を **dir2** に同期するには、次のコマンドを入力します:

```bash
rsync -av --progress --delete dir1/ dir2
```

- `-a`, `--archive` - アーカイブモード
- `--delete` - 余分なファイルを宛先ディレクトリから削除
- `-v`, `--verbose` - 詳細モード（冗長性を高める）
- `--progress` - 転送中に進行状況を表示

参考資料:

- [ローカルディレクトリを同期する方法 (オリジナル)](https://unix.stackexchange.com/questions/392536/how-can-i-sync-two-local-directories)
- [rsyncでフォルダを同期する](https://www.jveweb.net/en/archives/2010/11/synchronizing-folders-with-rsync.html)

</details>

<details>
<summary><b>多くの基本的なメンテナンスタスクでは、設定ファイルの編集が必要です。変更を元に戻す方法を説明してください。</b></summary><br>

- 編集前にファイルを手動でバックアップする（このようにブレース展開を使用: `cp filename{,.orig}`）
- ファイルが保存されているディレクトリ構造を手動でコピーする（例: `cp`、`rsync`、または `tar` を使用）
- 使用しているエディターで元のファイルをバックアップする（例: エディターの設定ファイルにルールを設定する）
- 最良の解決策は、`git`（または他のバージョン管理ツール）を使用して設定ファイルを管理することです（例: `/etc` ディレクトリに `etckeeper` を使用）

参考資料:

- [拡張子の前に .bak でファイルをバックアップする](https://unix.stackexchange.com/questions/66376/backup-file-with-bak-before-filename-extension)
- [設定ファイルのバージョン管理に git を使用するのは良い考えですか？](https://superuser.com/questions/1037211/is-it-a-good-idea-to-use-git-for-configuration-file-version-controlling)

</details>

<details>
<summary><b>20MBを超えるすべてのファイルを見つける必要があります。どうやってやりますか？</b></summary><br>

```bash
find / -type f -size +20M
```

参考資料:

- [xバイトより大きい/小さいファイルを見つけるにはどうすればよいですか？](https://superuser.com/questions/204564/how-can-i-find-files-that-are-bigger-smaller-than-x-bytes)

</details>

<details>
<summary><b>なぜ <code>sudo su -</code> を使うのか、ただの <code>sudo su</code> ではない理由は？</b></summary><br>

`sudo` はほとんどの最新のLinuxディストリビューションで使用されており、（常にではありませんが）rootユーザーが無効化され、パスワードが設定されていません。そのため、`su` でrootユーザーに切り替えることはできません（試してみてもよいでしょう）。root権限で `sudo` を呼び出す必要があります：`sudo su`。

`su` は単にユーザーを切り替えるだけで、通常のシェルが起動し、以前のユーザーとほぼ同じ環境が提供されます。

`su -` は、ユーザーを切り替えた後にログインシェルを起動します。ログインシェルは、ほとんどの環境変数をリセットし、クリーンな状態を提供します。

参考資料:

- [su vs sudo -s vs sudo -i vs sudo bash](https://unix.stackexchange.com/questions/35338/su-vs-sudo-s-vs-sudo-i-vs-sudo-bash)
- [なぜ su - を使い、ただの su を使わないのか？(オリジナル)](https://unix.stackexchange.com/questions/7013/why-do-we-use-su-and-not-just-su)

</details>

<details>
<summary><b>過去60分以内にシステム上で変更されたファイルを見つける方法は？</b></summary><br>

```bash
find / -mmin -60 -type f
```

参考資料:

- [ディレクトリ内で過去30日間に変更されたすべてのファイルを取得する（オリジナル）](https://stackoverflow.com/questions/23070245/get-all-files-modified-in-last-30-days-in-a-directory)

</details>

<details>
<summary><b>古いログファイルを保持する主な理由は何ですか？</b></summary><br>

それらは、システム上の問題を調査するために不可欠です。**ログ管理**は、ITセキュリティにとって非常に重要です。

サーバー、ファイアウォール、その他のIT機器は、重要なイベントや取引を記録するログファイルを保持します。この情報は、内部および外部からのネットワークに影響を与える敵対的な活動に関する重要な手がかりを提供する可能性があります。ログデータは、構成問題やハードウェアの故障などの機器の問題を特定し、トラブルシューティングするための情報も提供できます。

これは、誰があなたのサイトに来て、いつ、そして正確に何を見たかを記録するサーバーの記録です。それは非常に詳細で、以下の内容を示します。

- 訪問者の出身地
- 使用しているブラウザ
- 正確にどのファイルを見たか
- 各ファイルの読み込みにかかった時間
- その他多くの技術的な情報

考慮すべき要素:

- 保持や破棄に関する法的要件
- 会社の保持および破棄に関するポリシー
- ログの有用性の期間
- ログから解決したい質問
- ログが占めるスペースの量

ログを収集して分析することで、ネットワーク内で何が起こっているかを理解できます。各ログファイルには、多くの情報が含まれており、それを読み取り、分析する方法を知っていれば非常に貴重です。

参考資料:

- [ログファイルをどのくらいの期間保持しますか？](https://serverfault.com/questions/135365/how-long-do-you-keep-log-files)

</details>

<details>
<summary><b>増分バックアップとは何ですか？</b></summary><br>

増分バックアップは、前回のバックアップ以降に変更されたファイルのみをコピーするバックアップの一種です。

参考資料:

- [増分バックアップとは？](https://www.nakivo.com/blog/what-is-incremental-backup/)

</details>

<details>
<summary><b>RAIDとは何ですか？RAID0、RAID1、RAID5、RAID6、RAID10とは？</b></summary><br>

**RAID**（Redundant Array of Inexpensive Disks）は、データストレージのパフォーマンスや信頼性を向上させるための技術です。

- **RAID0**: ディスクの**ストライピング**とも呼ばれ、ファイルを分割してRAIDグループ内のすべてのディスクドライブにデータを分散させる技術です。故障に対する保護はありません。
- **RAID1**: 安全性を高めるために、同じデータを2つのドライブに書き込む一般的なディスクサブシステムです。**ミラーリング**とも呼ばれ、書き込みパフォーマンスの向上はありませんが、読み取りパフォーマンスは各ディスクのパフォーマンスの合計に匹敵する場合があります。ただし、1つのドライブが故障した場合、2番目のドライブが使用され、故障したドライブは手動で交換されます。交換後、RAIDコントローラが作業中のドライブの内容を新しいドライブに複製します。
- **RAID5**: パリティデータを計算し、安全性を高めると同時に、データを3つ以上のドライブにインターリーブして速度を向上させるディスクサブシステムです（**ストライピング**）。1つのドライブが故障した場合、分散パリティから計算され、データが失われることはありません。
- **RAID6**: RAID 6は、RAID 5を拡張し、もう1つのパリティブロックを追加します。最小4つのディスクが必要で、2つのディスクが同時に故障しても読み書きが継続できます。RAID 6は、読み取り操作にはパフォーマンスのペナルティがありませんが、パリティ計算に関連するオーバーヘッドのため、書き込み操作にはパフォーマンスのペナルティがあります。
- **RAID10**: **RAID 1+0**とも呼ばれ、ディスクのミラーリングとストライピングを組み合わせてデータを保護するRAID構成です。最小4つのディスクが必要で、ミラーリングされたペアにデータをストライプします。各ミラーリングペアのうち1つのディスクが機能していれば、データを取り出すことができます。ただし、同じミラーリングペアの2つのディスクが故障した場合、ストライプセットにはパリティがないため、すべてのデータが失われます。

参考資料:

- [RAID](https://www.prepressure.com/library/technology/raid)

</details>

<details>
<summary><b>ユーザーのデフォルトグループはどのように決定されますか？また、それをどのように変更しますか？</b></summary><br>

```bash
useradd -m -g initial_group username
```

`-g/--gid`: ユーザーの初期ログイングループのグループ名または番号を定義します。指定されている場合、グループ名は存在する必要があり、グループ番号が提供される場合は、既存のグループを指している必要があります。

指定されていない場合、`useradd`の動作は`/etc/login.defs`に含まれる`USERGROUPS_ENAB`変数に依存します。デフォルトの動作（`USERGROUPS_ENAB yes`）は、ユーザー名と同じ名前のグループを作成し、**GID**を**UID**と同じにします。

参考資料:

- [Linuxでユーザーのデフォルトグループを変更する方法](https://unix.stackexchange.com/questions/26675/how-can-i-change-a-users-default-group-in-linux)

</details>

<details>
<summary><b>日常作業やスクリプト作成に最適なコマンドラインテキストエディタは何ですか？</b></summary><br>

未完成です。

</details>

<details>
<summary><b>サーバーをラックにマウントする理由は何ですか？</b></summary><br>

- ハードウェアの保護
- 適切な冷却
- 整理された作業スペース
- より良い電力管理
- クリーンな環境

参考資料:

- [5 Reasons to Rackmount Your PC](https://www.racksolutions.com/news/custom-projects/5-reasons-to-rackmount-pc/)

</details>

###### ネットワークの質問 (23)

<details>
<summary><b>シンプルなネットワーク図を描いてください: 20台のシステム、1台のルーター、4台のスイッチ、5台のサーバー、小さなIPブロックがあります。</b></summary><br>

未完成です。

</details>

<details>
<summary><b>OSIモデル（または他のモデル）について理解しておくべき最も重要なことは何ですか？</b></summary><br>

**OSI**（または他の）モデルについて理解しておくべき最も重要なことは以下です：

- プロトコルを層に分けることができる
- 層がカプセル化を提供する
- 層が抽象化を提供する
- 層が他の機能からの分離を提供する

参考資料:

- [OSIモデルとネットワーキングプロトコルの関係](https://networkengineering.stackexchange.com/questions/6380/osi-model-and-networking-protocols-relationship)

</details>

<details>
<summary><b>VLANとサブネットの違いは何ですか？サブネットを設定するにはVLANが必要ですか？</b></summary><br>

**VLAN**と**サブネット**は異なる問題を解決します。**VLAN**はレイヤー2で動作し、例えばブロードキャストドメインを変更します。一方、**サブネット**は現在の文脈ではレイヤー3です。

**サブネット** - IPアドレスの範囲で、アドレスの一部（通常「ネットワークアドレス」と呼ばれる）とサブネットマスク（ネットマスク）によって決まります。例えば、ネットマスクが `255.255.255.0`（または短縮形で `/24`）で、ネットワークアドレスが `192.168.10.0` の場合、それはIPアドレスの範囲 `192.168.10.0` から `192.168.10.255` を定義します。これを短縮形で書くと `192.168.10.0/24` です。

**VLAN** - これを「スイッチの分割」と考えると良いでしょう。例えば、VLAN対応の8ポートスイッチがあるとします。4つのポートを1つの**VLAN**（例えば `VLAN 1`）に割り当て、残りの4つのポートを別の**VLAN**（例えば `VLAN 2`）に割り当てます。`VLAN 1`は`VLAN 2`のトラフィックを見ず、逆も同様です。論理的には、2つの別々のスイッチがあります。通常、スイッチがMACアドレスを見ていない場合、そのトラフィックは他のすべてのポートに「フラッディング」されます。**VLAN**はこれを防ぎます。

サブネットは、レイヤー2とレイヤー3でホストが通信するのを助けるIPアドレスの範囲に過ぎません。各サブネットは独自の**VLAN**を必要としません。**VLAN**は隔離（レイヤー2通信のサンドボックスであり、異なる**VLAN**の2つのシステムが通信できないが、**インターヴィLANルーティング**を通じて通信できる）、管理の容易さ、セキュリティのために実装されます。

参考資料:

- [VLANとサブネットの違いは何ですか？](https://superuser.com/questions/353664/what-is-the-difference-between-a-vlan-and-a-subnet)
- [ネットワークセキュリティとセグメンテーションのためのVLANSとサブネット](https://networkengineering.stackexchange.com/questions/46899/vlans-vs-subnets-for-network-security-and-segmentation)

</details>

<details>
<summary><b>知っておくべき一般的なネットワークポートを5つ挙げてください。</b></summary><br>

<table style="width:100%">
  <tr>
    <th>サービス</th>
    <th>ポート</th>
  </tr>
  <tr>
    <td>SMTP</td>
    <td>25</td>
  </tr>
  <tr>
    <td>FTP</td>
    <td>データ転送用が20、接続確立用が21</td>
  </tr>
  <tr>
    <td>DNS</td>
    <td>53</td>
  </tr>
  <tr>
    <td>DHCP</td>
    <td>DHCPサーバー用が67/UDP、DHCPクライアント用が68/UDP</td>
  </tr>
  <tr>
    <td>SSH</td>
    <td>22</td>
  </tr>
</table>

参考資料:

- [Red Hat Enterprise Linux 4: セキュリティガイド - 一般的なポート](https://web.mit.edu/rhel-doc/4/RH-DOCS/rhel-sg-en-4/ch-ports.html)

</details>

<details>
<summary><b>POPとIMAPとは何ですか？また、どちらを実装すべきかの選び方は？</b></summary><br>

POPとIMAPは、メールサーバーからメールクライアントにメッセージを取得するためのプロトコルです。

**POP** (_Post Office Protocol_) は、メールサーバーからクライアントへの一方向のプッシュを使用します。デフォルトでは、メッセージはPOPメールクライアントに送信され、メールサーバーからは削除されますが、メールサーバーがすべてのメッセージを保持するように設定することも可能です。メールクライアントでメッセージに対して行った操作（ラベル付け、削除、フォルダへの移動）は、メールサーバーには反映されず、他のメールクライアントからはアクセスできません。POPはメールサーバーのストレージスペースをほとんど使用せず、メッセージがメールサーバーや複数のクライアントではなく、1つのメールクライアントにのみ存在するため、より安全と見なされることがあります。

**IMAP** (_Internet Message Access Protocol_) は、メールサーバーとクライアント間で双方向の通信を使用します。IMAPで設定されたメールクライアントでメッセージを削除またはラベル付けすると、その操作はメールサーバーにも反映されます。IMAPは、メッセージが複数のデバイスで同じ状態で存在できるため、異なるクライアントやデバイスでのメールアクセス時に似たような体験を提供します。また、IMAPはメッセージを選択的に同期させることができ、クライアントから古いメッセージを削除し、必要に応じてメールサーバーから再同期できます。

複数のデバイスでメッセージにアクセスする必要があり、クライアントデバイスのディスクスペースを節約したい場合はIMAPを選択してください。メールサーバーのディスクスペースを節約し、1つのクライアントデバイスからのみメッセージにアクセスし、メッセージが複数のシステムに存在しないことを確認したい場合はPOPを選択してください。

</details>

<details>
<summary><b>デフォルトルートとルーティングテーブルを確認する方法は？</b></summary><br>

`netstat -nr`、`route -n`、または `ip route show` コマンドを使用することで、デフォルトルートとルーティングテーブルを確認できます。

参考資料:

- [Linuxでのルート（ルーティングテーブル）の確認方法](https://howto.lintel.in/how-to-check-routes-routing-table-in-linux/)
- [FreeBSDのデフォルトルート/ゲートウェイの設定](https://www.cyberciti.biz/faq/freebsd-setup-default-routing-with-route-command/)

</details>

<details>
<summary><b>127.0.0.1とlocalhostの違いは何ですか？</b></summary><br>

最も可能性の高い違いは、`localhost` の実際のルックアップがどこかで行われる必要があるということです。

`127.0.0.1` を使用する場合、（知能を持った）ソフトウェアはそれを直接IPアドレスに変換して使用します。`gethostbyname` の実装によっては、ドット形式（およびおそらく同等のIPv6形式）を検出し、ルックアップを行わないこともあります。

それ以外の場合、名前の解決が必要です。そして、ホストファイルがその解決に実際に使用される保証はありません（最初に、またはまったく）。そのため、`localhost` がまったく異なるIPアドレスになる可能性があります。

つまり、いくつかのシステムでは、ローカルホストファイルがバイパスされることがあります。Linuxでは、`host.conf` ファイルがこれを制御します（および多くの他のUnix系システムでも）。

Unixドメインソケットを使用すると、TCP/IPを使用するよりもわずかに高速になります（オーバーヘッドが少ないため）。WindowsはデフォルトでTCP/IPを使用しますが、Linuxは `localhost` を選択した場合にUnixドメインソケットを使用し、`127.0.0.1` を選択した場合にTCP/IPを使用しようとします。

参考資料:

- [127.0.0.1とlocalhostの違いは？](https://stackoverflow.com/questions/7382602/what-is-the-difference-between-127-0-0-1-and-localhost)
- [localhostと127.0.0.1](https://stackoverflow.com/questions/3715925/localhost-vs-127-0-0-1)

</details>

<details>
<summary><b><code>ping</code> コマンドに使用されるポートはどれですか？</b></summary><br>

`ping` は **ICMP** を使用します。具体的には **ICMPエコーリクエスト** と **ICMPエコーリプライ** パケットです。**ICMP** にはポートが関連付けられていません。ポートは、TCP と UDP という二つの IP トランスポート層プロトコルに関連しています。**ICMP**、TCP、UDP は「兄弟」であり、互いに基づいているわけではなく、IP の上で動作する三つの異なるプロトコルです。

**ICMP** パケットは、IP データグラムヘッダー内の「プロトコル」フィールドによって識別されます。**ICMP** は UDP や TCP の通信サービスを使用せず、生の IP 通信サービスを使用します。これは、**ICMP** メッセージが IP データグラムのデータフィールド内に直接運ばれることを意味します。`raw` という用語は、この処理がソフトウェアでどのように実装されているかに由来します。**ICMP** メッセージを作成して送信するためには、`raw` ソケットを開き、**ICMP** メッセージを含むバッファを構築し、そのバッファを `raw` ソケットに書き込みます。

**ICMP** の IP プロトコル値は 1 です。プロトコルフィールドは IP ヘッダーの一部で、IP データグラムのデータ部分に何が含まれているかを識別します。

ただし、ポートが開いているかどうかを確認するには、`nmap` を使用することができます：

```bash
nmap -p 80 example.com
```

参考資料:

- [Pingのポート番号](https://networkengineering.stackexchange.com/questions/42463/ping-port-number)
- [アドレス:ポートをpingすることは可能か？](https://superuser.com/questions/769541/is-it-possible-to-ping-an-addressport)

</details>

<details>
<summary><b>サーバーAがサーバーBと通信できない場合、考えられる原因をいくつかのステップで説明してください。</b></summary><br>

サーバー間の通信問題をトラブルシューティングするためには、TCP/IPスタックに従うのが理想的です：

1. **アプリケーション層**: 両方のサーバーでサービスは稼働していますか？サービスは正しく設定されていますか（例: 正しいIPアドレスとポートにバインドされているか）？アプリケーションおよびシステムログに意味のあるエラーが表示されていますか？

2. **トランスポート層**: アプリケーションが使用しているポートは開いていますか（telnetで確認してみてください）？サーバーにpingを送ることは可能ですか？

3. **ネットワーク層**: ネットワークまたはOSにファイアウォールが適切に設定されていますか？IPスタックが正しく設定されていますか（IPアドレス、ルート、DNSなど）？スイッチやルーターは正常に動作していますか（ARPテーブルを確認してください）？

4. **物理層**: サーバーはネットワークに接続されていますか？パケットが失われていませんか？

</details>

<details>
<summary><b>ホスト名がサーバー上で解決されないのはなぜですか？この問題を解決してください。***</b></summary><br>

詳細は未完成です。

</details>

<details>
<summary><b>ドメイン名をCLIで解決する方法（外部DNSを使用）と、IPアドレスをドメイン名に解決できるかどうか？</b></summary><br>

IPアドレスをドメイン名に解決する例:

```bash
# host コマンドを使用する場合:
host domain.com 8.8.8.8

# dig コマンドを使用する場合:
dig @9.9.9.9 google.com

# nslookup コマンドを使用する場合:
nslookup domain.com 8.8.8.8
```

IPアドレスをホスト名に解決できる場合があります。IPアドレスは**PTR**レコードに格納されていることがあります。その場合、次のコマンドを使用できます:

```bash
dig A <hostname>
```

ホストのIPv4アドレスを調べるには、次のコマンドを使用します:

```bash
dig AAAA <hostname>
```

ホストのIPv6アドレスを調べるには、次のコマンドを使用します:

```bash
dig PTR ZZZ.YYY.XXX.WWW.in-addr.arpa.
```

IPv4アドレス WWW.XXX.YYY.ZZZ のホスト名を調べるには（オクテットが逆順になっている点に注意）、次のようにします:

```bash
dig PTR b.a.9.8.7.6.5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa.
```

役立つリソース

- [Bashスクリプトでホスト名をIPアドレスに解決する方法](https://unix.stackexchange.com/questions/20784/how-can-i-resolve-a-hostname-to-an-ip-address-in-a-bash-script)
- [IPアドレスをドメイン名に解決する方法](https://superuser.com/questions/315687/how-to-resolve-ip-addresses-to-domain-names)

</details>

<details>
<summary><b>ポートの接続性を<code>telnet</code>または<code>nc</code>でテストする方法は？</b></summary><br>

```bash
# telnetコマンドを使用する場合：
telnet code42.example.com 5432

# nc (netcat)コマンドを使用する場合：
nc -vz code42.example.com 5432
```
</details>

<details>
<summary><b>なぜリモートでシステムを管理する際に<code>telnet</code>を避けるべきですか？</b></summary><br>

現代のオペレーティングシステムは、デフォルトで潜在的に安全でないサービスをすべてオフにしています。一方で、ネットワーク機器の一部のベンダーは、telnetプロトコルを使用して通信を確立することを許可しています。

**Telnet** は、最も安全でない通信方法を使用します。ネットワーク上でデータをプレーンテキスト形式で送信し、誰でもネットワークツールを使ってパスワードを簡単に見つけることができます。

**Telnet** の場合、ログイン認証情報がプレーンテキストで送信されるため、ネットワーク上でスニファーを実行している誰でも、**Telnet** のログインセッションを傍受することで数秒でデバイスを制御するために必要な情報を見つけることができます。

有用なリソース：

- [TelnetとSSHのセキュアな代替手段](https://www.ssh.com/ssh/telnet)
- [特定のポートでIPアドレスにtelnetする方法](https://superuser.com/questions/339107/how-to-telnet-to-an-ip-address-on-a-specific-port)

</details>

<details>
<summary><b><code>wget</code>と<code>curl</code>の違いは何ですか？</b></summary><br>

主な違いは次の通りです：`wget` の大きな強みは、再帰的にダウンロードできることです。`wget` はコマンドライン専用です。`curl` はFTP、FTPS、HTTP、HTTPS、SCP、SFTP、TFTP、TELNET、DICT、LDAP、LDAPS、FILE、POP3、IMAP、SMTP、RTMP、RTSPをサポートしています。

有用なリソース：

- [curlとwgetの違いは？ (原文)](https://unix.stackexchange.com/questions/47434/what-is-the-difference-between-curl-and-wget)

</details>

<details>
<summary><b>SSHとは何ですか？どのように動作しますか？</b></summary><br>

**SSH** は **Secure Shell** の略です。サーバー「A」からサーバー「B」へのシェルセッションを開始するためのプロトコルです。サーバー「B」と対話することができます。

**SSH** 接続を確立するには、リモートマシン（サーバーA）が **SSH** デーモンというソフトウェアを実行している必要があり、ユーザーのコンピュータ（サーバーB）には **SSH** クライアントが必要です。

**SSH** デーモンと **SSH** クライアントは、特定のネットワークポート（デフォルトは22）で接続を待ち受け、接続要求を認証し、ユーザーが正しい認証情報を提供した場合に適切な環境を生成します。

有用なリソース：

- [SSHの暗号化と接続プロセスの理解](https://www.digitalocean.com/community/tutorials/understanding-the-ssh-encryption-and-connection-process)

</details>

<details>
<summary><b>ほとんどのチュートリアルは、パスワード認証よりもSSHキー認証を使用することを推奨しています。なぜそれがより安全とされるのでしょうか？</b></summary><br>

**SSHキー** は、SSHプロトコルでのアクセス認証情報です。その機能はユーザー名やパスワードに似ていますが、キーは主に自動化プロセスやシステム管理者、パワーユーザーによるシングルサインオンの実装に使用されます。

ユーザーのパスワードを要求する代わりに、公開鍵と秘密鍵を使用して非対称暗号化アルゴリズムでクライアントの識別を確認することができます。

SSHサービスが公開鍵認証のみを許可する場合、攻撃者はサーバーに保存されている公開鍵に対応する秘密鍵のコピーが必要です。

SSHサービスがパスワードベースの認証を許可する場合、インターネットに接続されたSSHサーバーは、ユーザー名やパスワードを推測しようとするボットネットに昼夜問わず攻撃されます。ボットネットは情報を必要とせず、人気のある名前やパスワードを試すだけで済みます。これにより、ログが詰まることもあります。

有用なリソース：

- [鍵ベース認証（公開鍵認証）](http://www.crypto-it.net/eng/tools/key-based-authentication.html)
- [SSHのパスワード認証と鍵認証](https://security.stackexchange.com/questions/33381/ssh-password-vs-key-authentication)

</details>

<details>
<summary><b>パケットフィルタとは何ですか？どのように機能しますか？</b></summary><br>

**パケットフィルタリング** は、ネットワークアクセスを制御するためのファイアウォール技術で、送信および受信するパケットを監視し、ソースおよびデスティネーションのインターネットプロトコル（IP）アドレス、プロトコル、ポートに基づいて通過させるか停止させるかを決定します。

パケットフィルタリングは、セキュリティ要件がそれほど高くない場合に適しています。多くの組織の内部（プライベート）ネットワークは、あまりセグメント化されていません。一部の組織から他の部分を分離するために高度に洗練されたファイアウォールは必要ありません。

ただし、実験ネットワークやラボから本番ネットワークを保護するために、何らかの保護手段を講じることは賢明です。パケットフィルタリングデバイスは、一つのサブネットから別のサブネットを分離するための非常に適切な手段です。

TCP/IPプロトコルスタックのネットワーク層およびトランスポート層で動作し、すべてのパケットがプロトコルスタックに入る際に検査されます。ネットワークおよびトランスポートヘッダーは、次の情報が含まれているかを詳しく調べます：

- **プロトコル（IPヘッダー、ネットワーク層）** - IPヘッダーのバイト9（バイトカウントは0から始まる）でパケットのプロトコルが識別されます。ほとんどのフィルターデバイスは、TCP、UDP、ICMPを区別する機能があります。
- **ソースアドレス（IPヘッダー、ネットワーク層）** - ソースアドレスは、パケットを生成したホストの32ビットIPアドレスです。
- **デスティネーションアドレス（IPヘッダー、ネットワーク層）** - デスティネーションアドレスは、パケットが送信されるホストの32ビットIPアドレスです。
- **ソースポート（TCPまたはUDPヘッダー、トランスポート層）** - TCPまたはUDPネットワーク接続の各端はポートにバインドされています。TCPポートはUDPポートとは異なります。ポート番号が1024未満のものは予約されており、特定の用途が定義されています。ポート番号が1024以上（含む）のものはエフェメラルポートとして知られており、ベンダーが自由に使用できます。「Well known」ポートのリストについては、RFP1700を参照してください。ソースポートは疑似ランダムに割り当てられたエフェメラルポート番号です。そのため、ソースポートでフィルタリングすることはあまり有用ではありません。
- **デスティネーションポート（TCPまたはUDPヘッダー、トランスポート層）** - デスティネーションポート番号は、パケットが送信されるポートを示します。デスティネーションホストの各サービスはポートをリッスンしています。フィルタリングされる可能性がある一般的なポートには、20/TCPおよび21/TCP（ftp接続/データ）、23/TCP（telnet）、80/TCP（http）、53/TCP（DNSゾーントランスファー）があります。
- **接続状態（TCPヘッダー、トランスポート層）** - 接続状態は、パケットがネットワークセッションの最初のパケットであるかどうかを示します。TCPヘッダーのACKビットが「false」または0に設定されている場合、これはセッションの最初のパケットです。ACKビットが「false」または0に設定されているパケットを拒否または破棄することで、ホストが接続を確立することを防ぐのは簡単です。

有用なリソース：

- [インターネットファイアウォールの構築 - パケットフィルタリング](http://web.deu.edu.tr/static/oreily/networking/firewall/ch06_01.htm)

</details>

<details>
<summary><b>リバースプロキシサーバーを使用する利点は何ですか？</b></summary><br>

**バックエンドサーバーのトポロジーと特性を隠す**

**リバースプロキシサーバー** は、オリジンサーバーの存在と特性を隠すことができます。インターネットクラウドとWebサーバーの間に中間者として機能します。特にWebホスティングサービスを使用している場合には、セキュリティ上の理由で良い選択です。

**バックエンドサーバーの透過的なメンテナンスを許可**

リバースプロキシの背後で動作するサーバーに対して行う変更は、エンドユーザーには完全に透過的です。

**ロードバランシング**

リバースプロキシは、ラウンドロビン、ウェイテッドラウンドロビン、最小接続数、ウェイテッド最小接続数、またはランダムなどのロードバランシングアルゴリズムを強制し、クラスタ内のサーバー間で負荷を分散します。

サーバーがダウンすると、システムは自動的に次のサーバーにフェイルオーバーし、ユーザーはセキュアなファイル転送活動を続けることができます。

**SSLオフロード/終了**

HTTPS接続を処理し、リクエストを復号化して、暗号化されていないリクエストをWebサーバーに渡します。

**IPマスキング**

単一のIPを使用して、異なるURLを異なるバックエンドサーバーにルーティングします。

有用なリソース：

- [リバースプロキシの利点](https://dzone.com/articles/benefits-reverse-proxy)

</details>

<details>
<summary><b>ルーターとゲートウェイの違いは何ですか？デフォルトゲートウェイとは何ですか？</b></summary><br>

**ルーター** は、一般的な技術機能（レイヤー3のフォワーディング）やその目的のために設計されたハードウェアデバイスを説明します。一方、ゲートウェイはローカルセグメントの機能（他の場所への接続を提供）を説明します。"_ルーターをゲートウェイとして設定する_" とも言えます。別の用語として、サブネット間のフォワーディングを説明する「ホップ」があります。

**デフォルトゲートウェイ** という用語は、LAN内でのルーターで、LANの外のコンピュータへのトラフィックの最初の接点としての責任を持つルーターを意味します。

これは視点の問題であり、デバイスは同じです。

有用なリソース：

- [ルーターとゲートウェイの違い (原文)](https://networkengineering.stackexchange.com/questions/51426/difference-between-router-and-gateway)

</details>

<details>
<summary><b>次のDNSレコードの機能を説明してください：SOA、PTR、A、MX、CNAME。</b></summary><br>

**DNSレコード** は基本的にマッピングファイルで、DNSサーバーにどのIPアドレスが各ドメインに関連付けられているか、各ドメインに送信されたリクエストをどのように処理するかを伝えます。よく使用される**DNSレコード** の構文には、`A`、`AAAA`、`CNAME`、`MX`、`PTR`、`NS`、`SOA`、`SRV`、`TXT`、`NAPTR` があります。

- **SOA** - 権威の開始
- **A** - アドレスマッピングレコード
- **AAAA** - IPバージョン6アドレスレコード
- **CNAME** - 正規名レコード
- **MX** - メール交換レコード
- **NS** - ネームサーバーレコード
- **PTR** - 逆引きポインターレコード

有用なリソース：

- [DNSレコードタイプのリスト](https://en.wikipedia.org/wiki/List_of_DNS_record_types)

</details>

<details>
<summary><b>なぜMACアドレスをIPv4/6の代わりにネットワーキングに使用できないのですか？</b></summary><br>

**OSI** モデルは、ルーティング（**レイヤー3** の概念）を物理的な**レイヤー2** メカニズムに基づいて決定するのがなぜ意味がないのかを説明します。

現代のネットワーキングは、エンドツーエンドの通信を達成するために多くの異なるレイヤーに分かれています。ネットワークカード（MACアドレス - 物理アドレスでアドレスされる）は、自分の物理ネットワーク上のピアと通信することだけを担当する必要があります。

**MAC** アドレスで達成できる通信は、自分の機械と物理的に接触している他のデバイスに制限されます。例えば、インターネットでは、各マシンと物理的に接続されているわけではありません。だからこそ、物理的に接続されていないマシンと通信する必要があるときには、**TCP/IP**（**レイヤー3** の論理アドレス）メカニズムを使用します。

**IP** は、コンピュータのグループに階層的に課された任意の番号付けスキームで、グループとして論理的に区別します（これがサブネットです）。これらのグループ間でメッセージを送信するのは、ルーティングテーブルを使用し、それ自体が複数のレベルに分かれているため、すべてのサブネットを追跡する必要がありません。

これを別のペアのシステムに関連付けるのは簡単です。あなたには州発行のID番号がありますが、そのID番号がすでにあなたに固有であるなら、なぜ郵送先住所が必要でしょうか？郵送先住所が必要なのは、唯一の通信先がどこにあるかを説明するための任意のシステムだからです。

一方、ネットワーク全体にわたる**MAC** アドレスの配布はランダムで、トポロジーとは完全に無関係です。ルートグルーピングは不可能で、すべてのルーターが通過するトラフィックのためにすべてのデバイスのルートを追跡する必要があります。これが**レイヤー2** スイッチの役割で、一定のホスト数を超えるとスケールしません。

有用なリソース：

- [なぜMACアドレスをIPv4|6の代わりにネットワーキングに使用できないのか？ (原文)](https://serverfault.com/questions/410626/why-couldnt-mac-addresses-be-used-instead-of-ipv46-for-networking)

</details>

<details>
<summary><b>最大30デバイスを含むネットワークに適用できる最小のIPv4サブネットマスクは何ですか？</b></summary><br>

標準の `/24` VLANをエンドユーザー用に使用するか、`/30` をポイントツーポイントリンク用に使用するか、またはその間のサブネットが最大30デバイスを含む必要がある場合、`/27`（またはサブネットマスク `255.255.255.224`）が適切です。

有用なリソース：

- [プレフィックス、ネットワーク、サブネット、ホスト番号をどのように計算しますか？](https://networkengineering.stackexchange.com/questions/7106/how-do-you-calculate-the-prefix-network-subnet-and-host-numbers)
- [IPアドレスの後のスラッシュ - CIDR表記](https://networkengineering.stackexchange.com/questions/3697/the-slash-after-an-ip-address-cidr-notation)
- [なぜ3つのプライベートIPv4アドレス範囲があるのか？](https://networkengineering.stackexchange.com/questions/32119/why-are-there-3-ranges-of-private-ipv4-addresses)
- [IP計算機](http://jodies.de/ipcalc)

</details>

<details>
<summary><b>一般的なHTTPステータスコードにはどのようなものがありますか？</b></summary><br>

- **1xx** - 情報レスポンス - 転送プロトコルレベルの情報を伝えます
- **2xx** - 成功 - クライアントのリクエストが正常に受け入れられたことを示します
- **3xx** - リダイレクション - クライアントがリクエストを完了するために追加のアクションを取る必要があることを示します
- **4xx** - クライアント側のエラー - このカテゴリのエラー状態コードは、クライアントに問題があることを示します
- **5xx** - サーバー側のエラー - サーバーがこれらのエラー状態コードの責任を負います

有用なリソース：

- [HTTPステータスコード](https://httpstatuses.com/)

</details>

###### Devops Questions (5)

<details>
<summary><b>DevOpsとは何ですか？DevOpsコミュニティの成功において、コミュニケーションの取り方とツールの選択のどちらが重要ですか？***</b></summary><br>

**DevOps** とは、開発と運用のタスクを両方行う統合チームのことを指します。または、非常に密接に協力して働く個々の運用チームと開発チームのことを指します。これは、共通の目標を達成するために他の部門と協力して作業する「方法」に近いです。

</details>

<details>
<summary><b>バージョン管理とは何ですか？あなたのコミットメッセージは見栄えが良いですか？</b></summary><br>

バージョン管理とは、ファイルまたはファイルのセットに対する変更を時間とともに記録するシステムであり、後で特定のバージョンを再呼び出すことができます。バージョン管理システムは、チームメイトがファイルやファイルのセットに変更をコミットできる中央共有リポジトリで構成されます。その後、バージョン管理の使用方法について説明します。

バージョン管理により、以下のことが可能になります：

- ファイルを以前の状態に戻す
- プロジェクト全体を以前の状態に戻す
- 時間の経過に伴う変更を比較する
- 問題を引き起こしている可能性のあるものを最後に修正した人を見る
- 課題を導入した人とその時期を見る

優れたコミットメッセージの七つのルール：

- 本文とタイトルを空行で区切る
- タイトル行を50文字以内に制限する
- タイトル行を大文字にする
- タイトル行をピリオドで終わらせない
- タイトル行で命令形を使用する
- 本文は72文字で折り返す
- 本文を使って「何を」「なぜ」説明し、「どうやって」は避ける

有用なリソース：

- [はじめに - バージョン管理について (原文)](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)

</details>

<details>
<summary><b>基本的な <code>git</code> コマンドを説明してください。</b></summary><br>

- `git init` - 新しいローカルリポジトリを作成する
- `git commit -m "message"` - 変更をヘッドにコミットする
- `git status` - `git add` で追加したファイルと、それ以降に変更したファイルをリスト表示する
- `git push origin master` - リモートリポジトリのマスターブランチに変更を送信する

</details>

<details>
<summary><b>簡単な継続的インテグレーションパイプラインを説明してください。</b></summary><br>

- リポジトリをクローンする
- デプロイステージ（QA）
- テスト環境（QA）
- デプロイステージ（PROD）

</details>

<details>
<summary><b>基本的な <code>docker</code> コマンドを説明してください。</b></summary><br>

- `docker ps` - 実行中のコンテナを表示する
- `docker ps -a` - すべてのコンテナを表示する
- `docker images` - Dockerイメージを表示する
- `docker logs <container-id|container-name>` - コンテナからログを取得する
- `docker network ls` - すべてのDockerネットワークを表示する
- `docker volumes ls` - すべてのDockerボリュームを表示する
- `docker exec -it <container-id|container-name> bash` - インタラクティブシェルでコンテナ内でbashを実行する

</details>

###### Cyber Security Questions (1)

<details>
<summary><b>セキュリティの誤設定とは何ですか？</b></summary><br>

**セキュリティの誤設定** とは、デバイス、アプリケーション、ネットワークが攻撃者によって悪用される可能性がある方法で構成されている脆弱性です。例えば、デフォルトのユーザー名やパスワードが変更されていない、またはデバイスアカウントのパスワードが簡単すぎるなどのケースが考えられます。

</details>

### :diamond_shape_with_a_dot_inside: <a name="regular-sysadmin">普通のシステム管理者</a>

###### System Questions (60)

<details>
<summary><b>本番環境での経験について教えてください。***</b></summary><br>

未完了です。

</details>

<details>
<summary><b>主要なウェブサーバーを運用するためにどのディストリビューションを選びますか？***</b></summary><br>

未完了です。

</details>

<details>
<summary><b>Linuxシステムのブートプロセスをいくつかのポイントで説明してください。</b></summary><br>

**BIOS**: BIOSのフルフォームは「Basic Input or Output System」で、整合性チェックを実行し、ブートローダーを検索してロードし、その後実行します。

**ブートローダー**: 初期段階はオペレーティングシステムに特有のものではないため、x86およびx86-64アーキテクチャのBIOSベースのブートプロセスは、マスターブートレコード（MBR）コードがリアルモードで実行され、第一段階のブートローダーがロードされると始まります。UEFIシステムでは、Linuxカーネルなどのペイロードが直接実行されることがあります。そのため、ブートローダーは必要ありません。一般的なブートローダーには、**GRUB**、**Syslinux/Isolinux**、または**Lilo**があります。

**カーネル**: Linuxのカーネルは、メモリ管理、タスクスケジューリング、I/O、プロセス間通信、システム全体の制御など、すべてのオペレーティングシステムのプロセスを処理します。これは2段階でロードされます。最初の段階では、カーネル（圧縮されたイメージファイル）がメモリにロードされ、解凍され、基本的なメモリ管理などのいくつかの基本的な機能が設定されます。

**Init**: システム上のすべてのプロセスの親であり、カーネルによって実行され、他のすべてのプロセスを起動する責任があります。

- `SysV init` - initの役割は「カーネルが完全に動作するようになると、すべてが正しく動作するようにすること」です。基本的には、ユーザー空間全体を確立し運営します。これには、ファイルシステムのチェックとマウント、必要なユーザーサービスの起動、そしてシステム起動が完了した後にユーザー環境に切り替えることが含まれます。
- `systemd` - systemdの開発者は、Unix System Vから受け継いだLinux initシステムを置き換えることを目指しました。initと同様に、systemdも他のデーモンを管理するデーモンです。すべてのデーモン（systemdを含む）はバックグラウンドプロセスです。systemdは最初に起動し（ブート時）、最後に終了します（シャットダウン時）。
- `runinit` - runinitは、Unixライクなオペレーティングシステムのためのinitスキームで、オペレーティングシステム全体でプロセスを初期化、監視、終了させます。これは、Linux、Mac OS X、*BSD、およびSolarisオペレーティングシステムで実行されるdaemontoolsプロセス監視ツールキットの再実装です。

役立つリソース:

- [Linuxブートプロセスの分析](https://opensource.com/article/18/1/analyzing-linux-boot-process)
- [Linuxにおけるsystemdブートプロセスの詳細](https://linoxide.com/linux-how-to/systemd-boot-process/)

</details>

<details>
<summary><b>Linuxデーモンが権限を下げる方法と理由は何ですか？なぜ一部のデーモンは起動するためにroot権限が必要なのですか？説明してください。***</b></summary>

未完了です。

</details>

<details>
<summary><b>なぜシングルコアマシンでの1.00のロードは理想的ではないのですか？</b></summary><br>

1.00のロードの問題は、余裕がないことです。実際には、多くのシステム管理者は0.70を超えた場合に線を引きます。

「調べる必要がある」ルールオブサム: 0.70 ロードアベレージが0.70を超えている場合は、事態が悪化する前に調査する時期です。

「今すぐ修正する」ルールオブサム: 1.00 ロードアベレージが1.00を超えた場合は、問題を見つけて今すぐ修正してください。さもないと、夜中に起こされることになり、それは楽しいことではありません。

ルールオブサム: 5.0 ロードアベレージが5.00を超えると、深刻な問題に直面している可能性があり、マシンがハングするか、非常に遅くなる可能性があります。そして、これが最悪のタイミングで発生します。例えば夜中やカンファレンスでプレゼンをしているときなどです。そこまで行かせないようにしましょう。

役立つリソース:

- [4コア8スレッドプロセッサのシステムロードの正しい解釈方法](https://serverfault.com/questions/618130/proper-way-of-interpreting-system-load-on-a-4-core-8-thread-processor)
- [Linux CPUロードの理解 - いつ心配すべきか？](http://blog.scoutapp.com/articles/2009/07/31/understanding-load-averages)

</details>

<details>
<summary><b>実効ユーザーがrootで、実際のユーザーIDがまだあなたの名前である場合、何を意味しますか？</b></summary><br>

**実際のユーザーID**はあなた自身（プロセスを所有するユーザー）であり、**実効ユーザーID**はオペレーティングシステムがあなたが何かをする権限があるかどうかを決定するために見るものです（通常はそうですが、例外もあります）。

ログインすると、ログインシェルは**実際のユーザーID**と**実効ユーザーID**の両方をパスワードファイルにより提供された同じ値（あなたの**実際のユーザーID**）に設定します。

例えば、setuidを実行し、別のユーザー（例: **root**）として実行する場合、setuidプログラムはあなたの代わりに何かを実行することが想定されています。

setuidを実行した後、そのプロセスはあなたの**実際のID**（プロセスの所有者として）を持ち、ファイル所有者の実効ユーザーID（例えば**root**）を持ちます。これはsetuidによるものです。

`passwd`のケースを考えてみましょう:

```bash
-rwsr-xr-x 1 root root 45396 may 25  2012 /usr/bin/passwd
```

ユーザー2が自分のパスワードを変更したい場合、`/usr/bin/passwd`を実行します。

この場合、**RUID**はユーザー2ですが、そのプロセスの**EUID**はrootになります。

ユーザー2は自分のパスワードを変更するためにのみpasswdを使用できます。なぜなら、内部的にpasswdは**RUID**をチェックし、それがrootでない場合、アクションは実際のユーザーのパスワードに限定されるからです。

passwdのプロセスが`/etc/passwd`および/または`/etc/shadow`に書き込む必要があるため、**EUID**がrootである必要があります。

役立つリソース:

- [Real User ID、Effective User ID、およびSaved User IDの違いは何ですか？（原文）](https://stackoverflow.com/questions/30493424/what-is-the-difference-between-a-process-pid-ppid-uid-euid-gid-and-egid)
- [pid、ppid、uid、euid、gid、egidの違いは何ですか？](https://stackoverflow.com/questions/30493424/what-is-the-difference-between-a-process-pid-ppid-uid-euid-gid-and-egid)

</details>

<details>
<summary><b>開発者が大量のログファイルを生成するcronジョブを追加しました。それらが非常に大きくなるのを防ぐにはどうすればよいですか？</b></summary><br>

ログファイルを扱う一般的な方法は`logrotate`を使用することです。しかし、`/etc/logrotate.conf`に内容を追加する代わりに、`/etc/logrotate.d/`に自分のジョブを追加するべきです。そうしないと、リリースアップグレード時に設定ファイルの差分をより多く確認する必要があります。

ファイルがアクティブに書き込まれている場合、トランケート（切り捨て）による対処はあまりできません。唯一の選択肢は、ファイルをトランケートすることです：

```bash
: >/var/log/massive-logfile
```

これは、プロセスを中断することなくファイルをトランケートできるため、とても役立ちます。

役立つリソース:

- [ログファイルを管理するためのlogrotateの使用方法](https://www.linode.com/docs/uptime/logs/use-logrotate-to-manage-log-files/)
- [システムログ](https://www.ibm.com/developerworks/library/l-lpic1-108-2/index.html)

</details>

<details>
<summary><b>Linuxカーネルはシステム内でプロセスをどのように作成、管理、削除しますか？ ***</b></summary><br>

未完了です。

役立つリソース:

- [Linuxプロセス](https://www.tldp.org/LDP/tlk/kernel/processes.html)

</details>

<details>
<summary><b><code>top</code>や<code>htop</code>で見ることができる選択した情報を説明してください。これらのツールを使ってロード、高いユーザー時間、メモリ不足の問題を診断する方法は？ ***</b></summary><br>

未完了です。

役立つリソース:

- [topを視覚的に説明](https://www.svennd.be/top-explained-visually/)
- [htopを視覚的に説明](https://codeahoy.com/2017/01/20/hhtop-explained-visually/)
- [Linuxにおけるhtop/topのすべての情報の説明](https://peteris.rocks/blog/htop/)

</details>

<details>
<summary><b>リソースを大量に消費しているプロセスをどのように認識しますか？</b></summary><br>

`top`は、正しい数字を見ていればかなり効果的です。
- **M** 現在の常駐メモリ使用量でソート
- **T** 総CPU使用量（または累積CPU使用量）でソート
- **P** 現在のCPU使用量でソート（これがデフォルトのリフレッシュです）
- **?** すべてのtopコマンドの使用概要を表示

これは、コンピュータプロセスが遅く動作している理由を解決し、どのプロセスを終了させるか、またはソフトウェアをアンインストールするかを決定する際に非常に重要な情報です。

役立つリソース:

- [マシンのリソースを大量に消費しているプロセスを見つける方法](https://superuser.com/questions/326300/how-to-find-the-processes-which-are-hogging-the-machine)

</details>

<details>
<summary><b><code>ntpd</code>サービスを200台のサーバーでアップグレードする必要があります。これらすべてを最新にするための最良の方法は何ですか？</b></summary><br>

**Infrastructure as Code**アプローチを使用することで、複数の良い方法があります：

1. **設定同期変更管理モデル**：

Ansible、Chef、Puppet、Saltstackなどの設定管理ツールを使用して、すべてのサーバーで`ntpd`サービスを自動的に更新できます。システムの安定性を保つために、サーバー上のシステムパッケージは通常、セキュリティ更新のみが自動的に行われます。パッケージのメジャーまたはマイナーなバージョンは、サービスの誤設定を防ぐために通常、設定定義でバージョンがロックされています。そのため、`ntpd`のバージョンを設定定義で変更することで、変更が展開されます。

このアプローチでは、大規模にインフラストラクチャに変更を展開する際に注意が必要です。展開のパイプラインには、ユニットテスト、統合テスト、システムテストが含まれ、最初にステージング環境に展開して設定を確認する必要があります。テストで設定の正確性が確認された場合は、エラーや失敗の際にロールバック可能なインクリメンタルローアウトで展開を行います。

2. **イミュータブルサーバーモデル**：

イミュータブルサーバーモデルでは、実行中のサーバーに変更を加える代わりに、新しい更新済みイメージで全体のユニット（サーバー、コンテナ）を置き換えます（これにより設定のドリフトが排除されます）。このアプローチでは、通常、PackerやDockerといったツールを使ってサーバーイメージを作成します。このイメージはテストされ、上記のオプション（1.）と同様に展開されますが、Canary Releaseなどの技術を使用して、インクリメンタルローアウトとロールバックが可能です。

役立つリソース:

- [Infrastructure as Code - 第8章: サーバーの更新と変更のパターン](http://shop.oreilly.com/product/0636920039297.do)

</details>

<details>
<summary><b><code>$PATH</code>をLinux/Unixで永続的に設定する方法は？この変数はなぜ重要なのですか？ ***</b></summary>

未完了です。

</details>

<details>
<summary><b>サーバーが起動する際にコンソールにエラーが表示されます。ブートメッセージを調べる方法と、それらがどこに保存されているかは？</b></summary><br>

コンソールには2種類のメッセージがあります：

- **カーネルによって生成されたもの**（`printk`を通じて）
- **ユーザー空間によって生成されたもの**（通常はinitシステム）

カーネルメッセージは常に**kmsg**バッファに保存され、`dmesg`コマンドで表示できます。また、しばしば**syslog**にもコピーされます。これは、`/dev/kmsg`に書き込まれるユーザー空間のメッセージにも適用されますが、これらは比較的まれです。

一方で、ユーザー空間が`/dev/console`や`/dev/tty1`に fancyなブートステータステキストを書き込むと、それはどこにも保存されません。単に画面に表示されるだけです。

`dmesg`はカーネルリングバッファに含まれるブートメッセージを確認するために使用されます。リングバッファは固定サイズのバッファで、新しいデータが追加されると古いデータが上書きされます。

ブートプロセスが完了すると、カーネルに渡されたコマンドラインオプション、検出されたハードウェアコンポーネント、新しいUSBデバイスが追加されたイベント、NIC（ネットワークインターフェースカード）障害やネットワーク上でリンクアクティビティが検出されないなどのエラーが表示されます。

システムログがジャーナルコンポーネントを介して行われている場合は、`journalctl`を使用するべきです。これにはカーネルメッセージやブートメッセージ、syslogやさまざまなサービスからのメッセージが含まれます。

ブートの問題やエラーには、システム管理者が特定の重要なファイルとコマンド（Linuxの異なるバージョンで異なる方法で扱われます）を確認する必要があります：

- `/var/log/boot.log` - システムブートログで、システムブート中に展開されたすべての内容が含まれています。
- `/var/log/messages` - システムブート中にログされたメッセージを含むグローバルなシステムメッセージを保存します。
- `/var/log/dmesg` - カーネルリングバッファの情報を含みます。

役立つリソース:

- [Linuxブート後のすべてのブートメッセージを表示する方法（原文）](https://superuser.com/questions/1188407/how-to-view-all-boot-messages-in-linux-after-booting)
- [/var/log/{syslog,dmesg,messages}ログファイルの違い](https://superuser.com/questions/565927/differences-in-var-log-syslog-dmesg-messages-log-files)
- [Debianシステムをブートするときにスクロールするメッセージを後で確認する方法は？](https://serverfault.com/questions/516411/all-debian-boot-messages)

</details>

<details>
<summary><b>スワップ使用量が高すぎる。これにはどのような理由があり、スワッピングの問題を解決するにはどうすればよいですか？</b></summary><br>

**スワップ**領域は、利用可能なメモリが完全に使用されたときにオペレーティングシステムによって使用される制限された物理メモリの量です。これは、メモリのセクションを物理ストレージにスワップするメモリ管理です。

システムがより多くのメモリリソースを必要とし、RAMが満杯の場合、メモリ内の非アクティブなページがスワップ領域に移動されます。スワップ領域はRAMが少ないマシンに役立ちますが、より多くのRAMの代わりとして考えるべきではありません。**スワップ**領域はハードドライブに存在し、物理メモリよりもアクセス時間が遅いです。

ワークロードがRAMの需要を増加させます。より多くのメモリを必要とするワークロードを実行しています。全スワップ使用量はそれを示しています。また、`swappiness`を**1**に変更するのは賢明な決定ではないかもしれません。`swappiness`を**1**に設定しても、スワッピングが行われないわけではありません。それはカーネルがスワッピングに対してどれほど攻撃的になるかを示すだけで、スワッピングを排除するわけではありません。必要があればスワッピングは行われます。

- **スワップ領域のサイズを増やす** - まず、ディスクの使用量が増加します。ディスクが十分に速くない場合、システムがスラッシングを起こす可能性があり、メモリ内のデータがスワップインおよびスワップアウトされる際に遅延を経験するでしょう。これがボトルネックを引き起こします。

- **RAMを追加する** - 真の解決策はメモリを追加することです。RAMの代替はありません。十分なメモリがあれば、スワップは少なくなります。

スワップ領域の使用状況を監視するためには：

- `cat /proc/swaps` - 総スワップサイズと使用量を確認する
- `grep SwapTotal /proc/meminfo` - 総スワップ領域を表示する
- `free` - システムメモリの使用可能量と使用量を表示する（スワップも含む）
- `vmstat` - スワッピング統計を確認する
- `top`, `htop` - スワップ領域の使用状況を確認する
- `atop` - システムがメモリを過剰にコミットしているかどうかを示す
- または、スワップ領域を使用しているアプリケーションをキロバイト単位でリストするワンライナーシェルコマンドを使用する：
```bash
for _fd in /proc/*/status ; do
  awk '/VmSwap|Name/{printf $2 " " $3}END{ print ""}' $_fd
done | sort -k 2 -n -r | less
```

役立つリソース:

- [Linuxは私のRAMを食べました！](https://www.linuxatemyram.com/)
- [Linuxでどのプロセスがスワップ領域を使用しているかを確認する方法](https://stackoverflow.com/questions/479953/how-to-find-out-which-processes-are-using-swap-space-in-linux)
- [Linuxでスワップ領域の使用状況を監視するための8つの有用なコマンド](https://www.tecmint.com/commands-to-monitor-swap-space-usage-in-linux/)
- [Ubuntuサーバーでスワップが完全に使用されている場合の危険性は？](https://serverfault.com/questions/499301/what-is-the-danger-in-having-a-fully-used-swap-in-an-ubuntu-server)
- [無料のRAMがある場合にスワップを空にする方法](https://askubuntu.com/questions/1357/how-to-empty-swap-if-there-is-free-ram)

</details>

<details>
<summary><b>umaskとは何ですか？ユーザーのために永続的に設定するにはどうすればよいですか？</b></summary><br>

Linuxやその他のUnix系オペレーティングシステムでは、新しいファイルがデフォルトのパーミッションセットで作成されます。具体的には、新しいファイルのパーミッションは、`umask`というパーミッション「マスク」を適用することで特定の方法で制限されることがあります。`umask`コマンドは、このマスクを設定するために使用されるか、現在の値を表示するために使用されます。

永続的に変更するには（例: `umask 02`）:

- `~/.profile`
- `~/.bashrc`
- `~/.zshrc`
- `~/.cshrc`

役立つリソース:

- [Umaskとは何か、およびLinuxでのデフォルトumaskの設定方法](https://www.cyberciti.biz/tips/understanding-linux-unix-umask-value-usage.html)

</details>

<details>
<summary><b>以下のumask値の違いを説明してください: 000, 002, 022, 027, 077, および 277。</b></summary><br>

<table style="width:100%">
  <tr>
    <th>Umask</th>
    <th>ファイルの結果</th>
    <th>ディレクトリの結果</th>
  </tr>
  <tr>
    <td>000</td>
    <td>666 rw- rw- rw-</td>
    <td>777 rwx rwx rwx</td>
  </tr>
 <tr>
    <td>002</td>
    <td>664 rw- rw- r--</td>
    <td>775 rwx rwx r-x</td>
  </tr>
  <tr>
    <td>022</td>
    <td>644 rw- r-- r--</td>
    <td>755 rwx r-x r-x</td>
  </tr>
<tr>
    <td>027</td>
    <td>640 rw- r-- ---</td>
    <td>750 rwx r-x ---</td>
  </tr>
<tr>
    <td>077</td>
    <td>600 rw---- ---</td>
    <td>700 rwx --- ---</td>
  </tr>
<tr>
    <td>277</td>
    <td>400 r-- --- ---</td>
    <td>500 r-x --- ---</td>
  </tr>
</table>

役立つリソース:

- [Umaskとは何か、およびLinuxでのデフォルトumaskの設定方法](https://www.cyberciti.biz/tips/understanding-linux-unix-umask-value-usage.html)

</details>

<details>
<summary><b>シンボリックリンクとハードリンクの違いは何ですか？</b></summary><br>

ファイルシステムの下では、ファイルはinodeによって表されます（複数のinodeかもしれませんが、確かではありません）。

- ファイルシステム内のファイルは基本的にinodeへのリンクです。
- ハードリンクは、同じ基盤となるinodeへのリンクを持つ別のファイルを作成します。

ファイルを削除すると、基盤となるinodeへのリンクの1つが削除されます。inodeは、すべてのリンクが削除されるまで削除（または削除可能/上書き可能）されません。

- シンボリックリンクは、ファイルシステム内の別の名前へのリンクです。

ハードリンクが作成されると、そのリンクはinodeに対するものです。元のファイルを削除、名前変更、または移動しても、ハードリンクには影響しません。ハードリンクは基盤となるinodeにリンクしているためです。inode上のデータに対する変更は、そのinodeを参照するすべてのファイルに反映されます。

注意: ハードリンクは同じファイルシステム内でのみ有効です。シンボリックリンクは、ファイルシステムを跨ることができるため、単に別のファイルの名前です。

違い:

- **ハードリンク**はディレクトリに対して作成することはできません。ハードリンクはファイルに対してのみ作成できます。
- **ソフトリンク**（シンボリックリンクまたはsymlinkとも呼ばれる）はディレクトリにリンクすることができます。

役立つリソース:

- [ハードリンクとシンボリックリンクの違いは何ですか？](https://medium.com/@wendymayorgasegura/what-is-the-difference-between-a-hard-link-and-a-symbolic-link-8c0493041b62)

</details>

<details>
<summary><b>スティッキービット（sticky bit）はどのように機能しますか？<code>SUID/GUID</code>と同じですか？</b></summary><br>

これは多くの人がしばしば混乱する非常に厄介な問題の1つです。**SUID/GUID**ビットと**スティッキービット**は全く異なるものです。

`man chmod` を実行すれば、**SUID**と**スティッキービット**についての説明を読むことができます。

**SUID/GUID**

上記のマニュアルページが言おうとしているのは、xビットが rwxrwxrwx のユーザーオクタル（最初のrwグループ）およびグループオクタル（2番目のrwグループ）で占める位置が、xがsになる追加の状態を取ることができるということです。これが発生すると、ファイルが実行されると（プログラムであり、単なるシェルスクリプトではない場合）、ファイルの所有者またはグループの権限で実行されます。

したがって、ファイルがrootによって所有されていて、**SUID**ビットがオンになっていると、プログラムはrootとして実行されます。たとえ通常のユーザーとして実行してもです。同様のことが**GUID**ビットにも当てはまります。

例:

**SUID/GUIDなし** - ビット `rwxr-xr-x` が設定されています。

```bash
ls -lt b.pl
-rwxr-xr-x 1 root root 179 Jan  9 01:01 b.pl
```

**SUID & ユーザーの実行ビットが有効（小文字のs）** - ビット `rwsr-xr-x` が設定されています。

```bash
chmod u+s b.pl
ls -lt b.pl
-rwsr-xr-x 1 root root 179 Jan  9 01:01 b.pl
```

**SUID有効 & 実行ビット無効（大文字のS）** - ビット `rwSr-xr-x` が設定されています。

```bash
chmod u-x b.pl
ls -lt b.pl
-rwSr-xr-x 1 root root 179 Jan  9 01:01 b.pl
```

**GUID & グループの実行ビットが有効（小文字のs）** - ビット `rwxr-sr-x` が設定されています。

```bash
chmod g+s b.pl
ls -lt b.pl
-rwxr-sr-x 1 root root 179 Jan  9 01:01 b.pl
```

**GUID有効 & 実行ビット無効（大文字のS）** - ビット `rwxr-Sr-x` が設定されています。

```bash
chmod g-x b.pl
ls -lt b.pl
-rwxr-Sr-x 1 root root 179 Jan  9 01:01 b.pl
```

**スティッキービット**

一方、スティッキービットは `t` で示されます。たとえば `/tmp` ディレクトリで:

```bash
ls -l /|grep tmp
drwxrwxrwt. 168 root root 28672 Jun 14 08:36 tmp
```

このビットは、実際には「制限付き削除ビット」と呼ばれるべきです。スティッキービットが有効になっていると、ユーザーは自分が所有するファイルおよびディレクトリのみを削除できるようになります。

役立つリソース:

- [スティッキービットはどのように機能しますか？（原文）](https://unix.stackexchange.com/questions/79395/how-does-the-sticky-bit-work)

</details>

<details>
<summary><b>What does <code>LC_ALL=C</code> before command do? In what cases it will be useful?</b></summary><br>

`LC_ALL` is the environment variable that overrides all the other localisation settings. This sets all `LC_` type variables at once to a specified locale.

The main reason to set `LC_ALL=C` before command is that fine to simply get English output (general change the locale used by the command).

On the other hand, also important is to increase the speed of command execution with `LC_ALL=C` e.g. `grep` or `fgrep`. Using the `LC_ALL=C` locale increased our performance and brought command execution time down.

For example, if you set `LC_ALL=en_US.utf8` your system opened multiple files from the `/usr/lib/locale` directory. For `LC_ALL=C` a minimum amount of open and read operations is performed.

If you want to restore all your normal (original) locale settings for the session:

```bash
LC_ALL=
```

If `LC_ALL` does not work, try using `LANG` (if that still does not work, try `LANGUAGE`):

```bash
LANG=C date +%A
Monday
```

Useful resources:

- [What does LC_ALL=C do? (original)](https://unix.stackexchange.com/questions/87745/what-does-lc-all-c-do)
- [Speed up grep searches with LC_ALL=C](https://www.inmotionhosting.com/support/website/ssh/speed-up-grep-searches-with-lc-all)

</details>

<details>
<summary><b>How to make high availability of web application? ***</b></summary>

To be completed.

</details>

<details>
<summary><b>You are configuring a new server. One of the steps is setting the permissions to the app directories. What steps will you take and what mistakes to avoid?</b></summary><br>

**1) Main requirements - remember about this**

- which users have access to the app filesystem
- permissions for web servers, e.g. Apache and app servers e.g. uwsgi
- permissions for specific directories like a **uploads**, **cache** and main app directory like a `/var/www/app01/html`
- correct `umask` value for users and **suid**/**sgid** (only for specific situations)
- permissions for all future files and directories
- permissions for cron jobs and scripts

**2) Application directories**

`/var/www` contains a directory for each website (isolation of the apps), e.g. `/var/www/app01`, `/var/www/app02`

```bash
mkdir /var/www/{app01,app02}
```

**3) Application owner and group**

Each application has a designated **owner** (e.g. **u01-prod**, **u02-prod**) and **group** (e.g. **g01-prod**, **g02-prod**) which are set as the owner of all files and directories in the website's directory:

```bash
chown -R u01-prod:g01-prod /var/www/app01
chown -R u02-prod:g02-prod /var/www/app02
```

**4) Developers owner and group**

All of the users that maintain the website have own groups and they're attach to application group:

```bash
id alice
uid=2000(alice) gid=4000(alice) groups=8000(g01-prod)
id bob
uid=2001(bob) gid=4001(bob) groups=8000(g01-prod),8001(g02-prod)
```

So **alice** user has standard privileges for `/var/www/app01` and **bob** user has standard privileges for `/var/www/app01` and `/var/www/app02`.

**5) Web server owner and group**

Any files or directories that need to be written by the webserver have their owner. If the web servers is Apache, default owner/group are **apache:apache** or **www-data:www-data** and for Nginx it will be **nginx:nginx**. Don't change these settings.

If applications works with app servers like a **uwsgi** or **php-fpm** should set the appropriate user and group (e.g. for **app01** it will be **u01-prod:g01-prod**) in specific config files.

**6) Permissions**

Set properly permissions with **Access Control Lists**:

```bash
# For web server
setfacl -Rdm "g:apache:rwx" /var/www/app01
setfacl -Rm "g:apache:rwx" /var/www/app01

# For developers
setfacl -Rdm "g:g01-prod:rwx" /var/www/app01
setfacl -Rm "g:g01-prod:rwx" /var/www/app01
```

If you use **SELinux** remember about security context:

```bash
chcon -R system_u:object_r:httpd_sys_content_t /var/www/app01
```

**7) Security mistakes**

- **root** owner for files and directories
- **root** never executes any files in website directory, and shouldn't be creating files in there
- to wide permissions like a **777** so some critical files may be world-writable and world-readable
- avoid creating maintenance scripts or other critical files with suid root

If you allow your site to modify the files which form the code running your site, you make it much easier for someone to take over your server.

A file upload tool allows users to upload a file with any name and any contents. This allows a user to upload a mail relay PHP script to your site, which they can place wherever they want to turn your server into a machine to forward unsolicited commercial email. This script could also be used to read every email address out of your database, or other personal information.

If the malicious user can upload a file with any name but not control the contents, then they could easily upload a file which overwrites your `index.php` (or another critical file) and breaks your site.

Useful resources:

- [How to setup linux permissions for the WWW folder?](https://serverfault.com/questions/124800/how-to-setup-linux-permissions-for-the-www-folder)
- [What permissions should my website files/folders have on a Linux webserver?](https://serverfault.com/questions/357108/what-permissions-should-my-website-files-folders-have-on-a-linux-webserver)
- [Security Pitfalls of setgid Programs](https://www.agwa.name/blog/post/security_pitfalls_of_setgid_programs)

</details>

<details>
<summary><b>What steps will be taken by init when you run <code>telinit 1</code> from run level 3? What will be the final result of this? If you use <code>telinit 6</code> instead of <code>reboot</code> command your server will be restarted? ***</b></summary><br>

To be completed.

Useful resources:

- [What differences it will make, if i use “telinit 6” instead of “reboot” command to restart my computer?](https://unix.stackexchange.com/questions/434560/what-differences-it-will-make-if-i-use-telinit-6-instead-of-reboot-command)

</details>

<details>
<summary><b>I have forgotten the root password! What do I do in BSD? What is the purpose of booting into single user mode?</b></summary><br>

Restart the system, type `boot -s` at the `Boot:` prompt to enter **single-user mode**.

At the question about the shell to use, hit `Enter` which will display a `#` prompt.

Enter `mount -urw /` to remount the root file system read/write, then run `mount -a` to remount all the file systems.

Run `passwd root` to change the root password then run `exit` to continue booting.

**Single user mode** should basically let you log in with root access & change just about anything. For example, you might use single-user mode when you are restoring a damaged master database or a system database, or when you are changing server configuration options (e.g. password recovery).

Useful resources:

- [FreeBSD Reset or Recover Root Password](https://www.cyberciti.biz/tips/howto-freebsd-reset-recover-root-password.html)
- [Single User Mode Definition](http://www.linfo.org/single_user_mode.html)

</details>

<details>
<summary><b>How could you modify a text file without invoking a text editor?</b></summary><br>

For example:<br>

```bash
# cat  >filename ... - overwrite file
# cat >>filename ... - append to file
cat > filename << __EOF__
data
__EOF__
```

</details>

<details>
<summary><b>How to change the kernel parameters? What kernel options might you need to tune? ***</b></summary><br>

To set the kernel parameters in Unix-like, first edit the file `/etc/sysctl.conf` after making the changes save the file and run the command `sysctl -p`, this command will make the changes permanently without rebooting the machine.

Useful resources:

- [How to Change Kernel Runtime Parameters in a Persistent and Non-Persistent Way](https://www.tecmint.com/change-modify-linux-kernel-runtime-parameters/)

</details>

<details>
<summary><b>Explain the <code>/proc</code> filesystem.</b></summary><br>

`/proc` is a virtual file system that provides detailed information about kernel, hardware and running processes.

Since `/proc` contains virtual files, it is called virtual file system. These virtual files have unique qualities. Most of them are listed as zero bytes in size.

Virtual files such as `/proc/interrupts`, `/proc/meminfo`, `/proc/mounts` and `/proc/partitions` provide an up-to-the-moment glimpse of the system’s hardware. Others: `/proc/filesystems` file and the `/proc/sys/` directory provide system configuration information and interfaces.

Useful resources:

- [Linux Filesystem Hierarchy - /proc](https://www.tldp.org/LDP/Linux-Filesystem-Hierarchy/html/proc.html)

</details>

<details>
<summary><b>Describe your data backup process. How often should you test your backups? ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>Explain three types of journaling in ext3/ext4.</b></summary><br>

There are three types of journaling available in **ext3/ext4** file systems:

- **Journal** - metadata and content are saved in the journal
- **Ordered** - only metadata is saved in the journal. Metadata are  journaled only after writing the content to disk. This is the default
- **Writeback** - only metadata is saved in the journal. Metadata might be  journaled either before or after the content is written to the disk

</details>

<details>
<summary><b>What is an inode? How to find file's inode number and how can you use it?</b></summary><br>

An **inode** is a data structure on a filesystem on Linux and other Unix-like operating systems that stores all the information about a file except its name and its actual data. A data structure is a way of storing data so that it can be used efficiently.

A Unix file is stored in two different parts of the disk - the data blocks and the inodes. I won't get into superblocks and other esoteric information. The data blocks contain the "contents" of the file. The information about the file is stored elsewhere - in the inode.

A file's inode number can easily be found by using the `ls` command, which by default lists the objects (i.e. files, links and directories) in the current directory (i.e. the directory in which the user is currently working), with its `-i` option. Thus, for example, the following will show the name of each object in the current directory together with its inode number:

```bash
ls -i
```

`df's` `-i` option instructs it to supply information about inodes on each filesystem rather than about available space. Specifically, it tells df to return for each mounted filesystem the total number of inodes, the number of free inodes, the number of used inodes and the percentage of inodes used. This option can be used together with the `-h` option as follows to make the output easier to read:

```bash
df -hi
```

**Finding files by inodes**

If you know the inode, you can find it using the find command:

```bash
find . -inum 435304 -print
```

**Deleting files with strange names**

Sometimes files are created with strange characters in the filename. The Unix file system will allow any character as part of a filename except for a null (ASCII 000) or a "/". Every other character is allowed.

Users can create files with characters that make it difficult to see the directory or file. They can create the directory ".. " with a space at the end, or create a file that has a backspace in the name, using:

```bash
touch `printf "aa\bb"`
```

Now what what happens when you use the `ls` command:

```bash
ls
aa?b
ls | grep 'a'
ab
```

Note that when `ls` sends the result to a terminal, it places a "**?**" in the filename to show an unprintable character.

You can get rid of this file by using `rm -i *` and it will prompt you before it deletes each file. But you can also use `find` to remove the file, once you know the inode number.

```bash
ls -i
435304 aa?b
find . -inum 435304 -delete
```

Useful resources:

- [Understand UNIX/Linux Inodes Basics with Examples](https://www.thegeekstuff.com/2012/01/linux-inodes/)
- [What is an inode as defined by POSIX?](https://unix.stackexchange.com/questions/387087/what-is-an-inode-as-defined-by-posix/387093)

</details>

<details>
<summary><b><code>ls -l</code> shows file attributes as question marks. What this means and what steps will you take to remove unused "zombie" files?</b></summary><br>

This problem may be more difficult to solve because several steps may be required - sometimes you have get `test/file: Permission denied`, `test/file: No such file or directory` or `test/file: Input/output error`.

That happens when the user can't do a `stat()` on the files (which requires execute permissions), but can read the directory entries (which requires read access on the directory). So you get a list of files in the directory, but can't get any information on the files because they can't be read. If you have a directory which has read permission but not execute, you'll see this.

Some processes like a `rsync` generates temporary files that get created and dropped fast which will cause errors if you try to call other simple file management commands like `rm`, `mv` etc.

Example of output:

```bash
?????????? ? ?        ?               ?            ? sess_kee6fu9ag7tiph2jae
```

1) change permissions: `chmod 0777 sess_kee6fu9ag7tiph2jae` and try remove
2) change owner: `chown root:root sess_kee6fu9ag7tiph2jae` and try remove
3) change permissions and owner for directory: `chmod -R 0777 dir/ && chown -R root:root dir/` and try remove
4) recreate file: `touch sess_kee6fu9ag7tiph2jae` and try remove
5) watch out for other running processes on the server for example `rsync`, sometimes you can see this as a transient error when an NFS server is heavily overloaded
6) find file inode: `ls -i`, and try remove: `find . -inum <inode_num> -delete`
7) remount (if possible) your filesystem
8) boot system into single-user mode and repair your filesystem with `fsck`

Useful resources:

- [Question marks showing in ls of directory. IO errors too.](https://serverfault.com/questions/65616/question-marks-showing-in-ls-of-directory-io-errors-too)

</details>

<details>
<summary><b>To LVM or not to LVM. What benefits does it provide?</b></summary><br>

- LVM makes it quite easy to move file systems around
- you can extend a volume group onto a new physical volume
- move any number of logical volumes of an old physical one
- remove that volume from the volume group without needing to unmount any partitions
- you can also make snapshots of logical volumes for making backups
- LVM has built in mirroring support so you can have a logical volume mirrored across multiple physical volumes
- LVM even supports TRIM

Useful resources:

- [What is LVM and what is it used for?](https://askubuntu.com/questions/3596/what-is-lvm-and-what-is-it-used-for)

</details>

<details>
<summary><b>How to increase the size of LVM partition?</b></summary><br>

Use the `lvextend` command for resize LVM partition.

- extending the size by 500MB:

```bash
lvextend -L +500M /dev/vgroup/lvolume
```

- extending all available free space:

```bash
lvextend -l +100%FREE /dev/vgroup/lvolume
```

and `resize2fs` or `xfs_growfs` to resize filesystem:

- for ext filesystems:

```bash
resize2fs /dev/vgroup/lvolume
```

- for xfs filesystem:

```bash
xfs_growfs mountpoint_for_/dev/vgroup/lvolume
```

Useful resources:

- [Extending a logical volume](https://www.tldp.org/HOWTO/LVM-HOWTO/extendlv.html)

</details>

<details>
<summary><b>What is a zombie/defunct process?</b></summary><br>

Is a process that has completed execution (via the `exit` system call) but still has an entry in the process table: it is a process in the "**Terminated state**".

Processes marked **defunct** are dead processes (so-called "zombies") that remain because their parent has not destroyed them properly. These processes will be destroyed by init if the parent process exits.

Useful resources:

- [What is a <defunct> process, and why doesn't it get killed?](https://askubuntu.com/questions/201303/what-is-a-defunct-process-and-why-doesnt-it-get-killed)

</details>

<details>
<summary><b>What is the proper way to upgrade/update a system in production? Do you automate these processes? Do you set downtime for them? Write recommendations. ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>Your friend during configuration of the MySQL server asked you: <i>Should I run <code>sudo mysql_secure_installation</code> after installing mysql?</i> What do you think about it? </b></summary><br>

It would be better if you run command as it provides many security options like:

- You can set a password for root accounts
- You can remove root accounts that are accessible from outside the local host
- You can remove anonymous-user accounts
- You can remove the test database, which by default can be accessed by anonymous users

Useful resources:

- [What is Purpose of using mysql_secure_installation?](https://stackoverflow.com/questions/20760908/what-is-purpose-of-using-mysql-secure-installation)

</details>

<details>
<summary><b>Present and explain the good ways of using the <code>kill</code> command.</b></summary><br>

Speaking of killing processes never use `kill -9/SIGKILL` unless absolutely mandatory. This kill can cause problems because of its brute force.

Always try to use the following simple procedure:

- first, send **SIGTERM** (`kill -15`) signal first which tells the process to shutdown and is generally accepted as the signal to use when shutting down cleanly (but remember that this signal can be ignored).
- next try to send **SIGHUP** (`kill -1`) signal which is commonly used to tell a process to shutdown and restart, this signal can also be caught and ignored by a process.

The far majority of the time, this is all you need - and is much cleaner.

Useful resources:

- [When should I not kill -9 a process?](https://unix.stackexchange.com/questions/8916/when-should-i-not-kill-9-a-process)
- [SIGTERM vs. SIGKILL](https://major.io/2010/03/18/sigterm-vs-sigkill/)

</details>

<details>
<summary><b>What is <code>strace</code> command and how should be used? Explain example of connect to an already running process.</b></summary><br>

`strace` is a powerful command line tool for debugging and troubleshooting programs in Unix-like operating systems such as Linux. It captures and records all system calls made by a process and the signals received by the process.

**Strace Overview**

`strace` can be seen as a light weight debugger. It allows a programmer/user to quickly find out how a program is interacting with the OS. It does this by monitoring system calls and signals.

**Uses**

Good for when you don't have source code or don't want to be bothered to really go through it. Also, useful for your own code if you don't feel like opening up **GDB**, but are just interested in understanding external interaction.

**Example of attach to the process**

`strace -p <PID>` - to attach a process to strace.

`strace -e trace=read,write -p <PID>` - by this you can also trace a process/program for an event, like read and write (in this example). So here it will print all such events that include read and write system calls by the process.

Other such examples

- `-e trace=network` - trace all the network related system calls.
- `-e trace=signal` - trace all signal related system calls.
- `-e trace=ipc` - trace all IPC related system calls.
- `-e trace=desc` - trace all file descriptor related system calls.
- `-e trace=memory` - trace all memory mapping related system calls.

Useful resources:

- [How should strace be used? (original)](https://stackoverflow.com/questions/174942/how-should-strace-be-used)
- [How does strace connect to an already running process? (original)](https://stackoverflow.com/questions/7482076/how-does-strace-connect-to-an-already-running-process)
- [strace: for fun, profit, and debugging](http://timetobleed.com/hello-world/)

</details>

<details>
<summary><b>When would you use access control lists instead of or in conjunction with the <code>chmod</code> command? ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>Which algorithms are supported in <code>/etc/shadow</code> file?</b></summary><br>

Typical current algorithms are:

- MD5
- SHA-1 (also called SHA)

both should not be used for cryptographic/security purposes any more!!

- SHA-256
- SHA-512
- SHA-3 (KECCAK was announced the winner in the competition for a new federal approved hash algorithm in October 2012)

Useful resources:

- [What is the algorithm used to encrypt Linux passwords?](https://crypto.stackexchange.com/questions/40841/what-is-the-algorithm-used-to-encrypt-linux-passwords)
- [How to find the hashing algorithm used to obfuscate passwords?](https://unix.stackexchange.com/questions/430141/how-to-find-the-hashing-algorithm-used-to-obfuscate-passwords)

</details>

<details>
<summary><b>What is the use of ulimit in Unix-like systems?</b></summary><br>

Most Unix-like operating systems, including Linux and BSD, provide ways to limit and control the usage of system resources such as threads, files, and network connections on a per-process and per-user basis. These "**ulimits**" prevent single users from using too many system resources.

</details>

<details>
<summary><b>What are soft limits and hard limits?</b></summary><br>

**Hard limit** is the maximum allowed to a user, set by the superuser or root. This value is set in the file `/etc/security/limits.conf`. The user can increase the **soft limit** on their own in times of needing more resources, but cannot set the **soft limit** higher than the **hard limit**.

</details>

<details>
<summary><b>During configuration HAProxy to working with Redis you get <code>General socket error (Permission denied)</code> from log. SELinux is enable. Explain basic SELinux troubleshooting in CLI. ***</b></summary><br>

Useful resources:

- [Basic SELinux Troubleshooting in CLI](https://access.redhat.com/articles/2191331)

</details>

<details>
<summary><b>You have configured an RSA key login but your server show <code>Server refused our key</code> as expected. Where will you look for the cause of the problem?</b></summary><br>

**Server side**

Setting `LogLevel VERBOSE` in file `/etc/ssh/sshd_config` is probably what you need, although there are higher levels:

SSH auth failures are logged in `/var/log/auth.log`, `/var/log/secure` or `/var/log/audit/audit.log`.

The following should give you only ssh related log lines (for example):

```bash
grep 'sshd' /var/log/auth.log
```

Next, the most simple command to list all failed SSH logins is the one shown below:

```bash
grep "Failed password" /var/log/auth.log
```

also useful is:

```bash
grep "Failed\|Failure" /var/log/auth.log
```

On newer Linux distributions you can query the runtime log file maintained by Systemd daemon via `journalctl` command (`ssh.service` or `sshd.service`). For example:

```bash
journalctl _SYSTEMD_UNIT=ssh.service | egrep "Failed|Failure"
```

**Client side**

Also you should run SSH client with `-v|--verbose` - it is in first level of verbosity. Next, you can enable additional (level 2 and 3) verbosity for even more debugging messages as shown with e.g. `-vv`.

Useful resources:

- [Enable Debugging Mode in SSH to Troubleshoot Connectivity Issues](https://www.tecmint.com/enable-debugging-mode-in-ssh/)

</details>

<details>
<summary><b>Why do most distros use ext4, as opposed to XFS or other filesystems? Why are there so many of them? ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>A project manager needs a new SQL Server. What do you ask her/his? ***</b></summary><br>

I want the DBA to ask questions like:

- How big will the database be? (whether we can add the database to an existing server)
- How critical is the database? (about clustering, disaster recovery, high availability)

</details>

<details>
<summary><b>Create a file with 100 lines with random values.</b></summary><br>

For example:

```bash
cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 100 > /path/to/file
```

</details>

<details>
<summary><b>How to run script as another user without password?</b></summary><br>

For example (with `visudo` command):

```bash
user1 ALL=(user2) NOPASSWD: /opt/scripts/bin/generate.sh
```

The command paths must be absolute! Then call `sudo -u user2 /opt/scripts/bin/generate.sh` from a user1 shell.

</details>

<details>
<summary><b>How to check if running as root in a bash script? What should you watch out for?</b></summary><br>

In a bash script, you have several ways to check if the running user is root.

As a warning, do not check if a user is root by using the root username. Nothing guarantees that the user with ID 0 is called root. It's a very strong convention that is broadly followed but anybody could rename the superuser another name.

I think the best way when using bash is to use `$EUID` because `$UID` could be changed and not reflect the real user running the script.

```bash
if (( $EUID != 0 )); then
  echo "Please run as root"
  exit
fi
```
</details>

<details>
<summary><b>Can you give a particular example when is indicated to use <code>nobody</code> account? Tell me the differences running httpd service as a <code>nobody</code> and <code>www-data</code> accounts.</b></summary><br>

In many Unix variants, `nobody` is the conventional name of a user account which owns no files, is in no privileged groups, and has no abilities except those which every other user has.

It is common to run daemons as `nobody`, especially servers, in order to limit the damage that could be done by a malicious user who gained control of them.

However, the usefulness of this technique is reduced if more than one daemon is run like this, because then gaining control of one daemon would provide control of them all. The reason is that `nobody`-owned processes have the ability to send signals to each other and even debug each other, allowing them to read or even modify each other's memory.

**When should I use `nobody` account?**

When permissions aren't required for a program's operations. This is most notable when there isn't ever going to be any disk activity.

A real world example of this is **memcached** (a key-value in-memory cache/database/thing), sitting on my computer and my server running under the `nobody` account. Why? Because it just doesn't need any permissions and to give it an account that did have write access to files would just be a needless risk.

A good example are also web servers. Imagine if Apache ran as root and someone found a way to send custom commands to the console through Apache would have access to your entire system.

`nobody` account also is used as a restricted shell for giving users filesystem access without an actual shell like bash. This should prevent them from being able to execute things.

**`nobody` or `www-data` for httpd (Apache)**

Upon starting Apache needs root access, but it quickly drops this and assumes the identity of a non privileged user. This user can either be `nobody` or `apache`, or `www-data`.

Several applications use the user `nobody` as a default. For example you probably never really want say the Apache service to be overwriting files that belong to bind. Having a per-service account tends to be a very good idea.

Getting Apache to run as `nobody:nobody` is pretty easy, just update the user and group settings. But as I mentioned above I don't really recommend that particular user/group. It is entirely possible that you may be tempted to add a service to the system at some time in the future that also runs as `nobody`, and you will forget that have given write access on the filesystem to the user `nobody`.

 If somehow, `nobody` were to become compromised they could potentially have more impact than if an application isolate user, such as `www-data`. Of course a lot of this will depend on the file and group permissions. `nobody` uses the permissions of others, while an application specific user could be configured to allow file read access, but other could still be denied.

Useful resources:

- [What is nobody user and group?](https://unix.stackexchange.com/questions/186568/what-is-nobody-user-and-group)
- [The Linux and Unix Nobody User](http://linuxg.net/the-linux-and-unix-nobody-user/)
- [What is the purpose of the 'nobody' user?](https://askubuntu.com/questions/329714/what-is-the-purpose-of-the-nobody-user)

</details>

<details>
<summary><b>Is there a way to redirect output to a file and have it display on stdout?</b></summary><br>

The command you want is named tee:

`foo | tee output.file`

For example, if you only care about stdout:

`ls -a | tee output.file`

If you want to include stderr, do:

`program [arguments...] 2>&1 | tee outfile`

`2>&1` redirects channel 2 (stderr/standard error) into channel 1 (stdout/standard output), such that both is written as stdout. It is also directed to the given output file as of the tee command.

Furthermore, if you want to append to the log file, use `tee -a` as:

`program [arguments...] 2>&1 | tee -a outfile`

</details>

<details>
<summary><b>What is the preferred bash shebang and why? What is the difference between executing a file using <code>./script</code> or <code>bash script</code>?</b></summary><br>

You should use `#!/usr/bin/env bash` for portability: different \*nixes put bash in different places, and using `/usr/bin/env` is a workaround to run the first bash found on the `PATH`.

Running `./script` does exactly that, and requires execute permission on the file, but is agnostic to what type of a program it is. It might be a **bash script**, an **sh script**, or a **Perl**, **Python**, **awk**, or **expect script**, or an actual **binary executable**. Running `bash script` would force it to be run under `sh`, instead of anything else.

Useful resources:

- [What is the preferred Bash shebang? (original)](https://stackoverflow.com/questions/10376206/what-is-the-preferred-bash-shebang)

</details>

<details>
<summary><b>You must run command that will be performed for a very long time. How to prevent killing this process after the ssh session drops?</b></summary><br>

Use `nohup` to make your process ignore the hangup signal:

```bash
nohup long-running-process &
exit
```

or you want to be using **GNU Screen**:

```bash
screen -d -m long-running-process
exit
```

Useful resources:

- [5 Ways to Keep Remote SSH Sessions and Processes Running After Disconnection](https://www.tecmint.com/keep-remote-ssh-sessions-running-after-disconnection/)

</details>

<details>
<summary><b>What is the main purpose of the intermediate certification authorities?</b></summary><br>

To find out the main purpose of an intermediate CA, you should first learn about **Root CAs**, **Intermediate CAs**, and the **SSL Certificate Chain Trust**.

**Root CAs** are primary CAs which typically don’t directly sign end entity/server certificates. They issue Root certificates which are usually pre-installed within all browsers, mobiles, and applications. The private key of these certificates is used to sign other subsequent certificates called intermediate certificates. Root CAs are usually kept "offline” and in a highly secure environment with stringently limited access.

**Intermediates CAs** are CAs that subordinate to the Root CA by one or more levels, being trusted by these to sign certificates on their behalf. The purpose of creating and using Intermediate CAs is primarily for security because if the intermediate private key is compromised, then the Root CA can revoke the intermediate certificate and create a new one with a new cryptographic key pair.

**SSL Certificate Chain Trust** is the list of SSL certificates, from the root certificate to the end entity/server certificate. For an SSL Certificate to be trusted, it must be issued by a trusted CAs which is included in the trusted CA list of the connecting device (browser, mobile, and application). Therefore, the connecting device will test the trustworthiness of each SSL Certificate in the Chain Trust until it matches the one issued by a trusted CA.

The **Root-Intermediate CA** structure is created by each major CA to protect against the disastrous effects of a root key compromise. If a root key is compromised, it would render the root and all subordinated certificates untrustworthy. For this reason, creating an Intermediate CA is a best practice to ensure a rigorous protection of the primary root key.

Useful resources:

- [How certificate chains work](https://knowledge.digicert.com/solution/SO16297.html)

</details>

<details>
<summary><b>How to reload PostgreSQL after configuration changes?</b></summary><br>

Solution 1:

```bash
systemctl reload postgresql
```

Solution 2:

```
su - postgres
/usr/bin/pg_ctl reload
```

Solution 3:

```
SELECT pg_reload_conf();
```

</details>

<details>
<summary><b>You have added several aliases to <code>.profile</code>. How to reload shell without exit?</b></summary><br>

The best way is `exec $SHELL -l` because `exec` replaces the current process with a new one. Also good (but other) solution is `. ~/.profile`.

Useful resources:

- [How to reload .bash_profile from the command line?](https://stackoverflow.com/questions/4608187/how-to-reload-bash-profile-from-the-command-line)

</details>

<details>
<summary><b>How to exit without saving shell history?</b></summary><br>

```bash
kill -9 $$
```

or

```bash
unset HISTFILE && exit
```

Useful resources:

- [How do I close a terminal without saving the history?](https://unix.stackexchange.com/questions/25049/how-do-i-close-a-terminal-without-saving-the-history)

</details>

<details>
<summary><b>What is this UID 0 toor account? Have I been compromised?</b></summary><br>

**toor** is an alternative superuser account, where toor is root spelled backwards. It is intended to be used with a non-standard shell so the default shell for root does not need to change.

This is important as shells which are not part of the base distribution, but are instead installed from ports or packages, are installed in `/usr/local/bin` which, by default, resides on a different file system. If root's shell is located in `/usr/local/bin` and the file system containing `/usr/local/bin`) is not mounted, root will not be able to log in to fix a problem and will have to reboot into single-user mode in order to enter the path to a shell.

Some people use toor for day-to-day root tasks with a non-standard shell, leaving root, with a standard shell, for single-user mode or emergencies. By default, a user cannot log in using toor as it does not have a password, so log in as root and set a password for toor before using it to login.

Useful resources:

- [The root account (and toor)](https://administratosphere.wordpress.com/2007/10/04/the-root-account-and-toor/)

</details>

<details>
<summary><b>Is there an easy way to search inside 1000s of files in a complex directory structure to find files which contain a specific string?</b></summary><br>

For example use `fgrep`:

```bash
fgrep * -R "string"
```

or:

```bash
grep -insr "pattern" *
```

- `-i` ignore case distinctions in both the **PATTERN** and the input files
- `-n`  prefix each line of output with the 1-based line number within its input file
- `-s` suppress error messages about nonexistent or unreadable files.
- `-r` read all files under each directory, recursively.

Useful resources:

- [How to grep a string in a directory and all its subdirectories files in LINUX?](https://stackoverflow.com/questions/15622328/how-to-grep-a-string-in-a-directory-and-all-its-subdirectories-files-in-linux)

</details>

<details>
<summary><b>How to find out the dynamic libraries executables loads when run?</b></summary><br>

You can do this with `ldd` command:

```bash
ldd /bin/ls
```

</details>

<details>
<summary><b>You have the task of sync the testing and production environments. What steps will you take?</b></summary><br>

It's easy to get dragged down into bikeshedding about cloning environments and miss the real point:

- only production is production

and every time you deploy there you are testing a unique combination of deploy code + software + environment.

Every once in a while a good solution is regular cloning of the production servers to create testing servers. You can create instances with an exact copy of your production environment under a dev/test with snapshots, for example:

- generate a snapshot of production
- copy the snapshot to staging (or other)
- create a new disk using this snapshot

Sure, you can spin up clones of various system components or entire systems, and capture real traffic to replay offline (the gold standard of systems testing). But many systems are too big, complex, and cost-prohibitive to clone.

Before environment synchronization a good way is keeping track of every change that you make to the testing environment and provide a way for propagating this to the production environment, so that you do not skip any step and do it as smoothly as possible.

Also structure comparison tool or deploy scripts that update the testing environment from production environment is a good solution.

**Presync tasks**

First of all is informing developers and clients about not making changes on the test environment (if possible, disabling test domains that target this environment or set static pages with information about synchronization).

It is also important to make backup/snapshots of both environments.

**Database servers**

- sync/update system version (e.g. packages)
- create dump file from database on production db server
- import dump file on testing db server
- if necessary, syncs login permissions, roles, database permissions, open connections to the database and other

**Web/App servers**

- sync/update system version (e.g. packages)
- if necessary, updated kernel parameters, firewall rules and other
- sync/update configuration files of all running/important services
- sync/update user accounts (e.g. permissions) and their home directories
- deploy project from git/svn repository
- sync/update important directories existing in project, e.g. **static**, **asset** and other
- sync/update permissions for project directory
- remove/update all webhooks
- update cron jobs

**Others tasks**

- updated configurations of load balancers for testing domains and specific urls
- updated configurations of queues, session and storage instances

Useful resources:

- [Keeping testing and production server environments clean, in sync, and consistent](https://stackoverflow.com/questions/639668/keeping-testing-and-production-server-environments-clean-in-sync-and-consisten)

</details>

###### Network Questions (24)

<details>
<summary><b>Configure a virtual interface on your workstation. ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>According to an HTTP monitor, a website is down. You're able to telnet to the port, so how do you resolve it?</b></summary><br>

If you can telnet to the port, this means that the service listening on the port is running and you can connect to it (it's not a networking problem). It is good to check this way for the IP address to which the domain is resolved and using the same domain to test connection.

First of all check if your site is online from a other location. It then lets you know if the site is down everywhere, or if only your network is unable to view it. It is also a good idea to check what the web browser returns.

**If only IP connection working**

- you can use whois to see what DNS servers serve up the hostname to the site: `whois www.example.com`
- you can use tools like `dig` or `host` to test DNS to see if the host name is resolving: `host www.example.org dns.example.org`
- you can also check global public dns servers: `host www.example.com 9.9.9.9`

If domain not resolved it's probably problem with DNS servers.

**If domain resolved properly**

- investigate the log files and resolve the issue regarding to the logs, it's the best way to show what's wrong
- check the http status code, usually it will be the response with the 5xx, maybe server is overload because clients making lot's of connection to the website or specific url? maybe your caching rules not working properly?
- check web/proxy server configuration (e.g. `nginx -t -c </path/to/nginx.conf>`), maybe another sysadmin has made some changes to the domain configuration?
- maybe something on the server has crashed? maybe run out of space or run out of memory?
- maybe it's a programming error on the website?

</details>

<details>
<summary><b>Load balancing can dramatically impact server performance. Discuss several load balancing mechanisms. ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>List examples of network troubleshooting tools that can degrade during DNS issues. ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>Explain difference between HTTP 1.1 and HTTP 2.0.</b></summary><br>

<b>HTTP/2</b> supports queries multiplexing, headers compression, priority and more intelligent packet streaming management. This results in reduced latency and accelerates content download on modern web pages.

Key differences with **HTTP/1.1**:

- it is binary, instead of textual
- fully multiplexed, instead of ordered and blocking
- can therefore use one connection for parallelism
- uses header compression to reduce overhead
- allows servers to "push" responses proactively into client caches

Useful resources:

- [What is HTTP/2 - The Ultimate Guide](https://kinsta.com/learn/what-is-http2/)

</details>

<details>
<summary><b>Dev team reports an error: <code>POST http://ws.int/api/v1/Submit/ resulted in a 413 Request Entity Too Large</code>. What's wrong?</b></summary><br>

**Modify NGINX configuration file for domain**

Set correct `client_max_body_size` variable value:

```bash
client_max_body_size 20M;
```

Restart Nginx to apply the changes.

**Modify php.ini file for upload limits**

It’s not needed on all configurations, but you may also have to modify the PHP upload settings as well to ensure that nothing is going out of limit by php configurations.

Now find following directives one by one:

```bash
upload_max_filesize
post_max_size
```

and increase its limit to 20M, by default they are 8M and 2M:

```bash
upload_max_filesize = 20M
post_max_size = 20M
```

Finally save it and restart PHP.

Useful resources:

- [413 Request Entity Too Large in Nginx with client_max_body_size set](https://serverfault.com/questions/814767/413-request-entity-too-large-in-nginx-with-client-max-body-size-set)

</details>

<details>
<summary><b>What is handshake mechanism and why do we need 3 way handshake?</b></summary><br>

**Handshaking** begins when one device sends a message to another device indicating that it wants to establish a communications channel. The two devices then send several messages back and forth that enable them to agree on a communications protocol.

A **three-way handshake** is a method used in a TCP/IP network to create a connection between a local host/client and server. It is a three-step method that requires both the client and server to exchange `SYN` and `ACK` (`SYN`, `SYN-ACK`, `ACK`) packets before actual data communication begins.

Useful resources:

- [Why do we need a 3-way handshake? Why not just 2-way?](https://networkengineering.stackexchange.com/questions/24068/why-do-we-need-a-3-way-handshake-why-not-just-2-way)

</details>

<details>
<summary><b>Why is UDP faster than TCP?</b></summary><br>

**UDP** is faster than **TCP**, and the simple reason is because its nonexistent acknowledge packet (`ACK`) that permits a continuous packet stream, instead of TCP that acknowledges a set of packets, calculated by using the TCP window size and round-trip time (`RTT`).

Useful resources:

- [UDP vs TCP, how much faster is it?](https://stackoverflow.com/questions/47903/udp-vs-tcp-how-much-faster-is-it)

</details>

<details>
<summary><b>Which, in your opinion, are the 5 most important OpenSSH parameters that improve the security? ***</b></summary><br>

To be completed.

Useful resources:

- [OpenSSH security and hardening](https://linux-audit.com/audit-and-harden-your-ssh-configuration/)

</details>

<details>
<summary><b>What is NAT? What is it used for?</b></summary><br>

It enables private IP networks that use unregistered IP addresses to connect to the Internet. **NAT** operates on a router, usually connecting two networks together, and translates the private (not globally unique) addresses in the internal network into legal addresses, before packets are forwarded to another network.

Workstations or other computers requiring special access outside the network can be assigned specific external IPs using **NAT**, allowing them to communicate with computers and applications that require a unique public IP address. **NAT** is also a very important aspect of firewall security.

Useful resources:

- [Network Address Translation (NAT) Concepts](http://www.firewall.cx/networking-topics/network-address-translation-nat/227-nat-concepts.html)

</details>

<details>
<summary><b>What is the purpose of Spanning Tree?</b></summary><br>

This protocol operates at layer 2 of the OSI model with the purpose of preventing loops on the network. Without **STP**, a redundant switch deployment would create broadcast storms that cripple even the most robust networks. There are several iterations based on the original IEEE 802.1D standard; each operates slightly different than the others while largely accomplishing the same loop-free goal.

</details>

<details>
<summary><b>How to check which ports are listening on my Linux Server?</b></summary><br>

Use the:

- `lsof -i`
- `ss -l`
- `netstat -atn` - for tcp
- `netstat -aun` - for udp
- `netstat -tulapn`

</details>

<details>
<summary><b>What mean <code>Host key verification failed</code> when you connect to the remote host? Do you accept it automatically?</b></summary><br>

`Host key verification failed` means that the host key of the remote host was changed. This can easily happen when connecting to a computer who's host keys in `/etc/ssh` have changed if that computer was upgraded without copying its old host keys. The host keys here are proof when you reconnect to a remote computer with ssh that you are talking to the same computer you connected to the first time you accessed it.

Whenever you connect to a server via SSH, that server's public key is stored in your home directory (or possibly in your local account settings if using a Mac or Windows desktop) file called **known_hosts**. When you reconnect to the same server, the SSH connection will verify the current public key matches the one you have saved in your **known_hosts** file. If the server's key has changed since the last time you connected to it, you will receive the above error.

Don't delete the entire **known_hosts** file as recommended by some people, this totally voids the point of the warning. It's a security feature to warn you that a man in the middle attack may have happened.

Before accepting the new host key, contact your/other system administrator for verification.

Useful resources:

- [Git error: "Host Key Verification Failed" when connecting to remote repository](https://stackoverflow.com/questions/13363553/git-error-host-key-verification-failed-when-connecting-to-remote-repository)

</details>

<details>
<summary><b>How to send an HTTP request using <code>telnet</code>?</b></summary><br>

For example:

```bash
telnet example.com 80
Trying 192.168.252.10...
Connected to example.com.
Escape character is '^]'.
GET /questions HTTP/1.0
Host: example.com

HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
...
```

</details>

<details>
<summary><b>How do you kill program using e.g. 80 port in Linux?</b></summary><br>

To list any process listening to the port 80:

```bash
# with lsof
lsof -i:80

# with fuser
fuser 80/tcp
```

To kill any process listening to the port 80:

```bash
kill $(lsof -t -i:80)
```

or more violently:

```bash
kill -9 $(lsof -t -i:80)
```

or with `fuser` command:

```bash
fuser -k 80/tcp
```

Useful resources:

- [How to kill a process running on particular port in Linux?](https://stackoverflow.com/questions/11583562/how-to-kill-a-process-running-on-particular-port-in-linux/32592965)
- [Finding the PID of the process using a specific port?](https://unix.stackexchange.com/questions/106561/finding-the-pid-of-the-process-using-a-specific-port)

</details>

<details>
<summary><b>You get <code>curl: (56) TCP connection reset by peer</code>. What steps will you take to solve this problem?</b></summary><br>

- check if the URL is correct, maybe you should add `www` or set correctly `Host:` header? Check also scheme (http or https)
- check the domain is resolving into a correct IP address
- enable debug tracing with `--trace-ascii curl.dump`. `Recv failure` is a really generic error so its hard for more info
- use external proxy with `--proxy` for debug connection from external ip
- use network sniffer (e.g. `tcpdump`) for debug connection in the lower TCP/IP layers
- check firewall rules on the production environment and on the exit point of your network, also check your NAT rules
- check MTU size of packets traveling over your network
- check SSL version with ssl/tls `curl` params if you connecting to https protocol
- it may be a problem on the client side e.g. the netfilter drop or limit  connections from your IP address to the domain

Useful resources:

- [CURL ERROR: Recv failure: Connection reset by peer - PHP Curl](https://stackoverflow.com/questions/10285700/curl-error-recv-failure-connection-reset-by-peer-php-curl)

</details>

<details>
<summary><b>How to allow traffic to/from specific IP with iptables?</b></summary><br>

For example:

```bash
/sbin/iptables -A INPUT -p tcp -s XXX.XXX.XXX.XXX -j ACCEPT
/sbin/iptables -A OUTPUT -p tcp -d  XXX.XXX.XXX.XXX -j ACCEPT
```

</details>

<details>
<summary><b>How to block abusive IP addresses with <code>pf</code> in OpenBSD?</b></summary><br>

The best way to do this is to define a table and create a rule to block the hosts, in `pf.conf`:

```bash
table <badhosts> persist
block on fxp0 from <badhosts> to any
```

And then dynamically add/delete IP addresses from it:

```bash
pfctl -t badhosts -T add 1.2.3.4
pfctl -t badhosts -T delete 1.2.3.4
```

</details>

<details>
<summary><b>When does the web server like Apache or Nginx write info to log file? Before or after serving the request?</b></summary><br>

Both servers provides very comprehensive and flexible logging capabilities - for logging everything that happens on your server, from the initial request, through the URL mapping process, to the final resolution of the connection, including any errors that may have occurred in the process.

**Apache**

The Apache server access log records all requests processed by the server (after the request has been completed).

**Nginx**

NGINX writes information about client requests in the access log right after the request is processed.

Useful resources:

- [When does Apache log to access.log - before or after serving the request?](https://webmasters.stackexchange.com/questions/65566/when-does-apache-log-to-access-log-before-or-after-serving-the-request)
- [nginx log request before processing](https://serverfault.com/questions/693049/nginx-log-request-before-processing)

</details>

<details>
<summary><b>Analyse web server log and show only <code>5xx</code> http codes. What external tools do you use?</b></summary><br>

```bash
tail -n 100 -f /path/to/logfile | grep "HTTP/[1-2].[0-1]\" [5]"
```

Examples of http/https log management tools:

- **goaccess** - is an open source real-time web log analyzer and interactive viewer that runs in a terminal in *nix systems or through your browser
- **graylog** - is a free and open-source log management platform that supports in-depth log collection and analysis

Useful resources:

- [Best Log Management Tools: 51 Useful Tools for Log Management, Monitoring, Analytics, and More](https://stackify.com/best-log-management-tools/)

</details>

<details>
<summary><b>Developer uses private key on the server to deploy app through ssh. Why it is incorrect behavior and what is the better (but not ideal) solution in such situations?</b></summary><br>

You have the private key for your personal account. The server needs your public key so that it can verify that your private key for the account you are trying to use is authorized.

The whole point with private keys is that they are private, meaning only you have your private key. If someone takes over your private key, it will be able to impersonate you any time he wants.

A better solutions is the use of ssh key forwarding. An essence, you need to create a `~/.ssh/config` file, if it doesn't exist. Then, add the hosts (either domain name or IP address in the file and set `ForwardAgent yes`). Example:

```bash
Host git.example.com
    User john
    PreferredAuthentications publickey
    IdentityFile ~/.ssh/id_rsa.git.example.com
    ForwardAgent yes
```

Your remote server must allow SSH agent forwarding on inbound connections and your local `ssh-agent` must be running.

Forwarding an ssh agent carries its own security risk. If someone on the remote machine can gain access to your forwarded ssh agent connection, they can still make use of your keys. However, this is better than storing keys on remote machines: the attacker can only use the ssh agent connection, not the key itself. Thus, only while you're logged into the remote machine can they do anything. If you store the key on the remote machine, they can make a copy of it and use it whenever they want.

If you use ssh keys remember about passphrases which is strongly recommended to reduce risk of keys accidentally leaking.

Useful resources:

- [How to forward local keypair in a SSH session?](https://stackoverflow.com/questions/12257968/how-to-forward-local-keypair-in-a-ssh-session)
- [Using SSH agent forwarding](https://developer.github.com/v3/guides/using-ssh-agent-forwarding/)
- [SSH Agent Forwarding considered harmful](https://heipei.github.io/2015/02/26/SSH-Agent-Forwarding-considered-harmful/)
- [Security Consideration while using ssh-agent](https://www.commandprompt.com/blog/security_considerations_while_using_ssh-agent/)

</details>

<details>
<summary><b>What is the difference between CORS and CSPs?</b></summary><br>

**CORS** allows the **Same Origin Policy** to be relaxed for a domain.

e.g. normally if the user logs into both `example.com` and `example.org`, the Same Origin Policy prevents `example.com` from making an AJAX request to `example.org/current_user/full_user_details` and gaining access to the response.

This is the default policy of the web and prevents the user's data from being leaked when logged into multiple sites at the same time.

Now with **CORS**, `example.org` could set a policy to say it will allow the origin `https://example.com` to read responses made by AJAX. This would be done if both `example.com` and `example.org` are ran by the same company and data sharing between the origins is to be allowed in the user's browser. It only affects the client-side of things, not the server-side.

**CSPs** on the other hand set a policy of what content can run on the current site. For example, if JavaScript can be executed inline, or which domains `.js` files can be loaded from. This can be beneficial to act as another line of defense against **XSS** attacks, where the attacker will try and inject script into the HTML page. Normally output would be encoded, however say the developer had forgotten only on one output field. Because the policy is preventing in-line script from executing, the attack is thwarted.

Useful resources:

- [What is the difference between CORS and CSPs? (original)](https://stackoverflow.com/questions/39488241/what-is-the-difference-between-cors-and-csps)
- [CSP, SRI and CORS](https://colorblindprogramming.com/csp-sri-and-cors)

</details>

<details>
<summary><b>Explain four types of responses from firewall when scanning with <code>nmap</code>.</b></summary><br>

There might be four types of responses:

- **Open port** - few ports in the case of the firewall
- **Closed port** - most ports are closed because of the firewall
- **Filtered** - `nmap` is not sure whether the port is open or not
- **Unfiltered** - `nmap` can access the port but is still confused about the open status of the port

Useful resources:

- [NMAP - Closed vs Filtered](https://security.stackexchange.com/questions/182504/nmap-closed-vs-filtered)

</details>

<details>
<summary><b>What does a <code>tcpdump</code> do? How to capture only incoming traffic to your interface?</b></summary><br>

`tcpdump` is a most powerful and widely used command-line packets sniffer or package analyzer tool which is used to capture or filter TCP/IP packets that received or transferred over a network on a specific interface.

`tcpdump` puts your network card into promiscuous mode, which basically tells it to accept every packet it receives. It allows the user to see all traffic being passed over the network. Wireshark uses pcap to capture packets.

If you want to view only packets that come to your interface you should:

- `-Q in` - for Linux `tcpdump` version
- `-D in` - for BSD `tcpdump` version

Both params set send/receive direction direction for which packets should be captured.

```bash
tcpdump -nei eth0 -Q in host 192.168.252.125 and port 8080
```

</details>

###### Devops Questions (7)

<details>
<summary><b>Which are the top DevOps tools? Which tools have you worked on?</b></summary><br>

The most popular DevOps tools are mentioned below:

- **Git** : Version Control System tool
- **Jenkins** : Continuous Integration tool
- **Selenium** : Continuous Testing tool
- **Puppet**, **Chef**, **Ansible** : Configuration Management and Deployment tools
- **Nagios** : Continuous Monitoring tool
- **Docker** : Containerization tool

</details>

<details>
<summary><b>How do all these tools work together?</b></summary><br>

The most popular DevOps tools are mentioned below:

- Developers develop the code and this source code is managed by Version Control System tools like Git etc.
- Developers send this code to the Git repository and any changes made in the code is committed to this Repository
- Jenkins pulls this code from the repository using the Git plugin and build it using tools like Ant or Maven
- Configuration management tools like puppet deploys & provisions testing environment and then Jenkins releases this code on the test environment on which testing is done using tools like selenium
- Once the code is tested, Jenkins send it for deployment on the production server (even production server is provisioned & maintained by tools like puppet)
- After deployment It is continuously monitored by tools like Nagios
- Docker containers provides testing environment to test the build features

</details>

<details>
<summary><b>What are playbooks in Ansible?</b></summary><br>

Playbooks are Ansible’s configuration, deployment, and orchestration language.

They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process. Playbooks are designed to be human-readable and are developed in a basic text language.

At a basic level, playbooks can be used to manage configurations of and deployments to remote machines.

</details>

<details>
<summary><b>What is NRPE (Nagios Remote Plugin Executor) in Nagios?</b></summary><br>

The **NRPE** addon is designed to allow you to execute Nagios plugins on remote Linux/Unix machines. The main reason for doing this is to allow Nagios to monitor "local" resources (like CPU load, memory usage, etc.) on remote machines.

Since these public resources are not usually exposed to external machines, an agent like **NRPE** must be installed on the remote Linux/Unix machines.

</details>

<details>
<summary><b>What is the difference between Active and Passive check in Nagios?</b></summary><br>

The major difference between Active and Passive checks is that Active checks are initiated and performed by Nagios, while passive checks are performed by external applications.

Passive checks are useful for monitoring services that are:

- asynchronous in nature and cannot be monitored effectively by polling their status on a regularly scheduled basis.
- located behind a firewall and cannot be checked actively from the monitoring host.

The main features of Actives checks are as follows:

- active checks are initiated by the Nagios process.
- active checks are run on a regularly scheduled basis.

</details>

<details>
<summary><b>How to <code>git clone</code> including submodules?</b></summary><br>

For example:

```bash
# With -j8 - performance optimization
git clone --recurse-submodules -j8 git://github.com/foo/bar.git

# For already cloned repos or older Git versions
git clone git://github.com/foo/bar.git
cd bar
git submodule update --init --recursive
```

</details>

<details>
<summary><b>Mention what are the advantages of using Redis? What is <code>redis-cli</code>? </b></summary><br>

- it provides high speed (exceptionally faster than others)
- it supports a server-side locking
- it has got lots of client lib
- it has got command level Atomic Operation (tx operation)
- supports for rich data types like hashes, sets, bitmaps

`redis-cli` is the **Redis** command line interface, a simple program that allows to send commands to **Redis**, and read the replies sent by the server, directly from the terminal.

Useful resources:

- [10 Advantages of Redis](https://dzone.com/articles/10-traits-of-redis)

</details>

###### Cyber Security Questions (4)

<details>
<summary><b>What is XSS, how will you mitigate it?</b></summary><br>

**Cross Site Scripting** is a JavaScript vulnerability in the web applications. The easiest way to explain this is a case when a user enters a script in the client side input fields and that input gets processed without getting validated. This leads to untrusted data getting saved and executed on the client side.

Countermeasures of XSS are input validation, implementing a CSP (Content security policy) and other.

</details>

<details>
<summary><b>HIDS vs NIDS and which one is better and why?</b></summary><br>

**HIDS** is host intrusion detection system and **NIDS** is network intrusion detection system. Both the systems work on the similar lines. It’s just that the placement in different. **HIDS** is placed on each host whereas **NIDS** is placed in the network. For an enterprise, **NIDS** is preferred as **HIDS** is difficult to manage, plus it consumes processing power of the host as well.

</details>

<details>
<summary><b>What is compliance?</b></summary><br>

Abiding by a set of standards set by a government/Independent party/organisation, e.g. an industry which stores, processes or transmits Payment related information needs to be complied with PCI DSS (Payment card Industry Data Security Standard). Other compliance examples can be an organisation complying with its own policies.

</details>

<details>
<summary><b>What is a WAF and what are its types?</b></summary><br>

**WAF** stands for web application firewall. It is used to protect the application by filtering legitimate traffic from malicious traffic. **WAF** can be either a box type or cloud based.

</details>

### :diamond_shape_with_a_dot_inside: <a name="senior-sysadmin">Senior Sysadmin</a>

###### System Questions (61)

<details>
<summary><b>Explain the current architecture you’re responsible for and point out where it’s scalable or fault-tolerant. ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>Tell me how code gets deployed in your current production. ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>What are the different types of kernels? Explain.</b></summary><br>

**Monolithic Kernels**

Earlier in this type of kernel architecture, all the basic system services like a process and memory management, interrupt handling etc were packaged into a single module in kernel space. This type of architecture led to some serious drawbacks like:

- the size of the kernel, which was huge
- poor maintainability, which means bug fixing or addition of new features resulted in recompilation of the whole kernel which could consume hours

In a modern day approach to monolithic architecture, the kernel consists of different modules which can be dynamically loaded and unloaded. This modular approach allows easy extension of OS's capabilities. With this approach, maintainability of kernel became very easy as only the concerned module needs to be loaded and unloaded every time there is a change or bug fix in a particular module.

Linux follows the monolithic modular approach.

**Microkernels**

This architecture majorly caters to the problem of ever growing size of kernel code which we could not control in the monolithic approach. This architecture allows some basic services like device driver management, protocol stack, file system etc to run in user space.

In this architecture, all the basic OS services which are made part of user space are made to run as servers which are used by other programs in the system through inter process communication (IPC).

Example: We have servers for device drivers, network protocol stacks, file systems, graphics, etc. Microkernel servers are essentially daemon programs like any others, except that the kernel grants some of them privileges to interact with parts of physical memory that are otherwise off limits to most programs.

**Hybrid Kernels (Modular Kernels)**

This is a combination of the above two, where the key idea is that Operating System services are in Kernel Space, and there is no message passing, no performance overhead and no reliability benefits, of having services in user space.

This is used by Microsoft's NT kernels, all the way up to the latest Windows version.

Useful resources:

- [An Introduction to Kernels. The Heart of Computing Devices. (original)](https://keetmalin.wixsite.com/keetmalin/single-post/2017/08/24/An-Introduction-to-Kernels-The-Heart-of-Computing-Devices)

</details>

<details>
<summary><b>The program returns the error of the missing library. How to provide dynamically linkable libraries?</b></summary><br>

Environment variable `LD_LIBRARY_PATH` is a colon-separated set of directories where libraries should be searched for first, before the standard set of directories; this is useful when debugging a new library or using a nonstandard library for special purposes.

The best way to use `LD_LIBRARY_PATH` is to set it on the command line or script immediately before executing the program. This way the new `LD_LIBRARY_PATH` isolated from the rest of your system.

Example of use:

```bash
export LD_LIBRARY_PATH="/list/of/library/paths:/another/path" ./program
```

Useful resources:

- [How to correctly use LD_LIBRARY_PATH](http://wiredrevolution.com/system-administration/how-to-correctly-use-ld_library_path)

</details>

<details>
<summary><b>Write the most important rules for using root privileges safely for novice administrators. ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>What is the advantage of synchronizing UID/GID across multiple systems?</b></summary><br>

There are several principle reasons why you want to co-ordinate the **user/UID** and **group/GID** management across your network.

The first is relatively obvious - it has to do with user and administrative convenience.

If each of your users are expected to have relatively uniform access to the systems throughout the network, then they'll expect the same username and password to work on each system that they are supposed to use. If they change their password they will expect that change to be global.

It also has a relationship with names and group names in Unix and Linux. They are mapped into numeric forms (**UID's** and **GID's** respectively). All file ownership (inodes) and processes use these numerics for all access and identity determination throughout the kernel and drivers. These numeric values are reverse mapped back to their corresponding principle symbolic representations (the names) by the utilities that display or process that information.

It is also recommended that you adopt a policy that **UID's** are not re-used. When a user leaves your organization you "retire" their **UID** (disabling their access by \*'ing out their passwd, removing them from the groups maps, setting their "shell" to some `/bin/denied` binary and their home directory to a secured _graveyard_ - I use `/home/.graveyard` on my systems).

The reason for this may not be obvious. However, if you are maintaining archival backups for several years (or indefinitely) you'll want to avoid any ambiguities and confusion that might result from restoring one (long gone) user's files and finding them owned by one of your new users.

Useful resources:

- [UID/GID Synchronization and Management (original)](https://linuxgazette.net/issue31/tag_uidgid.html)
- [What's the advantage of synchronizing UID/GID across Linux machines?](https://serverfault.com/questions/603987/whats-the-advantage-of-synchronizing-uid-gid-across-linux-machines)
- [How can I keep user accounts consistent across multiple machines?](https://unix.stackexchange.com/questions/141023/how-can-i-keep-user-acccounts-consistent-accross-multiple-machines)

</details>

<details>
<summary><b>What principles to follow for successful system performance tuning? ***</b></summary><br>

To be completed.

Useful resources:

- [An Introduction to Performance Tuning](https://www.oreilly.com/library/view/system-performance-tuning/059600284X/ch01.html)

</details>

<details>
<summary><b>Describe start-up configuration files and directory in BSD systems.</b></summary><br>

In BSD the primary start-up configuration file is `/etc/defaults/rc.conf`. System startup scripts such as `/etc/rc` and `/etc/rc.d` just include this file.

If you want to add other programs to system startup you need to change `/etc/rc.conf` file instead of `/etc/defaults/rc.conf`.

</details>

<details>
<summary><b>CPU spent the most of the time for a IO operations to complete. Which tools do you use for diagnose what process(es) did exactly wait for IO? How to minimize IO wait time? ***</b></summary><br>

To be completed.

Useful resources:

- [Can anyone explain precisely what IOWait is?](https://serverfault.com/questions/12679/can-anyone-explain-precisely-what-iowait-is)

</details>

<details>
<summary><b>The Junior dev accidentally destroyed production database. How can you prevent such situations?</b></summary><br>

**Create disaster recovery plan**

Disaster recovery and business continuity planning are integral parts of the overall risk management for an organization. Is a documented process or set of procedures to recover and protect a business IT infrastructure.

If you don’t have a recovery solution, then your restoration efforts will become rebuilding efforts, starting from scratch to recreate whatever was lost.

You should use commonly occurring real life data disaster scenarios to simulate what your backups will and won’t do in a crisis.

**Create disaster recovery center**

As a result, in the event of unplanned interruptions in the functioning of the primary location, service and all operational activities are switched to the backup center and therefore the unavailability of services is limited to the absolute minimum.

Does the facility have sufficient bandwidth options and power to scale and deal with the increased load during a major disaster? Are resources available to periodically test failover?

**Create regular backups and tested it!**

Backups are a way to protect the investment in data. By having several copies of the data, it does not matter as much if one is destroyed (the cost is only that of the restoration of the lost data from the backup).

When you lose data, one thing is certain: downtime.

To assure the validity and integrity of any backup, it's essential to carry out regular restoration tests. Ideally, a test should be conducted after every backup completes to ensure data can be successfully secured and recovered. However, this often isn't practical due to a lack of available resources or time constraints.

Make backups of entire virtual machines and important components in the middle of them.

**Create snapshots: vm, disks or lvm**

Snapshots are perfect if you want to recover a server from a previous state but it's only a "quick method", it cannot restore the system after too many items changed.

Create them always before making changes on production environments (and not only).

Disk snapshots are used to generate a snapshot of an entire disk. These snapshots don't make it easy to restore individual chunks of data (e.g. a lost user account), though it's possible. The primary purpose is to restore entire disks in case of disk failure.

The LVM snapshots can be primarily used to easily copy data from production environment to staging environment.

Remember: Snapshots are not backups!

**Development and testing environments**

A production environment is the real instance of the application and its database used by the company or the clients. The production database has all the real data.

Setting up development environments based directly on the production database, instead of using a backup for this (removing the need for the above). Dev and test environment that your engineers can get to and a prod environment that only a few people can push updates to following an approved change.

All environments such as prod, dev and test should have one major difference: authorization data for services. For example postgres database instance on testing environment should be consistent (if possible) with the production base, however, in order to eliminate errors of database names and logins and passwords for authorization should be different.

**Single point of failure**

The general method to avoid single points of failures is to provide redundant components for each necessary resource, so service can continue if a component fails.

**Synchronization and replication process for databases**

The replication procedure is super fragile and prone to error.

A good idea is also slightly longer delay of data replication (e.g. for DRC). As in replicas, the data changes will usually be replicated within minutes, so the lost data won’t be on the replica database either once that happens.

**Create database model with users, roles and rights, use different methods of protection**

Only very advanced devs have permissions for db admin access. The other really don't need write access to clone a database. On the other hand just don't give a developer write access to prod.

The production database should refuse connections from any server and pc which isn't the one running the production application, even if it provides a valid username/password.

How the hell development machines can access a production database right like that? How about a simple firewall rule to just let the servers needing the DB data access the database?

**Create summary/postmortem documents after failures**

The post-mortem audience includes customers, direct reports, peers, the company's executive team and often investors.

Explain what caused the outage on a timeline. Every incident begins with a specific trigger at a specific time, which often causes some unexpected behavior. For example, our servers were rebooted and we expected them to come back up intact, which didn't happen.

Furthermore, every incident has a root cause: the reboot itself was trigger, however a bug in the driver caused the actual outage. Finally, there are consequences to every incident, the most obvious one is that the site goes down.

The post-mortem answers the single most important question of what could have prevented the outage.

Despite how painful an outage may have been, the worst thing you can do is to bury it and never properly close the incident in a clear and transparent way.

**If you also made a big mistake...**

  > "*Humans are just apes with bigger computers.*" - african_cheetah (Reddit)
  >
  > "*I've come to appreciate not having access to things I don't absolutely need.*" - warm_vanilla_sugar (Reddit)
  >
  > Document whatever happened somewhere. Write setup guides. Failure is instructive.

Useful resources:

- [Accidentally destroyed production database on first day of a job...](https://www.reddit.com/r/cscareerquestions/comments/6ez8ag/accidentally_destroyed_production_database_on/)
- [Postmortem of database outage of January 31](https://about.gitlab.com/2017/02/10/postmortem-of-database-outage-of-january-31/)
- [How to write an Incident Report/Postmortem](https://sysadmincasts.com/episodes/20-how-to-write-an-incident-report-postmortem)

</details>

<details>
<summary><b>How to add new disk in Linux server without rebooting? How to rescan and add it in LVM?</b></summary><br>

To be completed.

Useful resources:

- [How to Add New Disk in Linux CentOS 7 Without Rebooting](https://linoxide.com/linux-how-to/add-new-disk-centos-7-without-rebooting/)

</details>

<details>
<summary><b>Explain each system calls used for process management in Linux.</b></summary><br>

There are some system calls for process management. These are as follows:

- `fork()`: it is used to create a new process
- `exec()`: it is used to execute a new process
- `wait()`: it is used to make the process to wait
- `exit()`: it is used to exit or terminate the process
- `getpid()`: it is used to find the unique process ID
- `getppid()`: it is used to check the parent process ID
- `nice()`: it is used to bias the currently running process property

Useful resources:

- [System Calls](http://faculty.salina.k-state.edu/tim/ossg/Introduction/sys_calls.html)

</details>

<details>
<summary><b>Can’t mount the root file system. Why? ***</b></summary><br>

To be completed.

Useful resources:

- [What does "mounting a root file system" mean exactly?](https://superuser.com/questions/193918/what-does-mounting-a-root-file-system-mean-exactly)
- [How does a kernel mount the root partition?](https://unix.stackexchange.com/questions/9944/how-does-a-kernel-mount-the-root-partition)

</details>

<details>
<summary><b>You have to delete 100GB files. Which method will be the most optimal? ***</b></summary><br>

To be completed.

Useful resources:

- [Is there a way to delete 100GB file on Linux without thrashing IO/load?](https://serverfault.com/questions/336917/is-there-a-way-to-delete-100gb-file-on-linux-without-thrashing-io-load)
- [rm on a directory with millions of files](https://serverfault.com/questions/183821/rm-on-a-directory-with-millions-of-files)

</details>

<details>
<summary><b>Explain interrupts and interrupt handlers in Linux.</b></summary><br>

Here's a high-level view of the low-level processing. I'm describing a simple typical architecture, real architectures can be more complex or differ in ways that don't matter at this level of detail.

When an **interrupt** occurs, the processor looks if interrupts are masked. If they are, nothing happens until they are unmasked. When interrupts become unmasked, if there are any pending interrupts, the processor picks one.

Then the processor executes the interrupt by branching to a particular address in memory. The code at that address is called the **interrupt handler**. When the processor branches there, it masks interrupts (so the interrupt handler has exclusive control) and saves the contents of some registers in some place (typically other registers).

The interrupt handler does what it must do, typically by communicating with the peripheral that triggered the interrupt to send or receive data. If the interrupt was raised by the timer, the handler might trigger the OS scheduler, to switch to a different thread. When the handler finishes executing, it executes a special return-from-interrupt instruction that restores the saved registers and unmasks interrupts.

The interrupt handler must run quickly, because it's preventing any other interrupt from running. In the Linux kernel, interrupt processing is divided in two parts:

- The "top half" is the interrupt handler. It does the minimum necessary, typically communicate with the hardware and set a flag somewhere in kernel memory.
- The "bottom half" does any other necessary processing, for example copying data into process memory, updating kernel data structures, etc. It can take its time and even block waiting for some other part of the system since it runs with interrupts enabled.

Useful resources:

- [How is an Interrupt handled in Linux? (original)](https://unix.stackexchange.com/questions/5788/how-is-an-interrupt-handled-in-linux)
- [Interrupts and Interrupt Handlers](https://notes.shichao.io/lkd/ch7/)

</details>

<details>
<summary><b>What considerations come into play when designing a highly available application, both at the architecture level and the application level? ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>What fields are stored in an inode?</b></summary><br>

Within a POSIX system, a file has the following attributes which may be retrieved by the stat system call:

- **Device ID** (this identifies the device containing the file; that is, the scope of uniqueness of the serial number).
File serial numbers
- The **file mode** which determines the file type and how the file's owner, its group, and others can access the file
- A **link count** telling how many hard links point to the inode
- The **User ID** of the file's owner
- The **Group ID** of the file
- The **device ID** of the file if it is a device file.
- The **size of the file** in bytes
- **Timestamps** telling when the inode itself was last modified (ctime, inode change time), the file content last modified (mtime, modification time), and last accessed (atime, access time)
- The preferred **I/O block size**
- The **number of blocks** allocated to this file

Useful resources:

- [Inodes - an Introduction](http://www.grymoire.com/Unix/Inodes.html)

</details>

<details>
<summary><b>Ordinary users are able to read <code>/etc/passwd</code>. Is it a security hole? Do you know other password shadowing scheme?</b></summary><br>

Typically, the _hashed passwords_ are stored in `/etc/shadow` on most Linux systems:

```bash
-rw-r----- 1 root shadow 1349 2016-07-03 03:54 /etc/shadow
```

They are stored in `/etc/master.passwd` on BSD systems.

Programs that need to perform authentication still need to run with `root` privileges:

```bash
-rwsr-xr-x 1 root root 42792 2016-02-14 14:13 /usr/bin/passwd
```

If you dislike the `setuid root` programs and one single file containing all the hashed passwords on your system, you can replace it with the **Openwall TCB PAM module**. This provides every single user with their own file for storing their hashed password - as a result the number of `setuid root` programs on the system can be drastically reduced.

Useful resources:

- [Ordinary users are able to read /etc/passwd, is this a security hole? (original)](https://serverfault.com/questions/286654/ordinary-users-are-able-to-read-etc-passwd-is-this-a-security-hole/286657#286657)
- [tcb - the alternative to /etc/shadow](https://www.openwall.com/tcb/)
- [Why shadow your passwd file?](https://www.tldp.org/HOWTO/Shadow-Password-HOWTO-2.html)

</details>

<details>
<summary><b>What are some of the benefits of using systemd over SysV init? ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>How do you run command every time a file is modified?</b></summary><br>

For example:

```bash
while inotifywait -e close_write filename ; do

  echo "changed" >> /var/log/changed

done
```

</details>

<details>
<summary><b>You need to copy a large amount of data. Explain the most effective way. ***</b></summary><br>

To be completed.

Useful resources:

- [Copying a large directory tree locally? cp or rsync?](https://serverfault.com/questions/43014/copying-a-large-directory-tree-locally-cp-or-rsync)

</details>

<details>
<summary><b>Tell me about the dangers and caveats of LVM.</b></summary><br>

**Risks of using LVM**

- Vulnerable to write caching issues with SSD or VM hypervisor
- Harder to recover data due to more complex on-disk structures
- Harder to resize filesystems correctly
- Snapshots are hard to use, slow and buggy
- Requires some skill to configure correctly given these issues

Useful resources:

- [LVM dangers and caveats (original)](https://serverfault.com/questions/279571/lvm-dangers-and-caveats)

</details>

<details>
<summary><b>Python dev team in your company have a dilemma what to choose: uwsgi or gunicorn. What are the pros/cons of each of the solutions from the admin's perspective? ***</b></summary><br>

To be completed.

Useful resources:

- [uWSGI vs. Gunicorn, or How to Make Python Go Faster than Node](https://blog.kgriffs.com/2012/12/18/uwsgi-vs-gunicorn-vs-node-benchmarks.html)

</details>

<details>
<summary><b>What if <code>kill -9</code> does not work? Describe exceptions for which the use of SIGKILL is insufficient.</b></summary><br>

`kill -9` (`SIGKILL`) always works, provided you have the permission to kill the process. Basically either the process must be started by you and not be setuid or setgid, or you must be root. There is one exception: even root cannot send a fatal signal to PID 1 (the init process).

However `kill -9` is not guaranteed to work immediately. All signals, including `SIGKILL`, are delivered asynchronously: the kernel may take its time to deliver them. Usually, delivering a signal takes at most a few microseconds, just the time it takes for the target to get a time slice. However, if the target has blocked the signal, the signal will be queued until the target unblocks it.

Normally, processes cannot block `SIGKILL`. But kernel code can, and processes execute kernel code when they call system calls.

A process blocked in a system call is in uninterruptible sleep. The `ps` or `top` command will (on most unices) show it in state **D**.

To remove a **D** State Process, since it is uninterruptible, only a machine reboot can solve the problem in case its not automatically handled by the system.

Usually there is a very few chance that a process stays in **D** State for long. And if it does then there is something not properly being handled in the system. This can be a potential bug as well.

A classical case of long uninterruptible sleep is processes accessing files over NFS when the server is not responding; modern implementations tend not to impose uninterruptible sleep (e.g. under Linux, the intr mount option allows a signal to interrupt NFS file accesses).

You may sometimes see entries marked **Z** (or **H** under Linux) in the `ps` or `top` output. These are technically not processes, they are zombie processes, which are nothing more than an entry in the process table, kept around so that the parent process can be notified of the death of its child. They will go away when the parent process pays attention (or dies).

Summary exceptions:

- Zombie processes cannot be killed since they are already dead and waiting for their parent processes to reap them
- Processes that are in the blocked state will not die until they wake up again
- The init process is special: It does not get signals that it does not want to handle, and thus it can ignore **SIGKILL**. An exception from this exception is while init is ptraced on Linux
- An uninterruptibly sleeping process may not terminate (and free its resources) even when sent **SIGKILL**. This is one of the few cases in which a Unix system may have to be rebooted to solve a temporary software problem

Useful resources:

- [What if kill -9 does not work? (original)](https://unix.stackexchange.com/questions/5642/what-if-kill-9-does-not-work)
- [How to kill a process in Linux if kill -9 has no effect](https://serverfault.com/questions/458261/how-to-kill-a-process-in-linux-if-kill-9-has-no-effect)
- [When should I not kill -9 a process?](https://unix.stackexchange.com/questions/8916/when-should-i-not-kill-9-a-process)
- [SIGTERM vs. SIGKILL](https://major.io/2010/03/18/sigterm-vs-sigkill/)

</details>

<details>
<summary><b>Difference between <code>nohup</code>, <code>disown</code>, and <code>&</code>. What happens when using all together?</b></summary><br>

- `&` puts the job in the background, that is, makes it block on attempting to read input, and makes the shell not wait for its completion
- `disown` removes the process from the shell's job control, but it still leaves it connected to the terminal. One of the results is that the shell won't send it a **SIGHUP**. Obviously, it can only be applied to background jobs, because you cannot enter it when a foreground job is running
- `nohup` disconnects the process from the terminal, redirects its output to `nohup.out` and shields it from **SIGHUP**. One of the effects (the naming one) is that the process won't receive any sent **SIGHUP**. It is completely independent from job control and could in principle be used also for foreground jobs (although that's not very useful)

If you use all three together, the process is running in the background, is removed from the shell's job control and is effectively disconnected from the terminal.

Useful resources:

- [Difference between nohup, disown and & (original)](https://unix.stackexchange.com/questions/3886/difference-between-nohup-disown-and)

</details>

<details>
<summary><b>What is the main advantage of using <code>chroot</code>? When and  why do we use it? What is the purpose of the mount dev, proc, sys in a chroot environment?</b></summary><br>

An advantage of having a chroot environment is the file-system is totally isolated from the physical host. `chroot` has a separate file-system inside the file-system, the difference is its uses a newly created root(/) as its root directory.

A chroot jail is a way to isolate a process and its children from the rest of the system. It should only be used for processes that don't run as root, as root users can break out of the jail very easily.

The idea is that you create a directory tree where you copy or link in all the system files needed for a process to run. You then use the `chroot()` system call to change the root directory to be at the base of this new tree and start the process running in that chroot'd environment. Since it can't actually reference paths outside the modified root, it can't perform operations (read/write etc.) maliciously on those locations.

On Linux, using a bind mounts is a great way to populate the chroot tree. Using that, you can pull in folders like `/lib` and `/usr/lib` while not pulling in `/usr`, for example. Just bind the directory trees you want to directories you create in the jail directory.

Chroot environment is useful for:

- reinstall bootloader
- reset a forgotten password
- perform a kernel upgrade (or downgrade)
- rebuild your initramdisk
- fix your **/etc/fstab**
- reinstall packages using your package manager
- whatever

When working in a chrooted environment, there is a few special file systems that needs to be mounted so all programs behave properly.

Limitation is that `/dev`, `/sys` and `/proc` are not mounted by default but needed for many tasks.

Useful resources:

- [Its all about Chroot](https://medium.com/@itseranga/chroot-316dc3c89584)
- [Best Practices for UNIX chroot() Operations](http://www.unixwiz.net/techtips/chroot-practices.html)
- [Is there an easier way to chroot than bind-mounting?](https://askubuntu.com/questions/32418/is-there-an-easier-way-to-chroot-than-bind-mounting)
- [What's the proper way to prepare chroot to recover a broken Linux installation?](https://superuser.com/questions/111152/whats-the-proper-way-to-prepare-chroot-to-recover-a-broken-linux-installation)

</details>

<details>
<summary><b>What are segmentation faults (segfaults), and how can identify what's causing them?</b></summary><br>

A **segmentation fault** (aka _segfault_) is a common condition that causes programs to crash. Segfaults are caused by a program trying to read or write an illegal memory location.

Program memory is divided into different segments:

- a text segment for program instructions
- a data segment for variables and arrays defined at compile time
- a stack segment for temporary (or automatic) variables defined in subroutines and functions
- a heap segment for variables allocated during runtime by functions, such as `malloc` (in C)

In practice, segfaults are almost always due to trying to read or write a non-existent array element, not properly defining a pointer before using it, or (in C programs) accidentally using a variable's value as an address. Thus, when Process A reads memory location 0x877, it reads information residing at a different physical location in RAM than when Process B reads its own 0x877.

All modern operating systems support and use segmentation, and so all can produce a segmentation fault.

Segmentation fault can also occur under following circumstances:

- a buggy program/command, which can be only fixed by applying patch
- it can also appear when you try to access an array beyond the end of an array under C programming
- inside a chrooted jail this can occur when critical shared libs, config file or `/dev/` entry missing
- sometime hardware or faulty memory or driver can also create problem
- maintain suggested environment for all computer equipment (overheating can also generate this problem)

To debug this kind of error try one or all of the following techniques:

- enable core files: `$ ulimit -c unlimited`
- reproduce the crash: `$ ./<program>`
- debug crash with gdb: `$ gdb <program> [core file]`
- or run `LD_PRELOAD=...path-to.../libSegFault.so <program>` to get a report with backtrace, loaded libs, etc

Also:

- make sure correct hardware installed and configured
- always apply all patches and use updated system
- make sure all dependencies installed inside jail
- turn on core dumping for supported services such as Apache
- use `strace` which is a useful diagnostic, instructional, and debugging tool

Sometimes segmentation faults are not caused by bugs in the program but are caused instead by system memory limits being set too low. Usually it is the limit on stack size that causes this kind of problem (stack overflows). To check memory limits, use the `ulimit` command in bash.

Useful resources:

- [What are segmentation faults (segfaults), and how can I identify what's causing them? (original)](https://kb.iu.edu/d/aqsj)
- [What is a segmentation fault on Linux?](https://stackoverflow.com/questions/3200526/what-is-a-segmentation-fault-on-linux)
- [Segmentation fault when calling a recursive bash function](https://unix.stackexchange.com/questions/296641/segmentation-fault-when-calling-a-recursive-bash-function)
- [Troubleshooting Segmentation Violations/Faults](http://web.mit.edu/10.001/Web/Tips/tips_on_segmentation.html)
- [Can one use libSegFault.so to get backtraces for SIGABRT?](https://stackoverflow.com/questions/18706496/can-one-use-libsegfault-so-to-get-backtraces-for-sigabrt)

</details>

<details>
<summary><b>One of the processes runs slowly. How to check how long has been running and which tools will you use?</b></summary><br>

To be completed.

Useful resources:

- [How to check how long a process has been running?](https://unix.stackexchange.com/questions/7870/how-to-check-how-long-a-process-has-been-running)
- [Linux how long a process has been running?](https://www.cyberciti.biz/faq/how-to-check-how-long-a-process-has-been-running/)
- [How to see system call that executed in current time by process?](https://stackoverflow.com/questions/42677724/how-to-see-system-call-that-executed-in-current-time-by-process)

</details>

<details>
<summary><b>What is a file descriptor in Linux?</b></summary><br>

In Unix and related computer operating systems, a file descriptor (FD, less frequently fildes) is an abstract indicator (handle) used to access a file or other input/output resource, such as a pipe or network socket. File descriptors form part of the POSIX application programming interface.

</details>

<details>
<summary><b>Which way of additionally feeding random entropy pool would you suggest for producing random passwords? How to improve it?</b></summary><br>

You should use `/dev/urandom`, not `/dev/random`. The two differences between `/dev/random` and `/dev/urandom` are:

 - `/dev/random` might be theoretically better _in the context of an information-theoretically secure algorithm_. This is the kind of algorithm which is secure against today's technology, and also tomorrow's technology, and technology used by aliens, and God's own iPad as well.

 - `/dev/urandom` will not block, while `/dev/random` may do so. `/dev/random` maintains a counter of "how much entropy it still has" under the assumption that any bits it has produced is a lost entropy bit. Blocking induces very real issues, e.g. a server which fails to boot after an automated install because it is stalling on its SSH server key creation.

So you want to use `/dev/urandom` and stop to worry about this entropy business.

The trick is that `/dev/urandom` never blocks, ever, even when it should: `/dev/urandom` is secure as long as it has received enough bytes of "initial entropy" since the last boot (32 random bytes are enough). A normal Linux installation will create a random seed (from `/dev/random`) upon installation, and save it on the disk. Upon each reboot, the seed will be read, fed into `/dev/urandom`, and a new seed immediately generated (from `/dev/urandom`) to replace it. Thus, this guarantees that `/dev/urandom` will always have enough initial entropy to produce cryptographically strong alea, perfectly sufficient for any mundane cryptographic job, including password generation.

Should any of these daemons require randomness when all available entropy has been exhausted, they may pause to wait for more, which can cause excessive delays in your application. Even worse, since most modern applications will either resort to using its own random seed created at program initialization, or to using `/dev/urandom` to avoid blocking, your applications will suffer from lower quality random data. This can affect the integrity of your secure communications, and can increase the chance of cryptoanalysis on your private data.

To check the amount of bytes of entropy currently available, use:

```bash
cat /proc/sys/kernel/random/entropy_avail
```

**rng-tools**

Fedora/Rh/Centos types: `sudo yum install rng-tools`.

On deb types: `sudo apt-get install rng-tools` to set it up.

Then run `sudo rngd -r /dev/urandom` before generating the keys.

**haveged**

Fedora/Rh/Centos types: `sudo yum install haveged` and add `/usr/local/sbin/haveged -w 1024` to `/etc/rc.local`.

On deb types: `sudo apt-get install haveged` and add `DAEMON_ARGS="-w 1024"` to `/etc/default/haveged` to set it up.

Then run `sudo rngd -r /dev/urandom` before generating the keys.

Useful resources:

- [Feeding /dev/random entropy pool? (original)](https://security.stackexchange.com/questions/89/feeding-dev-random-entropy-pool)
- [GPG does not have enough entropy](https://serverfault.com/questions/214605/gpg-does-not-have-enough-entropy)

</details>

<details>
<summary><b>What is the difference between <code>/sbin/nologin</code>, <code>/bin/false</code>, and <code>/bin/true</code>?</b></summary><br>

When `/sbin/nologin` is set as the shell, if user with that shell logs in, they'll get a polite message saying 'This account is currently not available'.

`/bin/false` is just a binary that immediately exits, returning false, when it's called, so when someone who has false as shell logs in, they're immediately logged out when false exits. Setting the shell to `/bin/true` has the same effect of not allowing someone to log in but false is probably used as a convention over true since it's much better at conveying the concept that person doesn't have a shell.

`/bin/nologin` is the more user-friendly option, with a customizable message given to the user trying to log in, so you would theoretically want to use that; but both nologin and false will have the same end result of someone not having a shell and not being able to ssh in.

Useful resources:

- [What's the difference between /sbin/nologin and /bin/false](https://unix.stackexchange.com/questions/10852/whats-the-difference-between-sbin-nologin-and-bin-false)
- [Why do some system users have /usr/bin/false as their shell?](https://superuser.com/questions/1183311/why-do-some-system-users-have-usr-bin-false-as-their-shell)

</details>

<details>
<summary><b>Which symptoms might be suffering from a disk bottleneck? ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>What is the meaning of the error <code>maxproc limit exceeded by uid %i ...</code> in FreeBSD?</b></summary><br>

The FreeBSD kernel will only allow a certain number of processes to exist at one time. The number is based on the **kern.maxusers** variable.

**kern.maxusers** also affects various other in-kernel limits, such as network buffers. If the machine is heavily loaded, increase **kern.maxusers**. This will increase these other system limits in addition to the maximum number of processes.

To adjust the **kern.maxusers** value, see the File/Process Limits section of the Handbook. While that section refers to open files, the same limits apply to processes.

If the machine is lightly loaded but running a very large number of processes, adjust the **kern.maxproc** tunable by defining it in `/boot/loader.conf`.

</details>

<details>
<summary><b>How to read a file line by line and assigning the value to a variable?</b></summary><br>

For example:

```bash
while IFS='' read -r line || [[ -n "$line" ]] ; do
  echo "Text read from file: $line"
done < "/path/to/filename"
```

Explanation:

- `IFS=''` (or `IFS=`) prevents leading/trailing whitespace from being trimmed.
- `-r` prevents backslash escapes from being interpreted.
- `|| [[ -n $line ]]` prevents the last line from being ignored if it doesn't end with a `\n` (since  read returns a non-zero exit code when it encounters EOF).

Useful resources:

- [Read a file line by line assigning the value to a variable](https://stackoverflow.com/questions/10929453/read-a-file-line-by-line-assigning-the-value-to-a-variable)

</details>

<details>
<summary><b>The client reports that his site received a grade B in the ssllabs scanner. Prepare a checklist of best practice for ssl configuration. ***</b></summary><br>

Useful resources:

- [Getting a Perfect SSL Labs Score](https://michael.lustfield.net/nginx/getting-a-perfect-ssl-labs-score)
- [17 small suggestions how to improve ssllabs.com/ssltest/](https://community.qualys.com/thread/14023)
- [How do you score A+ with 100 on all categories on SSL Labs test with Let's Encrypt and Nginx?](https://stackoverflow.com/questions/41930060/how-do-you-score-a-with-100-on-all-categories-on-ssl-labs-test-with-lets-encry)

</details>

<details>
<summary><b>What does CPU jumps mean?</b></summary><br>

An OS is a very busy thing, particularly so when you have it doing something (and even when you aren't). And when we are looking at an active enterprise environment, something is always going on.

Most of this activity is "bursty", meaning processes are typically quiescent with short periods of intense activity. This is certainly true of any type of network-based activity (e.g. processing PHP requests), but also applies to OS maintenance (e.g. file system maintenance, page reclamation, disk I/O requests).

If you take a situation where you have a lot of such bursty processes, you get a very irregular and spiky CPU usage plot.

As `500 - Internal Server Error` says, the high number of context switches are going to make the situation even worse.

Useful resources:

- [What does "CPU jumps” mean? (original)](https://stackoverflow.com/questions/32185607/what-does-cpu-jumps-mean)

</details>

<details>
<summary><b>How do you trace a system call in Linux? Explain the possible methods.</b></summary><br>

**SystemTap**

This is the most powerful method. It can even show the call arguments:

Usage:

```bash
sudo apt-get install systemtap
sudo stap -e 'probe syscall.mkdir { printf("%s[%d] -> %s(%s)\n", execname(), pid(), name, argstr) }'
```

Then on another terminal:

```bash
sudo rm -rf /tmp/a /tmp/b
mkdir /tmp/a
mkdir /tmp/b
```

Sample output:

```bash
mkdir[4590] -> mkdir("/tmp/a", 0777)
mkdir[4593] -> mkdir("/tmp/b", 0777)
```

**`strace` with `-f|-ff` params**

You can use the `-f` and `-ff` option. Something like this:

```bash
strace -f -e trace=process bash -c 'ls; :
```

- `-f` : Trace child processes as they are created by currently traced processes as a result of the fork(2) system call.

- `-ff` : If the `-o` filename option is in effect, each processes trace is written to filename.pid where pid is the numeric process id of each process. This is incompatible with `-c`, since no per-process counts are kept.

**`ltrace -S` shows both system calls and library calls**

This awesome tool therefore gives even further visibility into what executables are doing.

**`ftrace` minimal runnable example**

Here goes a minimal runnable example. Run with `sudo`:

```bash
#!/bin/sh
set -eux

d=debug/tracing

mkdir -p debug
if ! mountpoint -q debug; then
  mount -t debugfs nodev debug
fi

# Stop tracing.
echo 0 > "${d}/tracing_on"

# Clear previous traces.
echo > "${d}/trace"

# Find the tracer name.
cat "${d}/available_tracers"

# Disable tracing functions, show only system call events.
echo nop > "${d}/current_tracer"

# Find the event name with.
grep mkdir "${d}/available_events"

# Enable tracing mkdir.
# Both statements below seem to do the exact same thing,
# just with different interfaces.
# https://www.kernel.org/static/html/v4.18/trace/events.html
echo sys_enter_mkdir > "${d}/set_event"
# echo 1 > "${d}/events/syscalls/sys_enter_mkdir/enable"

# Start tracing.
echo 1 > "${d}/tracing_on"

# Generate two mkdir calls by two different processes.
rm -rf /tmp/a /tmp/b
mkdir /tmp/a
mkdir /tmp/b

# View the trace.
cat "${d}/trace"

# Stop tracing.
echo 0 > "${d}/tracing_on"

umount debug
```

Sample output:

```bash
# tracer: nop
#
#                              _-----=> irqs-off https://sourceware.org/systemtap/documentation.html
#                             / _----=> need-resched
#                            | / _---=> hardirq/softirq
#                            || / _--=> preempt-depth
#                            ||| /     delay
#           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION
#              | |       |   ||||       |         |
            mkdir-5619  [005] .... 10249.262531: sys_mkdir(pathname: 7fff93cbfcb0, mode: 1ff)
            mkdir-5620  [003] .... 10249.264613: sys_mkdir(pathname: 7ffcdc91ecb0, mode: 1ff)
```

One cool thing about this method is that it shows the function call for all processes on the system at once, although you can also filter PIDs of interest with `set_ftrace_pid`.

Useful resources:

- [How do I trace a system call in Linux? (original)](https://stackoverflow.com/questions/29840213/how-do-i-trace-a-system-call-in-linux)
- [Does ftrace allow capture of system call arguments to the Linux kernel, or only function names?](https://stackoverflow.com/questions/27608752/does-ftrace-allow-capture-of-system-call-arguments-to-the-linux-kernel-or-only)
- [How to trace just system call events with ftrace without showing any other functions in the Linux kernel?](https://stackoverflow.com/questions/52764544/how-to-trace-just-system-call-events-with-ftrace-without-showing-any-other-funct)
- [What system call is used to load libraries in Linux?](https://unix.stackexchange.com/questions/226524/what-system-call-is-used-to-load-libraries-in-linux)

</details>

<details>
<summary><b>How to remove all files except some from a directory?</b></summary><br>

Solution 1 - with `extglob`:

```bash
shopt -s extglob
rm !(textfile.txt|backup.tar.gz|script.php|database.sql|info.txt)
```

Solution 2 - with `find`:

```bash
find . -type f -not -name '*txt' -print0 | xargs -0 rm --
```

</details>

<details>
<summary><b>How to check if a string contains a substring in Bash?</b></summary><br>

You can use `*` (wildcards) outside a case statement, too, if you use double brackets:

```bash
string='some text'
if [[ $string = *"My long"* ]] ; then
  true
fi
```

</details>

<details>
<summary><b>Explain differences between <code>2>&-</code>, <code>2>/dev/null</code>, <code>|&</code>, <code>&>/dev/null</code>, and <code>>/dev/null 2>&1</code>.</b></summary><br>

- a **number 1** = standard out (i.e. `STDOUT`)
- a **number 2** = standard error (i.e. `STDERR`)
- if a number isn't explicitly given, then **number 1** is assumed by the shell (bash)

First let's tackle the function of these.

`2>&-`

The general form of this one is `M>&-`, where **"M"** is a file descriptor number. This will close output for whichever file descriptor is referenced, i.e. **"M"**.

`2>/dev/null`

The general form of this one is `M>/dev/null`, where **"M"** is a file descriptor number. This will redirect the file descriptor, **"M"**, to `/dev/null`.

`2>&1`

The general form of this one is `M>&N`, where **"M"** & **"N"** are file descriptor numbers. It combines the output of file descriptors **"M"** and **"N"** into a single stream.

`|&`

This is just an abbreviation for `2>&1 |`. It was added in Bash 4.

`&>/dev/null`

This is just an abbreviation for `>/dev/null 2>&1`. It redirects file descriptor 2 (`STDERR`) and descriptor 1 (`STDOUT`) to `/dev/null`.

`>/dev/null`

This is just an abbreviation for `1>/dev/null`. It redirects file descriptor 1 (`STDOUT`) to `/dev/null`.

Useful resources:

- [Difference between 2>&-, 2>/dev/null, |&, &>/dev/null and >/dev/null 2>&1](https://unix.stackexchange.com/questions/70963/difference-between-2-2-dev-null-dev-null-and-dev-null-21)
- [Chapter 20. I/O Redirection](http://www.tldp.org/LDP/abs/html/io-redirection.html)

</details>

<details>
<summary><b>How to redirect stderr and stdout to different files in the same line?</b></summary><br>

Just add them in one line `command 2>> error 1>> output`.

However, note that `>>` is for appending if the file already has data. Whereas, `>` will overwrite any existing data in the file.

So, `command 2> error 1> output` if you do not want to append.

Just for completion's sake, you can write `1>` as just `>` since the default file descriptor is the output. so `1>` and `>` is the same thing.

So, `command 2> error 1> output` becomes, `command 2> error > output`.

</details>

<details>
<summary><b>Load averages are above 30 on a server with 24 cores but CPU shows around 70 percent idle. One of the common causes of this condition is? How to debug and fixed?</b></summary><br>

Requests which involve disk I/O can be slowed greatly if cpu(s) needs to wait on the disk to read or write data. I/O Wait, is the percentage of time the CPU has to wait on disk.

Lets looks at how we can confirm if disk I/O is slowing down application performance by using a few terminal command line tools (`top`, `atop` and `iotop`).

Example of debug:

- answering whether or not I/O is causing system slowness
- finding which disk is being written to
- finding the processes that are causing high I/O
- process list **state**
- finding what files are being written too heavily
- do you see your copy process put in **D** state waiting for I/O work to be done by pdflush?
- do you see heavy synchronous write activity on your disks?

also:

- using `top` command - load averages and wa (wait time)
- using `atop` command to monitor DSK (disk) I/O stats
- using `iotop` command for real-time insight on disk read/writes

For improvement performance:

- check drive array configuration
- check disk queuing algorithms and tuning them
- tuning general block I/O parameters
- tuning virtual memory management to improve I/O performance
- check and tuning mount options and filesystem params (also responsible for cache)

Useful resources:

- [Linux server performance: Is disk I/O slowing your application? (original)](https://haydenjames.io/linux-server-performance-disk-io-slowing-application/)
- [Troubleshooting High I/O Wait in Linux](https://bencane.com/2012/08/06/troubleshooting-high-io-wait-in-linux/)
- [Debugging Linux I/O latency](https://superuser.com/questions/396696/debugging-linux-i-o-latency)
- [How do pdflush, kjournald, swapd, etc interoperate?](https://unix.stackexchange.com/questions/76970/how-do-pdflush-kjournald-swapd-etc-interoperate)
- [5 ways to improve HDD speed on Linux](https://thecodeartist.blogspot.com/2012/06/improving-hdd-performance-linux.html)

</details>

<details>
<summary><b>How to enforce authorization methods in SSH? In what situations it would be useful?</b></summary><br>

Force login with a password:

```bash
ssh -o PreferredAuthentications=password -o PubkeyAuthentication=no user@remote_host
```

Force login using the key:

```bash
ssh -o PreferredAuthentications=publickey -o PubkeyAuthentication=yes -i id_rsa user@remote_host
```

Useful resources:

- [How to force ssh client to use only password auth?](https://unix.stackexchange.com/questions/15138/how-to-force-ssh-client-to-use-only-password-auth)

</details>

<details>
<summary><b>Getting <code>Too many Open files</code> error for Postgres. How to resolve it?</b></summary><br>

Fixed the issue by reducing `max_files_per_process` e.g. to 200 from default 1000. This parameter is in `postgresql.conf` file and this sets the maximum number of simultaneously open files allowed to each server subprocess.

Usually people start to edit `/etc/security/limits.conf` file, but forget that this file only apply to the actively logged in users through the PAM system.

</details>

<details>
<summary><b>In what circumstance can <code>df</code> and <code>du</code> disagree on available disk space? How do you solve it?</b></summary><br>

`du` checks usage of directories, but `df` checks free'd inodes, and files can be held open and take space after they're deleted.

**Solution 1**

Check for files on located under mount points. Frequently if you mount a directory (say a sambafs) onto a filesystem that already had a file or directories under it, you lose the ability to see those files, but they're still consuming space on the underlying disk.

I've had file copies while in single user mode dump files into directories that I couldn't see except in single usermode (due to other directory systems being mounted on top of them).

**Solution 2**

On the other hand `df -h` and `du -sh` could mismatched by about 50% of the hard disk size. This was caused by e.g. Apache (httpd) keeping large log files in memory which had been deleted from disk.

This was tracked down by running `lsof | grep "/var" | grep deleted` where `/var` was the partition I needed to clean up.

The output showed lines like this:

```
httpd     32617    nobody  106w      REG        9,4 1835222944     688166 /var/log/apache/awstats_log (deleted)
```

The situation was then resolved by restarting Apache (`service httpd restart`) and cleared of disk space, by allowing the locks on deleted files to be cleared.

Useful resources:

- [Why du and df display different values in Linux and Unix](https://linuxshellaccount.blogspot.com/2008/12/why-du-and-df-display-different-values.html)

</details>

<details>
<summary><b>What is the difference between encryption and hashing?</b></summary><br>

**Hashing**: Finally, hashing is a form of cryptographic security which differs from **encryption** whereas **encryption** is a two step process used to first encrypt and then decrypt a message, **hashing** condenses a message into an irreversible fixed-length value, or hash.

</details>

<details>
<summary><b>Should the root certificate go on the server?</b></summary><br>

**Self-signed root certificates** need not/should not be included in web server configuration. They serve no purpose (clients will always ignore them) and they incur a slight performance (latency) penalty because they increase the size of the SSL handshake.

If the client does not have the root in their trust store, then it won't trust the web site, and there is no way to work around that problem. Having the web server send the root certificate will not help - the root certificate has to come from a trusted 3rd party (in most cases the browser vendor).

Useful resources:

- [SSL root certificate optional?](https://security.stackexchange.com/questions/65332/ssl-root-certificate-optional)

</details>

<details>
<summary><b>How to log all commands run by root on production servers?</b></summary><br>

`auditd` is the correct tool for the job here:

1. Add these 2 lines to `/etc/audit/audit.rules`:

```bash
-a exit,always -F arch=b64 -F euid=0 -S execve
-a exit,always -F arch=b32 -F euid=0 -S execve
```

These will track all commands run by root (euid=0). Why two rules? The execve syscall must be tracked in both 32 and 64 bit code.

2. To get rid of `auid=4294967295` messages in logs, add `audit=1` to the kernel's cmdline (by editing `/etc/default/grub`)

3. Place the line

```bash
session  required                pam_loginuid.so
```

in all PAM config files that are relevant to login (`/etc/pam.d/{login,kdm,sshd}`), but not in the files that are relevant to su or sudo. This will allow auditd to get the calling user's uid correctly when calling sudo or su.

Restart your system now.

Let's login and run some commands:

```bash
$ id -u
1000
$ sudo ls /
bin  boot  data  dev  etc  home  initrd.img  initrd.img.old  lib  lib32  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  scratch  seLinux  srv  sys  tmp  usr  var  vmlinuz  vmlinuz.old
$ sudo su -
# ls /etc
[...]
```

Now read `/var/log/audit/auditd.log` for show what has been logged in.

Useful resources:

- [Log all commands run by admins on production servers](https://serverfault.com/questions/470755/log-all-commands-run-by-admins-on-production-servers)

</details>

<details>
<summary><b>How to prevent <code>dd</code> from freezing your system?</b></summary><br>

Try using ionice:

```bash
ionice -c3 dd if=/dev/zero of=file
```

This start the `dd` process with the "idle" IO priority: it only gets disk time when no other process is using disk IO for a certain amount of time.

Of course this can still flood the buffer cache and cause freezes while the system flushes out the cache to disk. There are tunables under `/proc/sys/vm/` to influence this, particularly the `dirty_*` entries.

</details>

<details>
<summary><b>How to limit processes to not exceed more than X% of CPU usage?</b></summary><br>

**nice/renice**

nice is a great tool for 'one off' tweaks to a system:

```bash
nice COMMAND
```

**cpulimit**

cpulimit if you need to run a CPU intensive job and having free CPU time is essential for the responsiveness of a system:

```bash
cpulimit -l 50 COMMAND
```

**cgroups**

cgroups apply limits to a set of processes, rather than to just one:

```bash
cgcreate -g cpu:/cpulimited
cgset -r cpu.shares=512 cpulimited
cgexec -g cpu:cpulimited COMMAND_1
cgexec -g cpu:cpulimited COMMAND_2
cgexec -g cpu:cpulimited COMMAND_3
```

</details>

<details>
<summary><b>How mount a temporary ram partition?</b></summary><br>

```bash
# -t - filesystem type
# -o - mount options
mount -t tmpfs tmpfs /mnt -o size=64M
```

</details>

<details>
<summary><b>How to kills a process that is locking a file?</b></summary><br>

```bash
fuser -k filename
```

</details>

<details>
<summary><b>Other admin trying to debug a server accidentally typed: <code>chmod -x /bin/chmod</code>. How to reset permissions back to default?</b></summary><br>

```bash
# 1:
cp /bin/ls chmod.01
cp /bin/chmod chmod.01
./chmod.01 700 file

# 2:
/bin/busybox chmod 0700 /bin/chmod

# 3:
setfacl --set u::rwx,g::---,o::--- /bin/chmod

# 4:
/usr/lib/ld*.so /bin/chmod 0700 /bin/chmod
```

Useful resources:

- [What can you do when you can't chmod chmod?](https://www.networkworld.com/article/3002286/operating-systems/what-can-you-do-when-you-cant-chmod-chmod.html)

</details>

<details>
<summary><b><code>grub></code> vs <code>grub-rescue></code>. Explain.</b></summary><br>

- `grub>` - this is the mode to which it passes if you find everything you need to run the system in addition to the configuration file. With this mode, we have access to most (if not all) modules and commands. This mode can be called from the menu by pressing the 'c' key
- `grub-rescue` - this is the mode to which it passes if it is impossible to find its own directory (especially the directory with modules and additional commands, e.g. directory `/boot/grub/i386-pc`), if its contents are damaged or if no normal module is found, contains only basic commands

</details>

<details>
<summary><b>How to check whether the private key and the certificate match?</b></summary><br>

```bash
(openssl rsa -noout -modulus -in private.key | openssl md5 ; openssl x509 -noout -modulus -in certificate.crt | openssl md5) | uniq
```

</details>

<details>
<summary><b>How to add new user without using <code>useradd</code>/<code>adduser</code> commands?</b></summary><br>

1. Add an entry of user details in <code>/etc/passwd</code> with `vipw`:

```bash
# username:password:UID:GID:Comments:Home_Directory:Login Shell
user:x:501:501:test user:/home/user:/bin/bash
```

  > Be careful with the syntax. Do not edit directly with an editor. `vipw` locks the file, so that other commands won't try to update it at the same time.

2. You will have to create a group with same name in <code>/etc/group</code> with `vigr` (similar tool for `vipw`):

```bash
user:x:501:
```

3. Assign a password to the user:

```bash
passwd user
```

4. Create the home directory of the user with mkdir:

```bash
mkdir -m 0700 /home/user
```

5. Copy the files from `/etc/skel` to the new home directory:

```bash
rsync -av --delete /etc/skel/ /home/user
```

6. Fix ownerships and permissions with `chown` and `chmod`:

```bash
chown -R user:user /home/user
chmod -R go-rwx /home/user
```

Useful resources:

- [What steps to add a user to a system without using useradd/adduser?](https://unix.stackexchange.com/questions/153225/what-steps-to-add-a-user-to-a-system-without-using-useradd-adduser)

</details>

<details>
<summary><b>Why do we need <code>mktemp</code> command? Present an example of use.</b></summary><br>

<code>mktemp</code> randomizes the name. It is very important from the security point of view.

Just imagine that you do something like:

```bash
echo "random_string" > /tmp/temp-file
```

in your root-running script. And someone (who has read your script) does

```bash
ln -s /etc/passwd /tmp/temp-file
```

The <code>mktemp</code> command could help you in this situation:

```bash
TEMP=$(mktemp /tmp/temp-file.XXXXXXXX)
echo "random_string" > ${TEMP}
```

Now this <code>ln /etc/passwd</code> attack will not work.

</details>

<details>
<summary><b>Is it safe to attach the <code>strace</code> to a running process on the production? What are the consequences?</b></summary><br>

`strace` is the system call tracer for Linux. It currently uses the arcane `ptrace()` (process trace) debugging interface, which operates in a violent manner: **pausing the target process** for each syscall so that the debugger can read state. And doing this twice: when the syscall begins, and when it ends.

This means `strace` pauses your application twice for each syscall, and context-switches each time between the application and `strace`. It's like putting traffic metering lights on your application.

Cons:

- can cause significant and sometimes massive performance overhead, in the worst case, slowing the target application by over 100x. This may not only make it unsuitable for production use, but any timing information may also be so distorted as to be misleading
- can't trace multiple processes simultaneously (with the exception of followed children)
- visibility is limited to the system call interface

Useful resources:

- [strace Wow Much Syscall (original)](http://www.brendangregg.com/blog/2014-05-11/strace-wow-much-syscall.html)

</details>

<details>
<summary><b>What is the easiest, safest and most portable way to remove <code>-rf</code> directory entry?</b></summary><br>

They're effective but not optimally portable:

- <code>rm -- -fr</code>
- <code>perl -le 'unlink("-fr");'</code>

People who go on about shell command line quoting and character escaping are almost as dangerous as those who simply don't even recognize why a file name like that poses any problem at all.

The most portable solution:

```bash
rm ./-fr
```

</details>

<details>
<summary><b>Write a simple bash script (or pair of scripts) to backup and restore your system. ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>What are salted hashes? Generate the password with salt for the <code>/etc/shadow</code> file.</b></summary><br>

**Salt** at its most fundamental level is random data. When a properly protected password system receives a new password, it will create a hashed value for that password, create a new random salt value, and then store that combined value in its database. This helps defend against dictionary attacks and known hash attacks.

For example, if a user uses the same password on two different systems, if they used the same hashing algorithm, they could end up with the same hash value. However, if even one of the systems uses salt with its hashes, the values will be different.

The encrypted passwords in `/etc/shadow` file are stored in the following format:

```bash
$ID$SALT$ENCRYPTED
```

The `$ID` indicates the type of encryption, the `$SALT` is a random (up to 16 characters) string and `$ENCRYPTED` is a password’s hash.

<table style="width:100%">
  <tr>
    <th>Hash Type</th>
    <th>ID</th>
    <th>Hash Length</th>
  </tr>
  <tr>
    <td>MD5</td>
    <td>$1</td>
    <td>22 characters</td>
  </tr>
  <tr>
    <td>SHA-256</td>
    <td>$5</td>
    <td>43 characters</td>
  </tr>
  <tr>
    <td>SHA-512</td>
    <td>$6</td>
    <td>86 characters</td>
  </tr>
</table>

Use the below commands from the Linux shell to generate hashed password for `/etc/shadow` with the random salt:

- Generate **MD5** password hash

```bash
python -c "import random,string,crypt; randomsalt = ''.join(random.sample(string.ascii_letters,8)); print crypt.crypt('MySecretPassword', '\$1\$%s\$' % randomsalt)"
```

- Generate **SHA-256** password hash

```bash
python -c "import random,string,crypt; randomsalt = ''.join(random.sample(string.ascii_letters,8)); print crypt.crypt('MySecretPassword', '\$5\$%s\$' % randomsalt)"
```

- Generate **SHA-512** password hash

```bash
python -c "import random,string,crypt; randomsalt = ''.join(random.sample(string.ascii_letters,8)); print crypt.crypt('MySecretPassword', '\$6\$%s\$' % randomsalt)"
```

</details>

###### Network Questions (27)

<details>
<summary><b>Create SPF records for your site to help control spam.</b></summary><br>

* Start with the SPF version, this part defines the record as SPF. An SPF record should always start with the version number v=spf1 (version 1) this tag defines the record as SPF. There used to be a second version of SPF (called: SenderID), but this was discontinued.

* After including the v=spf1 SPF version tag you should follow with all IP addresses that are authorized to send email on your behalf. For example: <code>v=spf1 ip4:34.243.61.237 ip6:2a05:d018:e3:8c00:bb71:dea8:8b83:851e</code>

* Next, you can include an include tag for every third-party organization that is used to send email on your behalf e.g. <code>include:thirdpartydomain.com.</code> This tag indicates that this particular third party is authorized to send email on behalf of your domain. You need to consult with the third party to learn which domain to use as a value for the ‘include’ statement.

* Once you have implemented all IP addresses and include tags you should end your record with an <code>~all</code> or <code>-all</code> tag. The all tag is an important part of the SPF record as it indicates what policy should be applied when ISPs detect a server which is not listed in your SPF record. If an unauthorized server does send email on behalf of your domain, action is taken according to the policy that has been published (e.g. reject the email or mark it as spam). What is the difference between these tags? You need to instruct how strict servers need to treat the emails. The <code>~all</code> tag indicates a soft fail and the <code>-all</code> indicates a hardfail. The all tag has the following basic markers:<br><br>
`-all` – servers that aren’t listed in the SPF record are not authorized to send email (not compliant emails will be rejected)<br>
`~all` – if the email is received from a server that isn’t listed, the email will be marked as a soft fail (emails will be accepted but marked)<br>
`+all` - we strongly recommend not to use this option, this tag allows any server to send email from your domain<br>

* After defining your SPF record your record might look something like this:
<code>v=spf1 ip4:34.243.61.237 ip6:2a05:d018:e3:8c00:bb71:dea8:8b83:851e include:thirdpartydomain.com -all</code>

Useful resources:

- [SPF Record Checker](https://www.dmarcanalyzer.com/spf/checker/)
- [SPF Syntax](https://www.spf-record.com/syntax)


</details>

<details>
<summary><b>What is the difference between an authoritative and a nonauthoritative answer to a DNS query? ***</b></summary><br>

An authoritative DNS query answer comes from the server that contains the zone files for the domain queried. This is the name server that the domain administrator set up the DNS records on. A nonauthoriative answer comes from a name server that does not host the domain zone files (for example, a commonly used name server has the answer cached such as Google's 8.8.8.8 or OpenDNS 208.67.222.222).

</details>

<details>
<summary><b>If you try resolve hostname you get <code>NXDOMAIN</code> from <code>host</code> command. Your <code>resolv.conf</code> stores two nameservers but only second of this store this domain name. Why did not the local resolver check the second nameserver?</b></summary><br>

**NXDOMAIN** is nothing but non-existent Internet or Intranet domain name. If domain name is unable to resolved using the DNS, a condition called the **NXDOMAIN** occurred.

The default behavior for `resolv.conf` and the `resolver` is to try the servers in the order listed. The resolver will only try the next nameserver if the first nameserver times out.

The algorithm used is to try a name server, and if the query times out, try the next, until out of name servers, then repeat trying all the name servers until a maximum number of retries are made.

If a nameserver responds with **SERVFAIL** or a referral (**nofail**) or terminate query (**fail**) also only the first dns server will be used.

Example:

```
nameserver 192.168.250.20   # it's not a dns
nameserver 8.8.8.8          # not store gate.test.int
nameserver 127.0.0.1        # store gate.test.int
```

so if you check:

```
host -v -t a gate.test.int
Trying "gate.test.int"                        # trying first dns (192.168.250.20) but response is time out, so try the next nameserver
Host gate.test.int not found: 3(NXDOMAIN)     # ok but response is NXDOMAIN (not found this domain name)
Received 88 bytes from 8.8.8.8#53 in 43 ms
Received 88 bytes from 8.8.8.8#53 in 43 ms
                                              # so the last server in the list was not asked
```

To avoid this you can use e.g. `nslookup` command which will use the second nameserver if it receives a **SERVFAIL** from the first nameserver.

Useful resources:

- [Second nameserver in /etc/resolv.conf not picked up by wget](https://serverfault.com/questions/398837/second-nameserver-in-etc-resolv-conf-not-picked-up-by-wget)

</details>

<details>
<summary><b>Explore the current MTA configuration at your site. What are some of the special features of the MTA that are in use? ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>How to find a domain based on the IP address? What techniques/tools can you use? ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>Is it possible to have SSL certificate for IP address, not domain name?</b></summary><br>

It is possible (but rarely used) as long as it is a public IP address.

An SSL certificate is typically issued to a Fully Qualified Domain Name (FQDN) such as `https://www.domain.com`. However, some organizations need an SSL certificate issued to a public IP address. This option allows you to specify a public IP address as the Common Name in your Certificate Signing Request (CSR). The issued certificate can then be used to secure connections directly with the public IP address (e.g. `https://1.1.1.1`.).

According to the CA Browser forum, there may be compatibility issues with certificates for IP addresses unless the IP address is in both the commonName and subjectAltName fields. This is due to legacy SSL implementations which are not aligned with RFC 5280, notably, Windows OS prior to Windows 10.

Useful resources:

- [Are SSL certificates bound to the servers ip address?](https://stackoverflow.com/questions/1095780/are-ssl-certificates-bound-to-the-servers-ip-address)
- [SSL certificate for a public IP address?](https://serverfault.com/questions/193775/ssl-certificate-for-a-public-ip-address)

</details>

<details>
<summary><b>How do you do load testing and capacity planning for websites? ***</b></summary><br>

To be completed.

Useful resources:

- [How do you do load testing and capacity planning for web sites? (original)](https://serverfault.com/questions/350454/how-do-you-do-load-testing-and-capacity-planning-for-web-sites)
- [Can you help me with my capacity planning?](https://serverfault.com/questions/384686/can-you-help-me-with-my-capacity-planning)
- [How do you do load testing and capacity planning for databases?](https://serverfault.com/questions/350458/how-do-you-do-load-testing-and-capacity-planning-for-databases)

</details>

<details>
<summary><b>Developer reports a problem with connectivity to the remote service. Use <code>/dev</code> for troubleshooting.</b></summary><br>

```bash
# <host> - set remote host
# <port> - set destination port

# 1
timeout 1 bash -c "</dev/tcp/<host>/<port>" >/dev/null 2>&1 ; echo $?

# 2
timeout 1 bash -c 'cat < /dev/null > </dev/tcp/<host>/<port>' ; echo $?

# 2
&> echo > "</dev/tcp/<host>/<port>"
```

Useful resources:

- [Advanced Bash-Scripting Guide - /dev](http://www.tldp.org/LDP/abs/html/devref1.html#DEVTCP)
- [/dev/tcp as a weapon](https://securityreliks.wordpress.com/2010/08/20/devtcp-as-a-weapon/)
- [Test from shell script if remote TCP port is open](https://stackoverflow.com/questions/4922943/test-from-shell-script-if-remote-tcp-port-is-open)

</details>

<details>
<summary><b>How do I measure request and response times at once using <code>curl</code>?</b></summary><br>

`curl` supports formatted output for the details of the request (see the `curl` manpage for details, under `-w| -write-out 'format'`). For our purposes we’ll focus just on the timing details that are provided.

1. Create a new file, `curl-format.txt`, and paste in:

```bash
    time_namelookup:  %{time_namelookup}\n
       time_connect:  %{time_connect}\n
    time_appconnect:  %{time_appconnect}\n
   time_pretransfer:  %{time_pretransfer}\n
      time_redirect:  %{time_redirect}\n
 time_starttransfer:  %{time_starttransfer}\n
                    ----------\n
         time_total:  %{time_total}\n
```

2. Make a request:

```bash
curl -w "@curl-format.txt" -o /dev/null -s "http://example.com/"
```

What this does:

- `-w "@curl-format.txt"` - tells cURL to use our format file
- `-o /dev/null` - redirects the output of the request to /dev/null
- `-s` - tells cURL not to show a progress meter
`http://example.com/` is the URL we are requesting. Use quotes particularly if your URL has "&" query string parameters

</details>

<details>
<summary><b>You need to move ext4 journal on another disk/partition. What are the reasons for this? ***</b></summary><br>

To be completed.

Useful resources:

- [ext4: using external journal to optimize performance](https://raid6.com.au/posts/fs_ext4_external_journal/)
- [How to move an ext4 journal](https://unix.stackexchange.com/questions/278998/how-to-move-an-ext4-journal)

</details>

<details>
<summary><b>Does having Varnish in front of your website/app mean you don't need to care about load balancing or redundancy?</b></summary><br>

It depends. Varnish is a cache server, so its purpose is to cache contents and to act as a reverse proxy, to speed up retrieval of data and to lessen the load on the webserver.
Varnish can be also configured as a load-balancer for multiple web servers, but if we use just one Varnish server, this will become our single point of failure on our infrastructure.

A better solution to ensure load-balancing or redundancy will be a cluster of at least two Varnish instances, in active-active mode or active-passive mode.

</details>

<details>
<summary><b>What are hits, misses, and hit-for-pass in Varnish Cache?</b></summary><br>

A **hit** is a request which is successfully served from the cache, a **miss** is a request that goes through the cache but finds an empty cache and therefore has to be fetched from the origin, the **hit-for-pass** comes in when Varnish Cache realizes that one of the objects it has requested is uncacheable and will result in a pass.

Useful resources:

- [VCL rules for hits](https://book.varnish-software.com/4.0/chapters/VCL_Subroutines.html#vcl-vcl-hit)
- [VCL rules for hit-for-pass](https://book.varnish-software.com/4.0/chapters/VCL_Subroutines.html#hit-for-pass)
- [Example of the use](https://book.varnish-software.com/4.0/chapters/VCL_Basics.html#vcl-backend-response)

</details>

<details>
<summary><b>What is a reasonable TTL for cached content given the following parameters? ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>Developer says: <i><code>htaccess</code> is full of magic and it should be used</i>. What is your opinion about using <code>htaccess</code> files? How has this effect on the web app</b></summary><br>

`.htaccess` files were born out of an era when shared hosting was common­place:

- sysadmins needed a way to allow multiple clients to access their server under different accounts, with different configurations for their web­sites.

The `.htaccess` file allowed them to modify how Apache works without having access to the entire server. These files can reside in any and every directory in the directory tree of the website and provide features to the directory and the files and folders inside it.

**It’s horrible for performance**

For `.htaccess` to work Apache needs to check EVERY directory in the requested path for the existence of a `.htaccess` file and if it exists it reads EVERY one of them and parses it. This happens for EVERY request. Remember that the second you change that file, it’s effective. This is because Apache reads it every time.

Every single request the web­server handles - even for the lowliest `.png` or `.css` file - causes Apache to:

- look for a `.htaccess` file in the directory of the current request
- then look for a `.htaccess` file in every directory from there up to the server root
- coalesce all of these `.htaccess` files together
- reconfigure the web­server using the new settings
- finally, deliver the file

Every web­page can generate dozens of requests. This is over­head you don’t need, and what’s more, it’s completely unnecessary.

**Security and permission loss**

Allowing individual users to modify the configuration of a server using `.htaccess` can cause security concerns if not taken care properly. If you add any directive in the `.htaccess` file, it will be considered as they are added to Apache configuration file.

This means it may be possible for non-admins to write these files and thus 'undo' all of your security. If you need to do something that is temporary, `.htaccess` is a good place to do it, if you need to do something more permanent, just put it in your `/etc/apache/sites-available/site.conf` (or `httpd.conf` or whatever your server calls).

**Summary**

You should avoid using `.htaccess` files completely if you have access to httpd main server config file. If it worked in `.htaccess`, it will work in your virtual host `.conf` file as well.

If you cannot avoid using `.htaccess` files, you should follow these rules.

- use only one `.htaccess` file or as few as possible
- place the `.htaccess` file in the site root directory
- keep your `.htaccess` file short and simple

Useful resources:

- [Like Apache: .htaccess](https://www.nginx.com/resources/wiki/start/topics/examples/likeapache-htaccess/)
- [Don't Use .htaccess Unless You Must](https://www.danielmorell.com/guides/htaccess-seo/basics/dont-use-htaccess-unless-you-must)

</details>

<details>
<summary><b>Is it safe to use SNI SSL in production? How to test connection with and without it? In which cases it is useful?</b></summary><br>

With <b>OpenSSL</b>:

```bash
# Testing connection to remote host (with SNI support)
echo | openssl s_client -showcerts -servername google.com -connect google.com:443
# Testing connection to remote host (without SNI support)
echo | openssl s_client -connect google.com:443 -showcerts
```

With <b>GnuTLS</b>:

```bash
# Testing connection to remote host (with SNI support)
gnutls-cli -p 443 google.com
# Testing connection to remote host (without SNI support)
gnutls-cli --disable-sni -p 443 google.com
```

</details>

<details>
<summary><b>How are cookies passed in the HTTP protocol?</b></summary><br>

The server sends the following in its response header to set a cookie field:

`Set-Cookie:name=value`

If there is a cookie set, then the browser sends the following in its request header:

`Cookie:name=value`

</details>

<details>
<summary><b>How to prevent processing requests in web server with undefined server names? No defined default server name rule can be security issue? ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>You should rewrite POST with payload to an external API but the POST requests loose the parameters passed on the URL. How to fix this problem (e.g. in Nginx) and what are the reasons for this behavior?</b></summary><br>

The issue is that external redirects will never resend **POST** data. This is written into the HTTP spec (check the `3xx` section). Any client that does do this is violating the spec.

**POST** data is passed in the body of the request, which gets dropped if you do a standard redirect.

Look at this:

```
   +-------------------------------------------+-----------+-----------+
   |                                           | Permanent | Temporary |
   +-------------------------------------------+-----------+-----------+
   | Allows changing the request method from   | 301       | 302       |
   | POST to GET                               |           |           |
   | Does not allow changing the request       | 308       | 307       |
   | method from POST to GET                   |           |           |
   +-------------------------------------------+-----------+-----------+
```

You can try with the HTTP status code **307**, a RFC compliant browser should repeat the post request. You just need to write a Nginx rewrite rule with HTTP status code **307** or **308**:

```bash
location / {
    proxy_pass              http://localhost:80;
    client_max_body_size    10m;
}

location /api {
    # HTTP 307 only for POST method.
    if ($request_method = POST) {
        return 307 https://api.example.com?request_uri;
    }

    # You can keep this for non-POST requests.
    rewrite ^ https://api.example.com?request_uri permanent;

    client_max_body_size    10m;
}
```

HTTP Status code **307** or **308** should be used instead of **301** because it changes the request method from **POST** to **GET**.

Useful resources:

- [Redirection on Apache (Maintain POST params)](https://stackoverflow.com/questions/17295085/redirection-on-apache-maintain-post-params)
- [Why doesn't HTTP have POST redirect?](https://softwareengineering.stackexchange.com/questions/99894/why-doesnt-http-have-post-redirect)

</details>

<details>
<summary><b>What is the proper way to test NFS performance? Prepare a short checklist.
</b></summary><br>

The best benchmark is always "the application(s) that you normally use". The load on a NFS system when you have 20 people simultaneously compiling a Linux kernel is completely different from a bunch of people logging in at the same time or the accounts uses as "home directories for the local web-server".

But we have some good tools for testing this.

- <b>boonie</b> - a classical performances evaluation tool tests. The main program tests database type access to a single file (or a set of files if you wish to test more than 1G of storage), and it tests creation, reading, and deleting of small files which can simulate the usage of programs such as Squid, INN, or Maildir format email.
- <b>DBench</b> - was written to allow independent developers to debug and test SAMBA. It is heavily inspired of the original SAMBA tool.
- <b>IOZone</b> - performance tests suite. POSIX and 64 bits compliant. This tests is the file system test from the L.S.E. Main features: POSIX async I/O, Mmap() file I/O, Normal file I/O Single stream measurement, Multiple stream measurement, Distributed file server measurements (Cluster) POSIX pthreads, Multi-process measurement selectable measurements with fsync, O_SYNC Latency plots.

</details>

<details>
<summary><b>You need to block several IPs from the same subnet. What is the most efficient way for the system to traverse the iptables rule set or the black-hole route?</b></summary><br>

If you have a system with thousands of routes defined in the routing table and nothing in the iptables rules than it might actually be more efficient to input an iptables rule.

In most systems however the routing table is fairly small, in cases like this it is actually more efficient to use null routes. This is especially true if you already have extensive iptables rules in place.

Assuming you're blocking based on source address and not destination, then doing the **DROP** in **raw/PREROUTING** would work well as you would essentially be able to drop the packet before any routing decision is made.

Remember however that iptables rules are essentially a linked-list and for optimum performance when blocking a number of addresses you should use an `ipset`.

On the other hand if blocking by destination, there is likely little difference between blocking at the routing table vs iptables **EXCEPT** if source IPs are spoofed in which case the blackholed entries may consume routing cache resources; in this case, **raw/PREROUTING** remains preferable.

Your outgoing route isn't going to matter until you try to send a packet back to the attacker. By that time you will have already incurred most of the cost of socket setup and may even have a thread blocking waiting for the kernel to conclude you have no route to host, plus whatever error handling your server process does when it concludes there's a network problem.

iptables or another firewall will allow you to block the incoming traffic and discard it before it reaches the daemon process on your server. It seems clearly superior in this use case.

```bash
iptables -A INPUT -s 192.168.200.0/24 -j DROP
```

When you define a route on a Linux/Unix system it tells the system in order to communicate with the specified IP address you will need to route your network communication to this specific place.

When you define a null route it simply tells the system to drop the network communication that is designated to the specified IP address. What this means is any TCP based network communication will not be able to be established as your server will no longer be able to send an SYN/ACK reply. Any UDP based network communication however will still be received; however your system will no longer send any response to the originating IP.

While iptables can accept tens of thousands of rules in a chain, the chains are walked sequentially until a match is found on every packet. So, lots of rules can lead to the system spending amazing amounts of CPU time walking through the rules.

The routing rules are much simpler than iptables. With iptables, a match can be based on many different variables including protocols, source and destination packets, and even other packets that were sent before the current packet.

In routing, all that matters is the remote IP address, so it's very easy to optimize. Also, many systems have a lot of routing rules. A typical system may only have 5 or 10, but something that's acting as a BGP router can have tens of thousands. So, for a very long time there have been extensive optimizations in selecting the right route for a particular packet.

In less technical terms this means your system will receive data from the attackers but no longer respond to it.

```bash
ip route add blackhole 192.168.200.0/24
```

or

```bash
ip route add 192.168.200.0/24 via 127.0.0.1
```

Useful resources:

- [The difference between iptables DROP and null-routing.](https://www.tummy.com/blogs/2006/07/27/the-difference-between-iptables-drop-and-null-routing/)

</details>

<details>
<summary><b>How to run <code>scp</code> with a second remote host?</b></summary><br>

With `ssh`:

```bash
ssh user1@remote1 'ssh user2@remote2 "cat file"' > file
```

With `tar` (with compression):

```bash
ssh user1@remote1 'ssh user2@remote2 "cd path2; tar cj file"' | tar xj
```

With `ssh` and port forwarding tunnel:

```bash
# First, open the tunnel
ssh -L 1234:remote2:22 -p 45678 user1@remote1

# Then, use the tunnel to copy the file directly from remote2
scp -P 1234 user2@localhost:file .
```

</details>

<details>
<summary><b>How can you reduce load time of a dynamic website?</b></summary><br>

- webpage optimization
- cached web pages
- quality web hosting
- compressed text files
- apache/nginx tuning

</details>

<details>
<summary><b>What types of dns cache working when you type api.example.com in your browser and press return?</b></summary><br>

Browser checks if the domain is in its cache (to see the DNS Cache in Chrome, go to `chrome://net-internals/#dns`). When this cache fails, it simply asks the OS to resolve the domain.

The OS resolver has it's own cache which it will check. If it fails this, it resorts to asking the OS configured DNS servers.

The OS configured DNS servers will typically be configured by DHCP from the router where the DNS servers are likely to be the ISP's DNS servers configured by DHCP from the internet gateway to the router.

In the event the router has it's own DNS servers, it may have it's own cache otherwise you should be directed straight to your ISP's DNS servers most typically as soon as the OS cache was found to be empty.

Useful resources:

- [What happens when...](https://github.com/alex/what-happens-when)
- [DNS Explained - How Your Browser Finds Websites](https://scotch.io/tutorials/dns-explained-how-your-browser-finds-websites)
- [Firefox invalidate dns cache](https://stackoverflow.com/questions/13063496/firefox-invalidate-dns-cache)

</details>

<details>
<summary><b>What is the difference between <code>Cache-Control: max-age=0</code> and <code>Cache-Control: no-cache</code>?</b></summary><br>

**When sent by the origin server**

`max-age=0` simply tells caches (and user agents) the response is stale from the get-go and so they SHOULD revalidate the response (e.g. with the If-Not-Modified header) before using a cached copy, whereas, `no-cache` tells them they MUST revalidate before using a cached copy.

In other words, caches may sometimes choose to use a stale response (although I believe they have to then add a Warning header), but `no-cache` says they're not allowed to use a stale response no matter what. Maybe you'd want the SHOULD-revalidate behavior when baseball stats are generated in a page, but you'd want the MUST-revalidate behavior when you've generated the response to an e-commerce purchase.

**When sent by the user agent**

If a user agent sends a request with `Cache-Control: max-age=0` (aka. "end-to-end revalidation"), then each cache along the way will revalidate its cache entry (e.g. with the If-Not-Modified header) all the way to the origin server. If the reply is then 304 (Not Modified), the cached entity can be used.

On the other hand, sending a request with `Cache-Control: no-cache` (aka. "end-to-end reload") doesn't revalidate and the server MUST NOT use a cached copy when responding.

</details>

<details>
<summary><b>What are the security risks of setting <code>Access-Control-Allow-Origin</code>?</b></summary><br>

By responding with <code>Access-Control-Allow-Origin: *</code>, the requested resource allows sharing with every origin. This basically means that any site can send an XHR request to your site and access the server’s response which would not be the case if you hadn’t implemented this CORS response.

So any site can make a request to your site on behalf of their visitors and process its response. If you have something implemented like an authentication or authorization scheme that is based on something that is automatically provided by the browser (cookies, cookie-based sessions, etc.), the requests triggered by the third party sites will use them too.

</details>

<details>
<summary><b>Create a single-use TCP or UDP proxy with <code>netcat</code>.</b></summary><br>

```bash
### TCP -> TCP
nc -l -p 2000 -c "nc [ip|hostname] 3000"

### TCP -> UDP
nc -l -p 2000 -c "nc -u [ip|hostname] 3000"

### UDP -> UDP
nc -l -u -p 2000 -c "nc -u [ip|hostname] 3000"

### UDP -> TCP
nc -l -u -p 2000 -c "nc [ip|hostname] 3000"
```

</details>

<details>
<summary><b>Explain 3 techniques for avoiding firewalls with <code>nmap</code>.</b></summary><br>

**Use Decoy addresses**

```bash
# Generates a random number of decoys.
nmap -D RND:10 [target]

# Manually specify the IP addresses of the decoys.
nmap -D decoy1,decoy2,decoy3
```

In this type of scan you can instruct Nmap to spoof packets from other hosts.In the firewall logs it will be not only our IP address but also and the IP addresses of the decoys so it will be much harder to determine from which system the scan started.

**Source port number specification**

```bash
nmap --source-port 53 [target]
```

A common error that many administrators are doing when configuring firewalls is to set up a rule to allow all incoming traffic that comes from a specific port number.The <code>--source-port</code> option of Nmap can be used to exploit this misconfiguration.Common ports that you can use for this type of scan are: 20, 53 and 67.

**Append Random Data**

```bash
nmap --data-length 25 [target]
```

Many firewalls are inspecting packets by looking at their size in order to identify a potential port scan.This is because many scanners are sending packets that have specific size.In order to avoid that kind of detection you can use the command <code>--data-length</code> to add additional data and to send packets with different size than the default.

**TCP ACK Scan**

```bash
nmap -sA [target]
```

It is always good to send the ACK packets rather than the SYN packets because if there is any active firewall working on the remote computer then because of the ACK packets the firewall cannot create the log, since firewalls treat ACK packet as the response of the SYN packet.

Useful resources:

- [Nmap - Techniques for Avoiding Firewalls](https://pentestlab.blog/2012/04/02/nmap-techniques-for-avoiding-firewalls/)

</details>

###### Devops Questions (5)

<details>
<summary><b>Explain how Flap Detection works in Nagios?</b></summary><br>

**Flapping** occurs when a service or host changes state too frequently, this causes lot of problem and recovery notifications.

Once you have defined **Flapping**, explain how Nagios detects **Flapping**. Whenever Nagios checks the status of a host or service, it will check to see if it has started or stopped flapping.

Nagios follows the below given procedure to do that:

- storing the results of the last 21 checks of the host or service analyzing the historical check results and determine where state changes/transitions occur
- using the state transitions to determine a percent state change value (a measure of change) for the host or service
- comparing the percent state change value against low and high flapping thresholds

</details>

<details>
<summary><b>What are the advantages that Containerization provides over Virtualization?</b></summary><br>

Below are the advantages of containerization over virtualization:

- containers provide real-time provisioning and scalability but VMs provide slow provisioning
- containers are lightweight when compared to VMs
- VMs have limited performance when compared to containers
- containers have better resource utilization compared to VMs

</details>

<details>
<summary><b>Is the way of distributing Docker apps (e.g. Apache, MySQL) from Docker Hub is good for production environments? Describe security problems and possible solutions. ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>Some of the common use cases of LXC and LXD come from the following requirements... Explain.</b></summary><br>

- the need for an isolated development environment without polluting your host machine
- isolation within production servers and the possibility to run more than one service in its own container
- a need to test things with more than one version of the same software or different operating system environments
- experimenting with different and new releases of GNU/Linux distributions without having to install them on a physical host machine
- trying out a software or development stack that may or may not be used after some playing around
- installing many types of software in your primary development machine or production server and maintaining them on a longer run
- doing a dry run of any installation or maintenance task before actually executing it on production machines
- better utilization and provisioning of server resources with multiple services running for different users or clients
- high-density virtual private server (VPS) hosting, where isolation without the cost of full virtualization is needed
- easy access to host hardware from a container, compared to complicated access methods from virtual machines
- multiple build environments with different customizations in place

</details>

<details>
<summary><b>You have to prepare a Redis cluster. How will you ensure security?</b></summary><br>

- protect a given Redis instance from outside accesses via firewall
- binding it to 127.0.0.1 if only local clients are accessing it
- sandboxed environment
- enabling **AUTH**
- enabling **Protected Mode**
- data encryption support (e.g. `spiped`)
- disabling of specific commands
- users **ACLs**

Useful resources:

- [Redis Security](https://redis.io/topics/security)
- [A few things about Redis security](http://antirez.com/news/96)

</details>

###### Cyber Security Questions (5)

<details>
<summary><b>What is OWASP Application Security Verification Standard? Explain in a few points. ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>What is CSRF?</b></summary><br>

**Cross Site Request Forgery** is a web application vulnerability in which the server does not check whether the request came from a trusted client or not. The request is just processed directly. It can be further followed by the ways to detect this, examples and countermeasures.

</details>

<details>
<summary><b>What is the difference between policies, processes and guidelines?</b></summary><br>

As **security policy** defines the security objectives and the security framework of an organisation. A **process** is a detailed step by step how to document that specifies the exact action which will be necessary to implement important security mechanism. **Guidelines** are recommendations which can be customized and used in the creation of procedures.

</details>

<details>
<summary><b>What is a false positive and false negative in case of IDS?</b></summary><br>

When the device generated an alert for an intrusion which has actually not happened: this is **false positive** and if the device has not generated any alert and the intrusion has actually happened, this is the case of a **false negative**.

</details>

<details>
<summary><b>10 quick points about web server hardening.</b></summary><br>

Example:

- if machine is a new install, protect it from hostile network traffic, until the operating system is installed and hardened
- create a separate partition with the `nodev`, `nosuid`, and `noexec` options set for `/tmp`
- create separate partitions for `/var`, `/var/log`, `/var/log/audit`, and `/home`
- enable randomized virtual memory region placement
- remove legacy services (e.g. `telnet-server`, `rsh`, `rlogin`, `rcp`, `ypserv`, `ypbind`, `tftp`, `tftp-server`, `talk`, `talk-server`).
- limit connections to services running on the host to authorized users of the service via firewalls and other access control technologies
- disable source routed packet acceptance
- enable **TCP/SYN** cookies
- disable SSH root login
- install and configure **AIDE**
- install and configure **OSsec HIDS**
- configure **SELinux**
- all administrator or root access must be logged
- integrity checking of system accounts, group memberships, and their associated privileges should be enabled and tested
- set password creation requirements (e.g. with PAM)

Useful resources:

- [Security Harden CentOS 7](https://highon.coffee/blog/security-harden-centos-7/)
- [CentOS 7 Server Hardening Guide](https://www.lisenet.com/2017/centos-7-server-hardening-guide/)

</details>

## <a name="secret-knowledge">Secret Knowledge</a>

### :diamond_shape_with_a_dot_inside: <a name="guru-sysadmin">Guru Sysadmin</a>

<details>
<summary><b>Explain what is Event-Driven architecture and how it improves performance? ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>An application encounters some performance issues. You should to find the code we have to optimize. How to profile app in Linux environment?</b></summary><br>

> Ideally, I need an app that will attach to a process and log periodic snapshots of: memory usage number of threads CPU usage.

1. You can use `top`in batch mode. It runs in the batch mode either until it is killed or until N iterations is done:

```bash
top -b -p `pidof a.out`
```

or

```bash
top -b -p `pidof a.out` -n 100
```

2. You can use ps (for instance in a shell script):

```bash
ps --format pid,pcpu,cputime,etime,size,vsz,cmd -p `pidof a.out`
```

> I need some means of recording the performance of an application on a Linux machine.

1. To record performance data:

```bash
perf record -p `pidof a.out`
```

or to record for 10 secs:

```bash
perf record -p `pidof a.out` sleep 10
```

or to record with call graph ():

```bash
perf record -g -p `pidof a.out`
```

2) To analyze the recorded data

```bash
perf report --stdio
perf report --stdio --sort=dso -g none
perf report --stdio -g none
perf report --stdio -g
```

**This is an example of profiling a test program**

1. I run my test program (c++):

```bash
./my_test 100000000
```

2. Then I record performance data of a running process:

```bash
perf record -g  -p `pidof my_test` -o ./my_test.perf.data sleep 30
```

3. Then I analyze load per module:

```bash
perf report --stdio -g none --sort comm,dso -i ./my_test.perf.data

# Overhead  Command                 Shared Object
# ........  .......  ............................
#
    70.06%  my_test  my_test
    28.33%  my_test  libtcmalloc_minimal.so.0.1.0
     1.61%  my_test  [kernel.kallsyms]
```

4. Then load per function is analyzed:

```bash
perf report --stdio -g none -i ./my_test.perf.data | c++filt

# Overhead  Command                 Shared Object                       Symbol
# ........  .......  ............................  ...........................
#
    29.30%  my_test  my_test                       [.] f2(long)
    29.14%  my_test  my_test                       [.] f1(long)
    15.17%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator new(unsigned long)
    13.16%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator delete(void*)
     9.44%  my_test  my_test                       [.] process_request(long)
     1.01%  my_test  my_test                       [.] operator delete(void*)@plt
     0.97%  my_test  my_test                       [.] operator new(unsigned long)@plt
     0.20%  my_test  my_test                       [.] main
     0.19%  my_test  [kernel.kallsyms]             [k] apic_timer_interrupt
     0.16%  my_test  [kernel.kallsyms]             [k] _spin_lock
     0.13%  my_test  [kernel.kallsyms]             [k] native_write_msr_safe

  ...
```

5. Then call chains are analyzed:

```bash
perf report --stdio -g graph -i ./my_test.perf.data | c++filt

# Overhead  Command                 Shared Object                       Symbol
# ........  .......  ............................  ...........................
#
    29.30%  my_test  my_test                       [.] f2(long)
            |
            --- f2(long)
               |
                --29.01%-- process_request(long)
                          main
                          __libc_start_main

    29.14%  my_test  my_test                       [.] f1(long)
            |
            --- f1(long)
               |
               |--15.05%-- process_request(long)
               |          main
               |          __libc_start_main
               |
                --13.79%-- f2(long)
                          process_request(long)
                          main
                          __libc_start_main

  ...
```

So at this point you know where your program spends time.

Also the simple way to do app profile is to use the `pstack` utility or `lsstack`.

Other tool is Valgrind. So this is what I recommend. Run program first:

```bash
valgrind --tool=callgrind --dump-instr=yes -v --instr-atstart=no ./binary > tmp
```

Now when it works and we want to start profiling we should run in another window:

```bash
callgrind_control -i on
```

This turns profiling on. To turn it off and stop whole task we might use:

```bash
callgrind_control -k
```

Now we have some files named callgrind.out.* in current directory. To see profiling results use:

```bash
kcachegrind callgrind.out.*
```

I recommend in next window to click on **Self** column header, otherwise it shows that `main()` is most time consuming task.

Useful resources:

- [Tracing processes for fun and profit](http://techblog.rosedu.org/tracing-processes-for-fun-and-profit.html)

</details>

<details>
<summary><b>Using a Linux system with a limited number of packages installed, and telnet is not available. Use sysfs virtual filesystem to test connection on all interfaces (without loopback).</b></summary><br>

For example:

```bash
#!/usr/bin/bash

for iface in $(ls /sys/class/net/ | grep -v lo) ; do

  if [[ $(cat /sys/class/net/$iface/carrier) = 1 ]] ; then state=1 ; fi

done

if [[ $state -ne 0 ]] ; then echo "not connection" > /dev/stderr ; exit ; fi
```

</details>

<details>
<summary><b>Write two golden rules for reducing the impact of hacked system.</b></summary><br>

1) **The principle of least privilege**

You should configure services to run as a user with the least possible rights necessary to complete the service's tasks. This can contain a hacker even after they break in to a machine.

As an example, a hacker breaking into a system using a zero-day exploit of the Apache webserver service is highly likely to be limited to just the system memory and file resources that can be accessed by that process. The hacker would be able to download your html and php source files, and probably look into your mysql database, but they should not be able to get root or extend their intrusion beyond apache-accessible files.

Many default Apache webserver installations create the 'apache' user and group by default and you can easily configure the main Apache configuration file (`httpd.conf`) to run apache using those groups.

2) **The principle of separation of privileges**

If your web site only needs read-only access to the database, then create an account that only has read-only permissions, and only to that database.

**SElinux** is a good choice for creating context for security, `app-armor` is another tool. **Bastille** was a previous choice for hardening.

Reduce the consequence of any attack, by separating the power of the service that has been compromised into it own "Box".

3) **Whitelist, don't blacklist**

You're describing a blacklist approach. A whitelist approach would be much safer.

An exclusive club will never try to list everyone who can't come in; they will list everyone who can come in and exclude those not on the list.

Similarly, trying to list everything that shouldn't access a machine is doomed. Restricting access to a short list of programs/IP addresses/users would be more effective.

Of course, like anything else, this involves some trade-offs. Specifically, a whitelist is massively inconvenient and requires constant maintenance.

To go even further in the tradeoff, you can get great security by disconnecting the machine from the network.

**Also interesting are**:

Use the tools available. It's highly unlikely that you can do as well as the guys who are security experts, so use their talents to protect yourself.

- public key encryption provides excellent security
- enforce password complexity
- understand why you are making exceptions to the rules above - review your exceptions regularly
- hold someone to account for failure, it keeps you on your toes

Useful resources:

- [How to prevent zero day attacks (original)](https://serverfault.com/questions/391370/how-to-prevent-zero-day-attacks)

</details>

<details>
<summary><b>You're on a security conference. Members debating about putting up the OpenBSD firewall on the core of the network. Go to the podium and express your opinion about this solution. What are the pros/cons and why? ***</b></summary><br>

To be completed.

</details>

<details>
<summary><b>Is there a way to allow multiple cross-domains using the Access-Control-Allow-Origin header in Nginx?</b></summary><br>

To match a list of domain and subdomain this regex make it ease to work with fonts:

```bash
location ~* \.(?:ttf|ttc|otf|eot|woff|woff2)$ {
   if ( $http_origin ~* (https?://(.+\.)?(domain1|domain2|domain3)\.(?:me|co|com)$) ) {
      add_header "Access-Control-Allow-Origin" "$http_origin";
   }
}
```

More slightly configuration:

```bash
location / {

    if ($http_origin ~* (^https?://([^/]+\.)*(domainone|domaintwo)\.com$)) {
        set $cors "true";
    }

    # Nginx doesn't support nested If statements. This is where things get slightly nasty.
    # Determine the HTTP request method used
    if ($request_method = 'GET') {
        set $cors "${cors}get";
    }
    if ($request_method = 'POST') {
        set $cors "${cors}post";
    }

    if ($cors = "true") {
        # Catch all in case there's a request method we're not dealing with properly
        add_header 'Access-Control-Allow-Origin' "$http_origin";
    }

    if ($cors = "trueget") {
        add_header 'Access-Control-Allow-Origin' "$http_origin";
        add_header 'Access-Control-Allow-Credentials' 'true';
        add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';
        add_header 'Access-Control-Allow-Headers' 'DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type';
    }

    if ($cors = "truepost") {
        add_header 'Access-Control-Allow-Origin' "$http_origin";
        add_header 'Access-Control-Allow-Credentials' 'true';
        add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';
        add_header 'Access-Control-Allow-Headers' 'DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type';
    }

}
```

</details>

<details>
<summary><b>Explain <code>:(){ :|:& };:</code> and how stop this code if you are already logged into a system?</b></summary><br>

It's a **fork bomb**.

- `:()` - this defines the function. `:` is the function name and the empty parenthesis shows that it will not accept any arguments
- `{ }` - these characters shows the beginning and end of function definition
- `:|:` - it loads a copy of the function `:` into memory and pipe its output to another copy of the `:` function, which has to be loaded into memory
- `&` - this will make the process as a background process, so that the child processes will not get killed even though the parent gets auto-killed
- `:` - final `:` will execute the function again and hence the chain reaction begins

The best way to protect a multi-user system is to use **PAM** to limit the number of processes a user can use. We know the biggest problem with a fork bomb is the fact it takes up so many processes.

So we have two ways of attempting to fix this, if you are already logged into the system:
- execute a **SIGSTOP** command to stop the process: `killall -STOP -u user1`
- if you can't run at the command line you will have to use `exec` to force it to run (due to processes all being used): `exec killall -STOP -u user1`

With fork bombs your best method for this is preventing from being to big of an issue in the first place.

</details>

<details>
<summary><b>How to recover deleted file held open e.g. by Apache?</b></summary><br>

If a file has been deleted but is still open, that means the file still exists in the filesystem (it has an inode) but has a hard link count of 0. Since there is no link to the file, you cannot open it by name. There is no facility to open a file by inode either.

Linux exposes open files through special symbolic links under `/proc`. These links are called `/proc/12345/fd/42` where 12345 is the **PID** of a process and 42 is the number of a file descriptor in that process. A program running as the same user as that process can access the file (the read/write/execute permissions are the same you had as when the file was deleted).

The name under which the file was opened is still visible in the target of the symbolic link: if the file was `/var/log/apache/foo.log`, then the target of the link is `/var/log/apache/foo.log (deleted)`.

Thus you can recover the content of an open deleted file given the **PID** of a process that has it open and the descriptor that it's opened on like this:

```bash
recover_open_deleted_file () {
  old_name=$(readlink "$1")
  case "$old_name" in
    *' (deleted)')
      old_name=${old_name%' (deleted)'}
      if [ -e "$old_name" ]; then
        new_name=$(TMPDIR=${old_name%/*} mktemp)
        echo "$oldname has been replaced, recovering content to $new_name"
      else
        new_name="$old_name"
      fi
      cat <"$1" >"$new_name";;
    *) echo "File is not deleted, doing nothing";;
  esac
}
recover_open_deleted_file "/proc/$pid/fd/$fd"
```

If you only know the process **ID** but not the descriptor, you can recover all files with:

```bash
for x in /proc/$pid/fd/* ; do
  recover_open_deleted_file "$x"
done
```

If you don't know the process **ID** either, you can search among all processes:

```bash
for x in /proc/[1-9]*/fd/* ; do
  case $(readlink "$x") in
    /var/log/apache/*) recover_open_deleted_file "$x";;
  esac
done
```

You can also obtain this list by parsing the output of `lsof`, but it isn't simpler nor more reliable nor more portable (this is Linux-specific anyhow).

</details>

<details>
<summary><b>The team of admins needs your support. You must remotely reinstall the system on one of the main servers. There is no access to the management console (e.g. iDRAC). How to install Linux on disk, from and where other Linux exist and running?</b></summary><br>

It is possible that the question should be: "_System installation from the level and in place of already other system working_".

On the example of the Debian GNU/Linux distribution.

1. Creating a working directory and downloading the system using the debootstrap tool.

```bash
_working_directory="/mnt/system"
mkdir $_working_directory
debootstrap --verbose --arch amd64 {wheezy|jessie} . http://ftp.en.debian.org/debian
```

2. Mounting sub-systems: `proc`, `sys`, `dev` and `dev/pts`.

```bash
for i in proc sys dev dev/pts ; do mount -o bind $i $_working_directory/$i ; done
```

3. Copy system backup for restore.

```bash
cp system_backup_22012015.tgz $_working_directory/mnt
```

However, it is better not to waste space and do it in a different way (assuming that the copy is in `/mnt/backup`):

```bash
_backup_directory="${_working_directory}/mnt/backup"
mkdir $_backup_directory && mount --bind /mnt/backup $_backup_directory
```

4. Chroot to "new" system.

```bash
chroot $_working_directory /bin/bash
```

5. Updating information about mounted devices.

```bash
grep -v rootfs /proc/mounts > /etc/mtab
```

6. In the "new" system, the next thing to do is mount the disk on which the "old" system is located (e.g. `/dev/sda1`).

```bash
_working_directory="/mnt/old_system"
_backup_directory="/mnt/backup"
mkdir $_working_directory && mount /dev/sda1 $_working_directory
```

7. Remove all files of the old system.

```bash
for i in $(ls | awk '!(/proc/ || /dev/ || /sys/ || /mnt/)') ; do rm -fr $i ; done
```

8. The next step is to restore the system from a backup.

```bash
tar xzvfp $_backup_directory/system_backup_22012015.tgz -C $_working_directory
```

9. And mount `proc`, `sys`, `dev` and `dev/pts` in a new working directory.

```bash
for i in proc sys dev dev/pts ; do mount -o bind $i $_working_directory/$i ; done
```

10. Install and update grub configuration.

```bash
chroot $_working_directory /bin/bash -c "grub-install --no-floppy --root-directory=/ /dev/sda"
chroot $_working_directory /bin/bash -c "update-grub"
```

11. Unmount `proc`, `sys`, `dev` and `dev/pts` filesystems.

```bash
cd
grep $_working_directory /proc/mounts | cut -f2 -d " " | sort -r | xargs umount -n
```

None of the available commands, i.e. `halt`, `shutdown` or `reboot`, will work. You need to reload the system configuration - to do this, use the **kernel debugger** (without the '**b**' option):

```bash
echo 1 > /proc/sys/kernel/sysrq
echo reisu > /proc/sysrq-trigger
```

Of course, it is recommended to fully restart the machine in order to completely load the current system. To do this:

```bash
sync ; reboot -f
```

</details>

<details>
<summary><b>Rsync triggered Linux OOM killer on a single 50 GB file. How does the OOM killer decide which process to kill first? How to control this?</b></summary><br>

Major distribution kernels set the default value of `/proc/sys/vm/overcommit_memory` to zero, which means that processes can request more memory than is currently free in the system.

If memory is exhaustively used up by processes, to the extent which can possibly threaten the stability of the system, then the **OOM killer** comes into the picture.

NOTE: It is the task of the **OOM Killer** to continue killing processes until enough memory is freed for the smooth functioning of the rest of the process that the Kernel is attempting to run.

The **OOM Killer** has to select the best process(es) to kill. Best here refers to that process which will free up the maximum memory upon killing and is also the least important to the system.

The primary goal is to kill the least number of processes that minimizes the damage done and at the same time maximizing the amount of memory freed.

To facilitate this, the kernel maintains an `oom_score` for each of the processes. You can see the oom_score of each of the processes in the `/proc` filesystem under the pid directory.

  > When analyzing OOM killer logs, it is important to look at what triggered it.

```bash
cat /proc/10292/oom_score
```

The higher the value of `oom_score` of any process, the higher is its likelihood of getting killed by the **OOM Killer** in an out-of-memory situation.

If you want to create a special control group containing the list of processes which should be the first to receive the **OOM killer's** attention, create a directory under `/mnt/oom-killer` to represent it:

```bash
mkdir lambs
```

Set `oom.priority` to a value high enough:

```bash
echo 256 > /mnt/oom-killer/lambs/oom.priority
```

`oom.priority` is a 64-bit unsigned integer, and can have a maximum value an unsigned 64-bit number can hold. While scanning for the process to be killed, the **OOM-killer** selects a process from the list of tasks with the highest `oom.priority` value.

Add the PID of the process to be added to the list of tasks:

```bash
echo <pid> > /mnt/oom-killer/lambs/tasks
```

To create a list of processes, which will not be killed by the **OOM-killer**, make a directory to contain the processes:

```bash
mkdir invincibles
```

Setting `oom.priority` to zero makes all the process in this cgroup to be excluded from the list of target processes to be killed.

```bash
echo 0 > /mnt/oom-killer/invincibles/oom.priority
```

To add more processes to this group, add the pid of the task to the list of tasks in the invincible group:

```bash
echo <pid> > /mnt/oom-killer/invincibles/tasks
```

Useful resources:

- [Rsync triggered Linux OOM killer on a single 50 GB file](https://serverfault.com/questions/724469/rsync-triggered-linux-oom-killer-on-a-single-50-gb-file)
- [Taming the OOM killer](https://lwn.net/Articles/317814/)

</details>

<details>
<summary><b>You have a lot of sockets, hanging in <code>TIME_WAIT</code>. Your http service behind proxy serve a lot of small http requests. How to check and reduce <code>TIME_WAIT</code> sockets? ***</b></summary><br>

To be completed.

Useful resources:

- [How to reduce number of sockets in TIME_WAIT?](https://serverfault.com/questions/212093/how-to-reduce-number-of-sockets-in-time-wait)

</details>

<details>
<summary><b>How do <code>SO_REUSEADDR</code> and <code>SO_REUSEPORT</code> differ? Explain all socket implementations. ***</b></summary><br>

To be completed.

</details>
